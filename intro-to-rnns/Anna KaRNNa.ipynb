{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Anna KaRNNa\n",
    "\n",
    "In this notebook, I'll build a character-wise RNN trained on Anna Karenina, one of my all-time favorite books. It'll be able to generate new text based on the text from the book.\n",
    "\n",
    "This network is based off of Andrej Karpathy's [post on RNNs](http://karpathy.github.io/2015/05/21/rnn-effectiveness/) and [implementation in Torch](https://github.com/karpathy/char-rnn). Also, some information [here at r2rt](http://r2rt.com/recurrent-neural-networks-in-tensorflow-ii.html) and from [Sherjil Ozair](https://github.com/sherjilozair/char-rnn-tensorflow) on GitHub. Below is the general architecture of the character-wise RNN.\n",
    "\n",
    "<img src=\"assets/charseq.jpeg\" width=\"500\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import time\n",
    "from collections import namedtuple\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we'll load the text file and convert it into integers for our network to use. Here I'm creating a couple dictionaries to convert the characters to and from integers. Encoding the characters as integers makes it easier to use as input in the network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with open('moa.txt', 'r') as f:\n",
    "    text=f.read()\n",
    "vocab = set(text)\n",
    "vocab_to_int = {c: i for i, c in enumerate(vocab)}\n",
    "int_to_vocab = dict(enumerate(vocab))\n",
    "chars = np.array([vocab_to_int[c] for c in text], dtype=np.int32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check out the first 100 characters, make sure everything is peachy. According to the [American Book Review](http://americanbookreview.org/100bestlines.asp), this is the 6th best first line of a book ever."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "text[:100]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we can see the characters encoded as integers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "chars[:100]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Making training and validation batches\n",
    "\n",
    "Now I need to split up the data into batches, and into training and validation sets. I should be making a test set here, but I'm not going to worry about that. My test will be if the network can generate new text.\n",
    "\n",
    "Here I'll make both input and target arrays. The targets are the same as the inputs, except shifted one character over. I'll also drop the last bit of data so that I'll only have completely full batches.\n",
    "\n",
    "The idea here is to make a 2D matrix where the number of rows is equal to the batch size. Each row will be one long concatenated string from the character data. We'll split this data into a training set and validation set using the `split_frac` keyword. This will keep 90% of the batches in the training set, the other 10% in the validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def split_data(chars, batch_size, num_steps, split_frac=0.9):\n",
    "    \"\"\" \n",
    "    Split character data into training and validation sets, inputs and targets for each set.\n",
    "    \n",
    "    Arguments\n",
    "    ---------\n",
    "    chars: character array\n",
    "    batch_size: Size of examples in each of batch\n",
    "    num_steps: Number of sequence steps to keep in the input and pass to the network\n",
    "    split_frac: Fraction of batches to keep in the training set\n",
    "    \n",
    "    \n",
    "    Returns train_x, train_y, val_x, val_y\n",
    "    \"\"\"\n",
    "    \n",
    "    slice_size = batch_size * num_steps\n",
    "    n_batches = int(len(chars) / slice_size)\n",
    "    \n",
    "    # Drop the last few characters to make only full batches\n",
    "    x = chars[: n_batches*slice_size]\n",
    "    y = chars[1: n_batches*slice_size + 1]\n",
    "    \n",
    "    # Split the data into batch_size slices, then stack them into a 2D matrix \n",
    "    x = np.stack(np.split(x, batch_size))\n",
    "    y = np.stack(np.split(y, batch_size))\n",
    "    \n",
    "    # Now x and y are arrays with dimensions batch_size x n_batches*num_steps\n",
    "    \n",
    "    # Split into training and validation sets, keep the virst split_frac batches for training\n",
    "    split_idx = int(n_batches*split_frac)\n",
    "    train_x, train_y= x[:, :split_idx*num_steps], y[:, :split_idx*num_steps]\n",
    "    val_x, val_y = x[:, split_idx*num_steps:], y[:, split_idx*num_steps:]\n",
    "    \n",
    "    return train_x, train_y, val_x, val_y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now I'll make my data sets and we can check out what's going on here. Here I'm going to use a batch size of 10 and 50 sequence steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_x, train_y, val_x, val_y = split_data(chars, 10, 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_x.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at the size of this array, we see that we have rows equal to the batch size. When we want to get a batch out of here, we can grab a subset of this array that contains all the rows but has a width equal to the number of steps in the sequence. The first batch looks like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_x[:,:50]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I'll write another function to grab batches out of the arrays made by `split_data`. Here each batch will be a sliding window on these arrays with size `batch_size X num_steps`. For example, if we want our network to train on a sequence of 100 characters, `num_steps = 100`. For the next batch, we'll shift this window the next sequence of `num_steps` characters. In this way we can feed batches to the network and the cell states will continue through on each batch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_batch(arrs, num_steps):\n",
    "    batch_size, slice_size = arrs[0].shape\n",
    "    \n",
    "    n_batches = int(slice_size/num_steps)\n",
    "    for b in range(n_batches):\n",
    "        yield [x[:, b*num_steps: (b+1)*num_steps] for x in arrs]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building the model\n",
    "\n",
    "Below is a function where I build the graph for the network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def build_rnn(num_classes, batch_size=50, num_steps=50, lstm_size=128, num_layers=2,\n",
    "              learning_rate=0.001, grad_clip=5, sampling=False):\n",
    "    \n",
    "    # When we're using this network for sampling later, we'll be passing in\n",
    "    # one character at a time, so providing an option for that\n",
    "    if sampling == True:\n",
    "        batch_size, num_steps = 1, 1\n",
    "\n",
    "    tf.reset_default_graph()\n",
    "    \n",
    "    # Declare placeholders we'll feed into the graph\n",
    "    inputs = tf.placeholder(tf.int32, [batch_size, num_steps], name='inputs')\n",
    "    targets = tf.placeholder(tf.int32, [batch_size, num_steps], name='targets')\n",
    "    \n",
    "    # Keep probability placeholder for drop out layers\n",
    "    keep_prob = tf.placeholder(tf.float32, name='keep_prob')\n",
    "    \n",
    "    # One-hot encoding the input and target characters\n",
    "    x_one_hot = tf.one_hot(inputs, num_classes)\n",
    "    y_one_hot = tf.one_hot(targets, num_classes)\n",
    "\n",
    "    ### Build the RNN layers\n",
    "    # Use a basic LSTM cell\n",
    "    lstm = tf.contrib.rnn.BasicLSTMCell(lstm_size)\n",
    "    \n",
    "    # Add dropout to the cell\n",
    "    drop = tf.contrib.rnn.DropoutWrapper(lstm, output_keep_prob=keep_prob)\n",
    "    \n",
    "    # Stack up multiple LSTM layers, for deep learning\n",
    "    cell = tf.contrib.rnn.MultiRNNCell([drop] * num_layers)\n",
    "    initial_state = cell.zero_state(batch_size, tf.float32)\n",
    "\n",
    "    ### Run the data through the RNN layers\n",
    "    # This makes a list where each element is on step in the sequence\n",
    "    rnn_inputs = [tf.squeeze(i, squeeze_dims=[1]) for i in tf.split(x_one_hot, num_steps, 1)]\n",
    "    \n",
    "    # Run each sequence step through the RNN and collect the outputs\n",
    "    outputs, state = tf.contrib.rnn.static_rnn(cell, rnn_inputs, initial_state=initial_state)\n",
    "    final_state = state\n",
    "    \n",
    "    # Reshape output so it's a bunch of rows, one output row for each step for each batch\n",
    "    seq_output = tf.concat(outputs, axis=1)\n",
    "    output = tf.reshape(seq_output, [-1, lstm_size])\n",
    "    \n",
    "    # Now connect the RNN putputs to a softmax layer\n",
    "    with tf.variable_scope('softmax'):\n",
    "        softmax_w = tf.Variable(tf.truncated_normal((lstm_size, num_classes), stddev=0.1))\n",
    "        softmax_b = tf.Variable(tf.zeros(num_classes))\n",
    "    \n",
    "    # Since output is a bunch of rows of RNN cell outputs, logits will be a bunch\n",
    "    # of rows of logit outputs, one for each step and batch\n",
    "    logits = tf.matmul(output, softmax_w) + softmax_b\n",
    "    \n",
    "    # Use softmax to get the probabilities for predicted characters\n",
    "    preds = tf.nn.softmax(logits, name='predictions')\n",
    "    \n",
    "    # Reshape the targets to match the logits\n",
    "    y_reshaped = tf.reshape(y_one_hot, [-1, num_classes])\n",
    "    loss = tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=y_reshaped)\n",
    "    cost = tf.reduce_mean(loss)\n",
    "\n",
    "    # Optimizer for training, using gradient clipping to control exploding gradients\n",
    "    tvars = tf.trainable_variables()\n",
    "    grads, _ = tf.clip_by_global_norm(tf.gradients(cost, tvars), grad_clip)\n",
    "    train_op = tf.train.AdamOptimizer(learning_rate)\n",
    "    optimizer = train_op.apply_gradients(zip(grads, tvars))\n",
    "    \n",
    "    # Export the nodes\n",
    "    # NOTE: I'm using a namedtuple here because I think they are cool\n",
    "    export_nodes = ['inputs', 'targets', 'initial_state', 'final_state',\n",
    "                    'keep_prob', 'cost', 'preds', 'optimizer']\n",
    "    Graph = namedtuple('Graph', export_nodes)\n",
    "    local_dict = locals()\n",
    "    graph = Graph(*[local_dict[each] for each in export_nodes])\n",
    "    \n",
    "    return graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameters\n",
    "\n",
    "Here I'm defining the hyperparameters for the network. \n",
    "\n",
    "* `batch_size` - Number of sequences running through the network in one pass.\n",
    "* `num_steps` - Number of characters in the sequence the network is trained on. Larger is better typically, the network will learn more long range dependencies. But it takes longer to train. 100 is typically a good number here.\n",
    "* `lstm_size` - The number of units in the hidden layers.\n",
    "* `num_layers` - Number of hidden LSTM layers to use\n",
    "* `learning_rate` - Learning rate for training\n",
    "* `keep_prob` - The dropout keep probability when training. If you're network is overfitting, try decreasing this.\n",
    "\n",
    "Here's some good advice from Andrej Karpathy on training the network. I'm going to write it in here for your benefit, but also link to [where it originally came from](https://github.com/karpathy/char-rnn#tips-and-tricks).\n",
    "\n",
    "> ## Tips and Tricks\n",
    "\n",
    ">### Monitoring Validation Loss vs. Training Loss\n",
    ">If you're somewhat new to Machine Learning or Neural Networks it can take a bit of expertise to get good models. The most important quantity to keep track of is the difference between your training loss (printed during training) and the validation loss (printed once in a while when the RNN is run on the validation data (by default every 1000 iterations)). In particular:\n",
    "\n",
    "> - If your training loss is much lower than validation loss then this means the network might be **overfitting**. Solutions to this are to decrease your network size, or to increase dropout. For example you could try dropout of 0.5 and so on.\n",
    "> - If your training/validation loss are about equal then your model is **underfitting**. Increase the size of your model (either number of layers or the raw number of neurons per layer)\n",
    "\n",
    "> ### Approximate number of parameters\n",
    "\n",
    "> The two most important parameters that control the model are `lstm_size` and `num_layers`. I would advise that you always use `num_layers` of either 2/3. The `lstm_size` can be adjusted based on how much data you have. The two important quantities to keep track of here are:\n",
    "\n",
    "> - The number of parameters in your model. This is printed when you start training.\n",
    "> - The size of your dataset. 1MB file is approximately 1 million characters.\n",
    "\n",
    ">These two should be about the same order of magnitude. It's a little tricky to tell. Here are some examples:\n",
    "\n",
    "> - I have a 100MB dataset and I'm using the default parameter settings (which currently print 150K parameters). My data size is significantly larger (100 mil >> 0.15 mil), so I expect to heavily underfit. I am thinking I can comfortably afford to make `lstm_size` larger.\n",
    "> - I have a 10MB dataset and running a 10 million parameter model. I'm slightly nervous and I'm carefully monitoring my validation loss. If it's larger than my training loss then I may want to try to increase dropout a bit and see if that heps the validation loss.\n",
    "\n",
    "> ### Best models strategy\n",
    "\n",
    ">The winning strategy to obtaining very good models (if you have the compute time) is to always err on making the network larger (as large as you're willing to wait for it to compute) and then try different dropout values (between 0,1). Whatever model has the best validation performance (the loss, written in the checkpoint filename, low is good) is the one you should use in the end.\n",
    "\n",
    ">It is very common in deep learning to run many different models with many different hyperparameter settings, and in the end take whatever checkpoint gave the best validation performance.\n",
    "\n",
    ">By the way, the size of your training and validation splits are also parameters. Make sure you have a decent amount of data in your validation set or otherwise the validation performance will be noisy and not very informative.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "batch_size = 100\n",
    "num_steps = 100 \n",
    "lstm_size = 512\n",
    "num_layers = 2\n",
    "learning_rate = 0.001\n",
    "keep_prob = 0.75"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training\n",
    "\n",
    "Time for training which is pretty straightforward. Here I pass in some data, and get an LSTM state back. Then I pass that state back in to the network so the next batch can continue the state from the previous batch. And every so often (set by `save_every_n`) I calculate the validation loss and save a checkpoint.\n",
    "\n",
    "Here I'm saving checkpoints with the format\n",
    "\n",
    "`i{iteration number}_l{# hidden layer units}_v{validation loss}.ckpt`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2000  Iteration 1/4000 Training loss: 4.2311 2.0077 sec/batch\n",
      "Epoch 1/2000  Iteration 2/4000 Training loss: 4.1713 0.4441 sec/batch\n",
      "Epoch 2/2000  Iteration 3/4000 Training loss: 3.6745 0.4440 sec/batch\n",
      "Epoch 2/2000  Iteration 4/4000 Training loss: 3.6576 0.4437 sec/batch\n",
      "Epoch 3/2000  Iteration 5/4000 Training loss: 3.6714 0.4434 sec/batch\n",
      "Epoch 3/2000  Iteration 6/4000 Training loss: 3.6340 0.4433 sec/batch\n",
      "Epoch 4/2000  Iteration 7/4000 Training loss: 3.4364 0.4429 sec/batch\n",
      "Epoch 4/2000  Iteration 8/4000 Training loss: 3.2962 0.4441 sec/batch\n",
      "Epoch 5/2000  Iteration 9/4000 Training loss: 3.2580 0.4429 sec/batch\n",
      "Epoch 5/2000  Iteration 10/4000 Training loss: 3.2520 0.4444 sec/batch\n",
      "Epoch 6/2000  Iteration 11/4000 Training loss: 3.2641 0.4436 sec/batch\n",
      "Epoch 6/2000  Iteration 12/4000 Training loss: 3.1867 0.4433 sec/batch\n",
      "Epoch 7/2000  Iteration 13/4000 Training loss: 3.1874 0.4430 sec/batch\n",
      "Epoch 7/2000  Iteration 14/4000 Training loss: 3.1415 0.4432 sec/batch\n",
      "Epoch 8/2000  Iteration 15/4000 Training loss: 3.1875 0.4438 sec/batch\n",
      "Epoch 8/2000  Iteration 16/4000 Training loss: 3.1469 0.4430 sec/batch\n",
      "Epoch 9/2000  Iteration 17/4000 Training loss: 3.1727 0.4432 sec/batch\n",
      "Epoch 9/2000  Iteration 18/4000 Training loss: 3.1298 0.4435 sec/batch\n",
      "Epoch 10/2000  Iteration 19/4000 Training loss: 3.1486 0.4440 sec/batch\n",
      "Epoch 10/2000  Iteration 20/4000 Training loss: 3.1130 0.4431 sec/batch\n",
      "Epoch 11/2000  Iteration 21/4000 Training loss: 3.1311 0.4428 sec/batch\n",
      "Epoch 11/2000  Iteration 22/4000 Training loss: 3.0987 0.4431 sec/batch\n",
      "Epoch 12/2000  Iteration 23/4000 Training loss: 3.1249 0.4435 sec/batch\n",
      "Epoch 12/2000  Iteration 24/4000 Training loss: 3.0933 0.4439 sec/batch\n",
      "Epoch 13/2000  Iteration 25/4000 Training loss: 3.1220 0.4427 sec/batch\n",
      "Epoch 13/2000  Iteration 26/4000 Training loss: 3.0917 0.4435 sec/batch\n",
      "Epoch 14/2000  Iteration 27/4000 Training loss: 3.1139 0.4432 sec/batch\n",
      "Epoch 14/2000  Iteration 28/4000 Training loss: 3.0808 0.4433 sec/batch\n",
      "Epoch 15/2000  Iteration 29/4000 Training loss: 3.1127 0.4433 sec/batch\n",
      "Epoch 15/2000  Iteration 30/4000 Training loss: 3.0789 0.4435 sec/batch\n",
      "Epoch 16/2000  Iteration 31/4000 Training loss: 3.1060 0.4434 sec/batch\n",
      "Epoch 16/2000  Iteration 32/4000 Training loss: 3.0768 0.4438 sec/batch\n",
      "Epoch 17/2000  Iteration 33/4000 Training loss: 3.1073 0.4429 sec/batch\n",
      "Epoch 17/2000  Iteration 34/4000 Training loss: 3.0723 0.4430 sec/batch\n",
      "Epoch 18/2000  Iteration 35/4000 Training loss: 3.0984 0.4434 sec/batch\n",
      "Epoch 18/2000  Iteration 36/4000 Training loss: 3.0673 0.4429 sec/batch\n",
      "Epoch 19/2000  Iteration 37/4000 Training loss: 3.0960 0.4437 sec/batch\n",
      "Epoch 19/2000  Iteration 38/4000 Training loss: 3.0668 0.4435 sec/batch\n",
      "Epoch 20/2000  Iteration 39/4000 Training loss: 3.0878 0.4433 sec/batch\n",
      "Epoch 20/2000  Iteration 40/4000 Training loss: 3.0624 0.4433 sec/batch\n",
      "Epoch 21/2000  Iteration 41/4000 Training loss: 3.0845 0.4429 sec/batch\n",
      "Epoch 21/2000  Iteration 42/4000 Training loss: 3.0599 0.4430 sec/batch\n",
      "Epoch 22/2000  Iteration 43/4000 Training loss: 3.0840 0.4431 sec/batch\n",
      "Epoch 22/2000  Iteration 44/4000 Training loss: 3.0579 0.4439 sec/batch\n",
      "Epoch 23/2000  Iteration 45/4000 Training loss: 3.0814 0.4429 sec/batch\n",
      "Epoch 23/2000  Iteration 46/4000 Training loss: 3.0526 0.4434 sec/batch\n",
      "Epoch 24/2000  Iteration 47/4000 Training loss: 3.0787 0.4433 sec/batch\n",
      "Epoch 24/2000  Iteration 48/4000 Training loss: 3.0483 0.4433 sec/batch\n",
      "Epoch 25/2000  Iteration 49/4000 Training loss: 3.0713 0.4431 sec/batch\n",
      "Epoch 25/2000  Iteration 50/4000 Training loss: 3.0457 0.4432 sec/batch\n",
      "Epoch 26/2000  Iteration 51/4000 Training loss: 3.0690 0.4432 sec/batch\n",
      "Epoch 26/2000  Iteration 52/4000 Training loss: 3.0439 0.4437 sec/batch\n",
      "Epoch 27/2000  Iteration 53/4000 Training loss: 3.0661 0.4437 sec/batch\n",
      "Epoch 27/2000  Iteration 54/4000 Training loss: 3.0381 0.4433 sec/batch\n",
      "Epoch 28/2000  Iteration 55/4000 Training loss: 3.0579 0.4432 sec/batch\n",
      "Epoch 28/2000  Iteration 56/4000 Training loss: 3.0325 0.4437 sec/batch\n",
      "Epoch 29/2000  Iteration 57/4000 Training loss: 3.0582 0.4427 sec/batch\n",
      "Epoch 29/2000  Iteration 58/4000 Training loss: 3.0293 0.4437 sec/batch\n",
      "Epoch 30/2000  Iteration 59/4000 Training loss: 3.0540 0.4437 sec/batch\n",
      "Epoch 30/2000  Iteration 60/4000 Training loss: 3.0285 0.4439 sec/batch\n",
      "Epoch 31/2000  Iteration 61/4000 Training loss: 3.0515 0.4430 sec/batch\n",
      "Epoch 31/2000  Iteration 62/4000 Training loss: 3.0412 0.4437 sec/batch\n",
      "Epoch 32/2000  Iteration 63/4000 Training loss: 3.0427 0.4429 sec/batch\n",
      "Epoch 32/2000  Iteration 64/4000 Training loss: 3.0201 0.4442 sec/batch\n",
      "Epoch 33/2000  Iteration 65/4000 Training loss: 3.0432 0.4434 sec/batch\n",
      "Epoch 33/2000  Iteration 66/4000 Training loss: 3.0189 0.4438 sec/batch\n",
      "Epoch 34/2000  Iteration 67/4000 Training loss: 3.0353 0.4431 sec/batch\n",
      "Epoch 34/2000  Iteration 68/4000 Training loss: 3.0118 0.4439 sec/batch\n",
      "Epoch 35/2000  Iteration 69/4000 Training loss: 3.0363 0.4435 sec/batch\n",
      "Epoch 35/2000  Iteration 70/4000 Training loss: 3.0101 0.4435 sec/batch\n",
      "Epoch 36/2000  Iteration 71/4000 Training loss: 3.0261 0.4434 sec/batch\n",
      "Epoch 36/2000  Iteration 72/4000 Training loss: 3.0028 0.4435 sec/batch\n",
      "Epoch 37/2000  Iteration 73/4000 Training loss: 3.0205 0.4432 sec/batch\n",
      "Epoch 37/2000  Iteration 74/4000 Training loss: 2.9964 0.4430 sec/batch\n",
      "Epoch 38/2000  Iteration 75/4000 Training loss: 3.0095 0.4432 sec/batch\n",
      "Epoch 38/2000  Iteration 76/4000 Training loss: 2.9848 0.4437 sec/batch\n",
      "Epoch 39/2000  Iteration 77/4000 Training loss: 3.0027 0.4437 sec/batch\n",
      "Epoch 39/2000  Iteration 78/4000 Training loss: 2.9766 0.4434 sec/batch\n",
      "Epoch 40/2000  Iteration 79/4000 Training loss: 2.9915 0.4442 sec/batch\n",
      "Epoch 40/2000  Iteration 80/4000 Training loss: 2.9614 0.4435 sec/batch\n",
      "Epoch 41/2000  Iteration 81/4000 Training loss: 2.9831 0.4435 sec/batch\n",
      "Epoch 41/2000  Iteration 82/4000 Training loss: 2.9656 0.4434 sec/batch\n",
      "Epoch 42/2000  Iteration 83/4000 Training loss: 2.9777 0.4439 sec/batch\n",
      "Epoch 42/2000  Iteration 84/4000 Training loss: 2.9490 0.4433 sec/batch\n",
      "Epoch 43/2000  Iteration 85/4000 Training loss: 2.9602 0.4438 sec/batch\n",
      "Epoch 43/2000  Iteration 86/4000 Training loss: 2.9352 0.4433 sec/batch\n",
      "Epoch 44/2000  Iteration 87/4000 Training loss: 2.9449 0.4435 sec/batch\n",
      "Epoch 44/2000  Iteration 88/4000 Training loss: 2.9108 0.4434 sec/batch\n",
      "Epoch 45/2000  Iteration 89/4000 Training loss: 2.9415 0.4435 sec/batch\n",
      "Epoch 45/2000  Iteration 90/4000 Training loss: 2.9617 0.4450 sec/batch\n",
      "Epoch 46/2000  Iteration 91/4000 Training loss: 2.9434 0.4432 sec/batch\n",
      "Epoch 46/2000  Iteration 92/4000 Training loss: 2.9161 0.4433 sec/batch\n",
      "Epoch 47/2000  Iteration 93/4000 Training loss: 2.9361 0.4428 sec/batch\n",
      "Epoch 47/2000  Iteration 94/4000 Training loss: 2.9129 0.4434 sec/batch\n",
      "Epoch 48/2000  Iteration 95/4000 Training loss: 2.9160 0.4432 sec/batch\n",
      "Epoch 48/2000  Iteration 96/4000 Training loss: 2.8892 0.4435 sec/batch\n",
      "Epoch 49/2000  Iteration 97/4000 Training loss: 2.9123 0.4432 sec/batch\n",
      "Epoch 49/2000  Iteration 98/4000 Training loss: 2.8820 0.4434 sec/batch\n",
      "Epoch 50/2000  Iteration 99/4000 Training loss: 2.8883 0.4431 sec/batch\n",
      "Epoch 50/2000  Iteration 100/4000 Training loss: 2.8590 0.4432 sec/batch\n",
      "Epoch 51/2000  Iteration 101/4000 Training loss: 2.8659 0.4428 sec/batch\n",
      "Epoch 51/2000  Iteration 102/4000 Training loss: 2.8341 0.4435 sec/batch\n",
      "Epoch 52/2000  Iteration 103/4000 Training loss: 2.8338 0.4435 sec/batch\n",
      "Epoch 52/2000  Iteration 104/4000 Training loss: 2.8135 0.4436 sec/batch\n",
      "Epoch 53/2000  Iteration 105/4000 Training loss: 2.8335 0.4438 sec/batch\n",
      "Epoch 53/2000  Iteration 106/4000 Training loss: 2.8029 0.4432 sec/batch\n",
      "Epoch 54/2000  Iteration 107/4000 Training loss: 2.8181 0.4429 sec/batch\n",
      "Epoch 54/2000  Iteration 108/4000 Training loss: 2.7812 0.4437 sec/batch\n",
      "Epoch 55/2000  Iteration 109/4000 Training loss: 2.7750 0.4428 sec/batch\n",
      "Epoch 55/2000  Iteration 110/4000 Training loss: 2.7480 0.4442 sec/batch\n",
      "Epoch 56/2000  Iteration 111/4000 Training loss: 2.7706 0.4434 sec/batch\n",
      "Epoch 56/2000  Iteration 112/4000 Training loss: 2.7360 0.4436 sec/batch\n",
      "Epoch 57/2000  Iteration 113/4000 Training loss: 2.7788 0.4433 sec/batch\n",
      "Epoch 57/2000  Iteration 114/4000 Training loss: 2.7360 0.4434 sec/batch\n",
      "Epoch 58/2000  Iteration 115/4000 Training loss: 2.7569 0.4432 sec/batch\n",
      "Epoch 58/2000  Iteration 116/4000 Training loss: 2.7225 0.4435 sec/batch\n",
      "Epoch 59/2000  Iteration 117/4000 Training loss: 2.7130 0.4432 sec/batch\n",
      "Epoch 59/2000  Iteration 118/4000 Training loss: 2.6882 0.4432 sec/batch\n",
      "Epoch 60/2000  Iteration 119/4000 Training loss: 2.7018 0.4429 sec/batch\n",
      "Epoch 60/2000  Iteration 120/4000 Training loss: 2.6642 0.4436 sec/batch\n",
      "Epoch 61/2000  Iteration 121/4000 Training loss: 2.6759 0.4441 sec/batch\n",
      "Epoch 61/2000  Iteration 122/4000 Training loss: 2.6442 0.4435 sec/batch\n",
      "Epoch 62/2000  Iteration 123/4000 Training loss: 2.6535 0.4431 sec/batch\n",
      "Epoch 62/2000  Iteration 124/4000 Training loss: 2.6210 0.4434 sec/batch\n",
      "Epoch 63/2000  Iteration 125/4000 Training loss: 2.6284 0.4429 sec/batch\n",
      "Epoch 63/2000  Iteration 126/4000 Training loss: 2.5970 0.4434 sec/batch\n",
      "Epoch 64/2000  Iteration 127/4000 Training loss: 2.6038 0.4434 sec/batch\n",
      "Epoch 64/2000  Iteration 128/4000 Training loss: 2.5713 0.4433 sec/batch\n",
      "Epoch 65/2000  Iteration 129/4000 Training loss: 2.5802 0.4432 sec/batch\n",
      "Epoch 65/2000  Iteration 130/4000 Training loss: 2.5499 0.4435 sec/batch\n",
      "Epoch 66/2000  Iteration 131/4000 Training loss: 2.5731 0.4432 sec/batch\n",
      "Epoch 66/2000  Iteration 132/4000 Training loss: 2.5342 0.4432 sec/batch\n",
      "Epoch 67/2000  Iteration 133/4000 Training loss: 2.5463 0.4432 sec/batch\n",
      "Epoch 67/2000  Iteration 134/4000 Training loss: 2.5098 0.4436 sec/batch\n",
      "Epoch 68/2000  Iteration 135/4000 Training loss: 2.5350 0.4433 sec/batch\n",
      "Epoch 68/2000  Iteration 136/4000 Training loss: 2.4993 0.4436 sec/batch\n",
      "Epoch 69/2000  Iteration 137/4000 Training loss: 2.5226 0.4429 sec/batch\n",
      "Epoch 69/2000  Iteration 138/4000 Training loss: 2.4822 0.4434 sec/batch\n",
      "Epoch 70/2000  Iteration 139/4000 Training loss: 2.5094 0.4432 sec/batch\n",
      "Epoch 70/2000  Iteration 140/4000 Training loss: 2.4665 0.4433 sec/batch\n",
      "Epoch 71/2000  Iteration 141/4000 Training loss: 2.4976 0.4431 sec/batch\n",
      "Epoch 71/2000  Iteration 142/4000 Training loss: 2.4551 0.4442 sec/batch\n",
      "Epoch 72/2000  Iteration 143/4000 Training loss: 2.4801 0.4435 sec/batch\n",
      "Epoch 72/2000  Iteration 144/4000 Training loss: 2.4386 0.4442 sec/batch\n",
      "Epoch 73/2000  Iteration 145/4000 Training loss: 2.4731 0.4436 sec/batch\n",
      "Epoch 73/2000  Iteration 146/4000 Training loss: 2.4348 0.4434 sec/batch\n",
      "Epoch 74/2000  Iteration 147/4000 Training loss: 2.4733 0.4433 sec/batch\n",
      "Epoch 74/2000  Iteration 148/4000 Training loss: 2.4316 0.4440 sec/batch\n",
      "Epoch 75/2000  Iteration 149/4000 Training loss: 2.4537 0.4432 sec/batch\n",
      "Epoch 75/2000  Iteration 150/4000 Training loss: 2.4151 0.4442 sec/batch\n",
      "Epoch 76/2000  Iteration 151/4000 Training loss: 2.4508 0.4433 sec/batch\n",
      "Epoch 76/2000  Iteration 152/4000 Training loss: 2.4075 0.4450 sec/batch\n",
      "Epoch 77/2000  Iteration 153/4000 Training loss: 2.4338 0.4435 sec/batch\n",
      "Epoch 77/2000  Iteration 154/4000 Training loss: 2.3931 0.4435 sec/batch\n",
      "Epoch 78/2000  Iteration 155/4000 Training loss: 2.4349 0.4431 sec/batch\n",
      "Epoch 78/2000  Iteration 156/4000 Training loss: 2.3913 0.4436 sec/batch\n",
      "Epoch 79/2000  Iteration 157/4000 Training loss: 2.4190 0.4432 sec/batch\n",
      "Epoch 79/2000  Iteration 158/4000 Training loss: 2.3782 0.4433 sec/batch\n",
      "Epoch 80/2000  Iteration 159/4000 Training loss: 2.4081 0.4440 sec/batch\n",
      "Epoch 80/2000  Iteration 160/4000 Training loss: 2.3650 0.4435 sec/batch\n",
      "Epoch 81/2000  Iteration 161/4000 Training loss: 2.3968 0.4434 sec/batch\n",
      "Epoch 81/2000  Iteration 162/4000 Training loss: 2.3589 0.4438 sec/batch\n",
      "Epoch 82/2000  Iteration 163/4000 Training loss: 2.3897 0.4434 sec/batch\n",
      "Epoch 82/2000  Iteration 164/4000 Training loss: 2.3512 0.4432 sec/batch\n",
      "Epoch 83/2000  Iteration 165/4000 Training loss: 2.3902 0.4431 sec/batch\n",
      "Epoch 83/2000  Iteration 166/4000 Training loss: 2.3484 0.4432 sec/batch\n",
      "Epoch 84/2000  Iteration 167/4000 Training loss: 2.3956 0.4435 sec/batch\n",
      "Epoch 84/2000  Iteration 168/4000 Training loss: 2.3885 0.4436 sec/batch\n",
      "Epoch 85/2000  Iteration 169/4000 Training loss: 2.3998 0.4438 sec/batch\n",
      "Epoch 85/2000  Iteration 170/4000 Training loss: 2.3628 0.4436 sec/batch\n",
      "Epoch 86/2000  Iteration 171/4000 Training loss: 2.3944 0.4431 sec/batch\n",
      "Epoch 86/2000  Iteration 172/4000 Training loss: 2.3543 0.4433 sec/batch\n",
      "Epoch 87/2000  Iteration 173/4000 Training loss: 2.3787 0.4430 sec/batch\n",
      "Epoch 87/2000  Iteration 174/4000 Training loss: 2.3449 0.4435 sec/batch\n",
      "Epoch 88/2000  Iteration 175/4000 Training loss: 2.3707 0.4434 sec/batch\n",
      "Epoch 88/2000  Iteration 176/4000 Training loss: 2.3350 0.4436 sec/batch\n",
      "Epoch 89/2000  Iteration 177/4000 Training loss: 2.3613 0.4435 sec/batch\n",
      "Epoch 89/2000  Iteration 178/4000 Training loss: 2.3226 0.4436 sec/batch\n",
      "Epoch 90/2000  Iteration 179/4000 Training loss: 2.3543 0.4433 sec/batch\n",
      "Epoch 90/2000  Iteration 180/4000 Training loss: 2.3163 0.4434 sec/batch\n",
      "Epoch 91/2000  Iteration 181/4000 Training loss: 2.3398 0.4430 sec/batch\n",
      "Epoch 91/2000  Iteration 182/4000 Training loss: 2.3017 0.4432 sec/batch\n",
      "Epoch 92/2000  Iteration 183/4000 Training loss: 2.3345 0.4431 sec/batch\n",
      "Epoch 92/2000  Iteration 184/4000 Training loss: 2.2950 0.4437 sec/batch\n",
      "Epoch 93/2000  Iteration 185/4000 Training loss: 2.3275 0.4432 sec/batch\n",
      "Epoch 93/2000  Iteration 186/4000 Training loss: 2.2940 0.4437 sec/batch\n",
      "Epoch 94/2000  Iteration 187/4000 Training loss: 2.3207 0.4428 sec/batch\n",
      "Epoch 94/2000  Iteration 188/4000 Training loss: 2.2772 0.4435 sec/batch\n",
      "Epoch 95/2000  Iteration 189/4000 Training loss: 2.3083 0.4434 sec/batch\n",
      "Epoch 95/2000  Iteration 190/4000 Training loss: 2.2686 0.4429 sec/batch\n",
      "Epoch 96/2000  Iteration 191/4000 Training loss: 2.2994 0.4430 sec/batch\n",
      "Epoch 96/2000  Iteration 192/4000 Training loss: 2.2604 0.4432 sec/batch\n",
      "Epoch 97/2000  Iteration 193/4000 Training loss: 2.2899 0.4434 sec/batch\n",
      "Epoch 97/2000  Iteration 194/4000 Training loss: 2.2560 0.4437 sec/batch\n",
      "Epoch 98/2000  Iteration 195/4000 Training loss: 2.2894 0.4433 sec/batch\n",
      "Epoch 98/2000  Iteration 196/4000 Training loss: 2.2497 0.4432 sec/batch\n",
      "Epoch 99/2000  Iteration 197/4000 Training loss: 2.2737 0.4433 sec/batch\n",
      "Epoch 99/2000  Iteration 198/4000 Training loss: 2.2391 0.4435 sec/batch\n",
      "Epoch 100/2000  Iteration 199/4000 Training loss: 2.2643 0.4433 sec/batch\n",
      "Epoch 100/2000  Iteration 200/4000 Training loss: 2.2325 0.4436 sec/batch\n",
      "Validation loss: 2.27513 Saving checkpoint!\n",
      "Epoch 101/2000  Iteration 201/4000 Training loss: 2.2551 0.4440 sec/batch\n",
      "Epoch 101/2000  Iteration 202/4000 Training loss: 2.2229 0.4438 sec/batch\n",
      "Epoch 102/2000  Iteration 203/4000 Training loss: 2.2619 0.4430 sec/batch\n",
      "Epoch 102/2000  Iteration 204/4000 Training loss: 2.2459 0.4433 sec/batch\n",
      "Epoch 103/2000  Iteration 205/4000 Training loss: 2.2554 0.4433 sec/batch\n",
      "Epoch 103/2000  Iteration 206/4000 Training loss: 2.2263 0.4448 sec/batch\n",
      "Epoch 104/2000  Iteration 207/4000 Training loss: 2.2497 0.4449 sec/batch\n",
      "Epoch 104/2000  Iteration 208/4000 Training loss: 2.2140 0.4436 sec/batch\n",
      "Epoch 105/2000  Iteration 209/4000 Training loss: 2.2467 0.4436 sec/batch\n",
      "Epoch 105/2000  Iteration 210/4000 Training loss: 2.2107 0.4436 sec/batch\n",
      "Epoch 106/2000  Iteration 211/4000 Training loss: 2.2339 0.4439 sec/batch\n",
      "Epoch 106/2000  Iteration 212/4000 Training loss: 2.1964 0.4435 sec/batch\n",
      "Epoch 107/2000  Iteration 213/4000 Training loss: 2.2220 0.4433 sec/batch\n",
      "Epoch 107/2000  Iteration 214/4000 Training loss: 2.1886 0.4436 sec/batch\n",
      "Epoch 108/2000  Iteration 215/4000 Training loss: 2.2128 0.4433 sec/batch\n",
      "Epoch 108/2000  Iteration 216/4000 Training loss: 2.1799 0.4442 sec/batch\n",
      "Epoch 109/2000  Iteration 217/4000 Training loss: 2.2088 0.4447 sec/batch\n",
      "Epoch 109/2000  Iteration 218/4000 Training loss: 2.1694 0.4438 sec/batch\n",
      "Epoch 110/2000  Iteration 219/4000 Training loss: 2.1992 0.4436 sec/batch\n",
      "Epoch 110/2000  Iteration 220/4000 Training loss: 2.1687 0.4435 sec/batch\n",
      "Epoch 111/2000  Iteration 221/4000 Training loss: 2.2075 0.4438 sec/batch\n",
      "Epoch 111/2000  Iteration 222/4000 Training loss: 2.1649 0.4434 sec/batch\n",
      "Epoch 112/2000  Iteration 223/4000 Training loss: 2.1863 0.4433 sec/batch\n",
      "Epoch 112/2000  Iteration 224/4000 Training loss: 2.1516 0.4436 sec/batch\n",
      "Epoch 113/2000  Iteration 225/4000 Training loss: 2.1764 0.4431 sec/batch\n",
      "Epoch 113/2000  Iteration 226/4000 Training loss: 2.1384 0.4431 sec/batch\n",
      "Epoch 114/2000  Iteration 227/4000 Training loss: 2.1665 0.4432 sec/batch\n",
      "Epoch 114/2000  Iteration 228/4000 Training loss: 2.1324 0.4435 sec/batch\n",
      "Epoch 115/2000  Iteration 229/4000 Training loss: 2.1641 0.4432 sec/batch\n",
      "Epoch 115/2000  Iteration 230/4000 Training loss: 2.1332 0.4434 sec/batch\n",
      "Epoch 116/2000  Iteration 231/4000 Training loss: 2.1504 0.4430 sec/batch\n",
      "Epoch 116/2000  Iteration 232/4000 Training loss: 2.1227 0.4433 sec/batch\n",
      "Epoch 117/2000  Iteration 233/4000 Training loss: 2.1698 0.4431 sec/batch\n",
      "Epoch 117/2000  Iteration 234/4000 Training loss: 2.1449 0.4445 sec/batch\n",
      "Epoch 118/2000  Iteration 235/4000 Training loss: 2.1577 0.4435 sec/batch\n",
      "Epoch 118/2000  Iteration 236/4000 Training loss: 2.1213 0.4433 sec/batch\n",
      "Epoch 119/2000  Iteration 237/4000 Training loss: 2.1386 0.4429 sec/batch\n",
      "Epoch 119/2000  Iteration 238/4000 Training loss: 2.1114 0.4431 sec/batch\n",
      "Epoch 120/2000  Iteration 239/4000 Training loss: 2.1266 0.4434 sec/batch\n",
      "Epoch 120/2000  Iteration 240/4000 Training loss: 2.0982 0.4434 sec/batch\n",
      "Epoch 121/2000  Iteration 241/4000 Training loss: 2.1229 0.4431 sec/batch\n",
      "Epoch 121/2000  Iteration 242/4000 Training loss: 2.0914 0.4436 sec/batch\n",
      "Epoch 122/2000  Iteration 243/4000 Training loss: 2.1133 0.4436 sec/batch\n",
      "Epoch 122/2000  Iteration 244/4000 Training loss: 2.0802 0.4435 sec/batch\n",
      "Epoch 123/2000  Iteration 245/4000 Training loss: 2.0999 0.4430 sec/batch\n",
      "Epoch 123/2000  Iteration 246/4000 Training loss: 2.0691 0.4431 sec/batch\n",
      "Epoch 124/2000  Iteration 247/4000 Training loss: 2.0888 0.4431 sec/batch\n",
      "Epoch 124/2000  Iteration 248/4000 Training loss: 2.0599 0.4433 sec/batch\n",
      "Epoch 125/2000  Iteration 249/4000 Training loss: 2.0866 0.4429 sec/batch\n",
      "Epoch 125/2000  Iteration 250/4000 Training loss: 2.0548 0.4437 sec/batch\n",
      "Epoch 126/2000  Iteration 251/4000 Training loss: 2.0934 0.4427 sec/batch\n",
      "Epoch 126/2000  Iteration 252/4000 Training loss: 2.0597 0.4436 sec/batch\n",
      "Epoch 127/2000  Iteration 253/4000 Training loss: 2.0685 0.4431 sec/batch\n",
      "Epoch 127/2000  Iteration 254/4000 Training loss: 2.0392 0.4432 sec/batch\n",
      "Epoch 128/2000  Iteration 255/4000 Training loss: 2.0568 0.4431 sec/batch\n",
      "Epoch 128/2000  Iteration 256/4000 Training loss: 2.0264 0.4433 sec/batch\n",
      "Epoch 129/2000  Iteration 257/4000 Training loss: 2.0509 0.4429 sec/batch\n",
      "Epoch 129/2000  Iteration 258/4000 Training loss: 2.0323 0.4430 sec/batch\n",
      "Epoch 130/2000  Iteration 259/4000 Training loss: 2.0564 0.4432 sec/batch\n",
      "Epoch 130/2000  Iteration 260/4000 Training loss: 2.0254 0.4433 sec/batch\n",
      "Epoch 131/2000  Iteration 261/4000 Training loss: 2.0400 0.4430 sec/batch\n",
      "Epoch 131/2000  Iteration 262/4000 Training loss: 2.0089 0.4434 sec/batch\n",
      "Epoch 132/2000  Iteration 263/4000 Training loss: 2.0241 0.4432 sec/batch\n",
      "Epoch 132/2000  Iteration 264/4000 Training loss: 1.9943 0.4432 sec/batch\n",
      "Epoch 133/2000  Iteration 265/4000 Training loss: 2.0222 0.4433 sec/batch\n",
      "Epoch 133/2000  Iteration 266/4000 Training loss: 1.9960 0.4439 sec/batch\n",
      "Epoch 134/2000  Iteration 267/4000 Training loss: 2.0053 0.4432 sec/batch\n",
      "Epoch 134/2000  Iteration 268/4000 Training loss: 1.9770 0.4433 sec/batch\n",
      "Epoch 135/2000  Iteration 269/4000 Training loss: 1.9956 0.4438 sec/batch\n",
      "Epoch 135/2000  Iteration 270/4000 Training loss: 1.9634 0.4432 sec/batch\n",
      "Epoch 136/2000  Iteration 271/4000 Training loss: 1.9797 0.4432 sec/batch\n",
      "Epoch 136/2000  Iteration 272/4000 Training loss: 1.9483 0.4432 sec/batch\n",
      "Epoch 137/2000  Iteration 273/4000 Training loss: 1.9645 0.4432 sec/batch\n",
      "Epoch 137/2000  Iteration 274/4000 Training loss: 1.9355 0.4432 sec/batch\n",
      "Epoch 138/2000  Iteration 275/4000 Training loss: 1.9599 0.4430 sec/batch\n",
      "Epoch 138/2000  Iteration 276/4000 Training loss: 1.9286 0.4435 sec/batch\n",
      "Epoch 139/2000  Iteration 277/4000 Training loss: 1.9517 0.4428 sec/batch\n",
      "Epoch 139/2000  Iteration 278/4000 Training loss: 1.9209 0.4432 sec/batch\n",
      "Epoch 140/2000  Iteration 279/4000 Training loss: 1.9295 0.4437 sec/batch\n",
      "Epoch 140/2000  Iteration 280/4000 Training loss: 1.9079 0.4436 sec/batch\n",
      "Epoch 141/2000  Iteration 281/4000 Training loss: 1.9567 0.4429 sec/batch\n",
      "Epoch 141/2000  Iteration 282/4000 Training loss: 1.9271 0.4436 sec/batch\n",
      "Epoch 142/2000  Iteration 283/4000 Training loss: 1.9350 0.4431 sec/batch\n",
      "Epoch 142/2000  Iteration 284/4000 Training loss: 1.9017 0.4434 sec/batch\n",
      "Epoch 143/2000  Iteration 285/4000 Training loss: 1.9239 0.4429 sec/batch\n",
      "Epoch 143/2000  Iteration 286/4000 Training loss: 1.8942 0.4438 sec/batch\n",
      "Epoch 144/2000  Iteration 287/4000 Training loss: 1.8968 0.4428 sec/batch\n",
      "Epoch 144/2000  Iteration 288/4000 Training loss: 1.8696 0.4432 sec/batch\n",
      "Epoch 145/2000  Iteration 289/4000 Training loss: 1.8838 0.4434 sec/batch\n",
      "Epoch 145/2000  Iteration 290/4000 Training loss: 1.8572 0.4431 sec/batch\n",
      "Epoch 146/2000  Iteration 291/4000 Training loss: 1.8764 0.4427 sec/batch\n",
      "Epoch 146/2000  Iteration 292/4000 Training loss: 1.8493 0.4434 sec/batch\n",
      "Epoch 147/2000  Iteration 293/4000 Training loss: 1.8506 0.4432 sec/batch\n",
      "Epoch 147/2000  Iteration 294/4000 Training loss: 1.8406 0.4435 sec/batch\n",
      "Epoch 148/2000  Iteration 295/4000 Training loss: 1.8495 0.4428 sec/batch\n",
      "Epoch 148/2000  Iteration 296/4000 Training loss: 1.8267 0.4433 sec/batch\n",
      "Epoch 149/2000  Iteration 297/4000 Training loss: 1.8473 0.4431 sec/batch\n",
      "Epoch 149/2000  Iteration 298/4000 Training loss: 1.8136 0.4436 sec/batch\n",
      "Epoch 150/2000  Iteration 299/4000 Training loss: 1.8215 0.4432 sec/batch\n",
      "Epoch 150/2000  Iteration 300/4000 Training loss: 1.7912 0.4437 sec/batch\n",
      "Epoch 151/2000  Iteration 301/4000 Training loss: 1.8297 0.4434 sec/batch\n",
      "Epoch 151/2000  Iteration 302/4000 Training loss: 1.8090 0.4440 sec/batch\n",
      "Epoch 152/2000  Iteration 303/4000 Training loss: 1.8346 0.4429 sec/batch\n",
      "Epoch 152/2000  Iteration 304/4000 Training loss: 1.8033 0.4430 sec/batch\n",
      "Epoch 153/2000  Iteration 305/4000 Training loss: 1.8095 0.4434 sec/batch\n",
      "Epoch 153/2000  Iteration 306/4000 Training loss: 1.7773 0.4432 sec/batch\n",
      "Epoch 154/2000  Iteration 307/4000 Training loss: 1.7905 0.4428 sec/batch\n",
      "Epoch 154/2000  Iteration 308/4000 Training loss: 1.7587 0.4436 sec/batch\n",
      "Epoch 155/2000  Iteration 309/4000 Training loss: 1.7620 0.4431 sec/batch\n",
      "Epoch 155/2000  Iteration 310/4000 Training loss: 1.7341 0.4438 sec/batch\n",
      "Epoch 156/2000  Iteration 311/4000 Training loss: 1.7416 0.4433 sec/batch\n",
      "Epoch 156/2000  Iteration 312/4000 Training loss: 1.7101 0.4445 sec/batch\n",
      "Epoch 157/2000  Iteration 313/4000 Training loss: 1.7156 0.4434 sec/batch\n",
      "Epoch 157/2000  Iteration 314/4000 Training loss: 1.6920 0.4435 sec/batch\n",
      "Epoch 158/2000  Iteration 315/4000 Training loss: 1.7071 0.4433 sec/batch\n",
      "Epoch 158/2000  Iteration 316/4000 Training loss: 1.6797 0.4432 sec/batch\n",
      "Epoch 159/2000  Iteration 317/4000 Training loss: 1.7013 0.4430 sec/batch\n",
      "Epoch 159/2000  Iteration 318/4000 Training loss: 1.6841 0.4434 sec/batch\n",
      "Epoch 160/2000  Iteration 319/4000 Training loss: 1.7041 0.4433 sec/batch\n",
      "Epoch 160/2000  Iteration 320/4000 Training loss: 1.6751 0.4430 sec/batch\n",
      "Epoch 161/2000  Iteration 321/4000 Training loss: 1.6752 0.4428 sec/batch\n",
      "Epoch 161/2000  Iteration 322/4000 Training loss: 1.6588 0.4433 sec/batch\n",
      "Epoch 162/2000  Iteration 323/4000 Training loss: 1.6785 0.4430 sec/batch\n",
      "Epoch 162/2000  Iteration 324/4000 Training loss: 1.6530 0.4434 sec/batch\n",
      "Epoch 163/2000  Iteration 325/4000 Training loss: 1.6565 0.4431 sec/batch\n",
      "Epoch 163/2000  Iteration 326/4000 Training loss: 1.6238 0.4432 sec/batch\n",
      "Epoch 164/2000  Iteration 327/4000 Training loss: 1.6257 0.4432 sec/batch\n",
      "Epoch 164/2000  Iteration 328/4000 Training loss: 1.6014 0.4435 sec/batch\n",
      "Epoch 165/2000  Iteration 329/4000 Training loss: 1.5975 0.4428 sec/batch\n",
      "Epoch 165/2000  Iteration 330/4000 Training loss: 1.5817 0.4433 sec/batch\n",
      "Epoch 166/2000  Iteration 331/4000 Training loss: 1.5940 0.4433 sec/batch\n",
      "Epoch 166/2000  Iteration 332/4000 Training loss: 1.5753 0.4436 sec/batch\n",
      "Epoch 167/2000  Iteration 333/4000 Training loss: 1.5748 0.4428 sec/batch\n",
      "Epoch 167/2000  Iteration 334/4000 Training loss: 1.5412 0.4437 sec/batch\n",
      "Epoch 168/2000  Iteration 335/4000 Training loss: 1.5606 0.4431 sec/batch\n",
      "Epoch 168/2000  Iteration 336/4000 Training loss: 1.5299 0.4438 sec/batch\n",
      "Epoch 169/2000  Iteration 337/4000 Training loss: 1.5388 0.4433 sec/batch\n",
      "Epoch 169/2000  Iteration 338/4000 Training loss: 1.5111 0.4435 sec/batch\n",
      "Epoch 170/2000  Iteration 339/4000 Training loss: 1.5119 0.4429 sec/batch\n",
      "Epoch 170/2000  Iteration 340/4000 Training loss: 1.4863 0.4435 sec/batch\n",
      "Epoch 171/2000  Iteration 341/4000 Training loss: 1.4920 0.4435 sec/batch\n",
      "Epoch 171/2000  Iteration 342/4000 Training loss: 1.4608 0.4434 sec/batch\n",
      "Epoch 172/2000  Iteration 343/4000 Training loss: 1.4618 0.4435 sec/batch\n",
      "Epoch 172/2000  Iteration 344/4000 Training loss: 1.4348 0.4434 sec/batch\n",
      "Epoch 173/2000  Iteration 345/4000 Training loss: 1.4417 0.4430 sec/batch\n",
      "Epoch 173/2000  Iteration 346/4000 Training loss: 1.4113 0.4432 sec/batch\n",
      "Epoch 174/2000  Iteration 347/4000 Training loss: 1.4209 0.4435 sec/batch\n",
      "Epoch 174/2000  Iteration 348/4000 Training loss: 1.3902 0.4436 sec/batch\n",
      "Epoch 175/2000  Iteration 349/4000 Training loss: 1.4027 0.4428 sec/batch\n",
      "Epoch 175/2000  Iteration 350/4000 Training loss: 1.3645 0.4432 sec/batch\n",
      "Epoch 176/2000  Iteration 351/4000 Training loss: 1.3868 0.4429 sec/batch\n",
      "Epoch 176/2000  Iteration 352/4000 Training loss: 1.3441 0.4430 sec/batch\n",
      "Epoch 177/2000  Iteration 353/4000 Training loss: 1.3435 0.4432 sec/batch\n",
      "Epoch 177/2000  Iteration 354/4000 Training loss: 1.3115 0.4432 sec/batch\n",
      "Epoch 178/2000  Iteration 355/4000 Training loss: 1.3434 0.4433 sec/batch\n",
      "Epoch 178/2000  Iteration 356/4000 Training loss: 1.3007 0.4435 sec/batch\n",
      "Epoch 179/2000  Iteration 357/4000 Training loss: 1.2939 0.4431 sec/batch\n",
      "Epoch 179/2000  Iteration 358/4000 Training loss: 1.2662 0.4434 sec/batch\n",
      "Epoch 180/2000  Iteration 359/4000 Training loss: 1.2899 0.4430 sec/batch\n",
      "Epoch 180/2000  Iteration 360/4000 Training loss: 1.2705 0.4434 sec/batch\n",
      "Epoch 181/2000  Iteration 361/4000 Training loss: 1.2924 0.4432 sec/batch\n",
      "Epoch 181/2000  Iteration 362/4000 Training loss: 1.2654 0.4432 sec/batch\n",
      "Epoch 182/2000  Iteration 363/4000 Training loss: 1.3085 0.4428 sec/batch\n",
      "Epoch 182/2000  Iteration 364/4000 Training loss: 1.2736 0.4433 sec/batch\n",
      "Epoch 183/2000  Iteration 365/4000 Training loss: 1.2907 0.4432 sec/batch\n",
      "Epoch 183/2000  Iteration 366/4000 Training loss: 1.2501 0.4433 sec/batch\n",
      "Epoch 184/2000  Iteration 367/4000 Training loss: 1.2348 0.4436 sec/batch\n",
      "Epoch 184/2000  Iteration 368/4000 Training loss: 1.1980 0.4438 sec/batch\n",
      "Epoch 185/2000  Iteration 369/4000 Training loss: 1.2008 0.4432 sec/batch\n",
      "Epoch 185/2000  Iteration 370/4000 Training loss: 1.1656 0.4435 sec/batch\n",
      "Epoch 186/2000  Iteration 371/4000 Training loss: 1.1859 0.4430 sec/batch\n",
      "Epoch 186/2000  Iteration 372/4000 Training loss: 1.1640 0.4437 sec/batch\n",
      "Epoch 187/2000  Iteration 373/4000 Training loss: 1.1644 0.4456 sec/batch\n",
      "Epoch 187/2000  Iteration 374/4000 Training loss: 1.1309 0.4431 sec/batch\n",
      "Epoch 188/2000  Iteration 375/4000 Training loss: 1.1406 0.4432 sec/batch\n",
      "Epoch 188/2000  Iteration 376/4000 Training loss: 1.1011 0.4434 sec/batch\n",
      "Epoch 189/2000  Iteration 377/4000 Training loss: 1.1039 0.4433 sec/batch\n",
      "Epoch 189/2000  Iteration 378/4000 Training loss: 1.0639 0.4432 sec/batch\n",
      "Epoch 190/2000  Iteration 379/4000 Training loss: 1.0824 0.4429 sec/batch\n",
      "Epoch 190/2000  Iteration 380/4000 Training loss: 1.0369 0.4436 sec/batch\n",
      "Epoch 191/2000  Iteration 381/4000 Training loss: 1.0547 0.4430 sec/batch\n",
      "Epoch 191/2000  Iteration 382/4000 Training loss: 1.0124 0.4433 sec/batch\n",
      "Epoch 192/2000  Iteration 383/4000 Training loss: 1.0299 0.4430 sec/batch\n",
      "Epoch 192/2000  Iteration 384/4000 Training loss: 0.9830 0.4431 sec/batch\n",
      "Epoch 193/2000  Iteration 385/4000 Training loss: 1.0099 0.4431 sec/batch\n",
      "Epoch 193/2000  Iteration 386/4000 Training loss: 0.9614 0.4434 sec/batch\n",
      "Epoch 194/2000  Iteration 387/4000 Training loss: 0.9668 0.4430 sec/batch\n",
      "Epoch 194/2000  Iteration 388/4000 Training loss: 0.9192 0.4431 sec/batch\n",
      "Epoch 195/2000  Iteration 389/4000 Training loss: 0.9451 0.4432 sec/batch\n",
      "Epoch 195/2000  Iteration 390/4000 Training loss: 0.9031 0.4437 sec/batch\n",
      "Epoch 196/2000  Iteration 391/4000 Training loss: 0.9271 0.4433 sec/batch\n",
      "Epoch 196/2000  Iteration 392/4000 Training loss: 0.8796 0.4434 sec/batch\n",
      "Epoch 197/2000  Iteration 393/4000 Training loss: 0.9112 0.4435 sec/batch\n",
      "Epoch 197/2000  Iteration 394/4000 Training loss: 0.8681 0.4436 sec/batch\n",
      "Epoch 198/2000  Iteration 395/4000 Training loss: 0.8841 0.4432 sec/batch\n",
      "Epoch 198/2000  Iteration 396/4000 Training loss: 0.8420 0.4435 sec/batch\n",
      "Epoch 199/2000  Iteration 397/4000 Training loss: 0.8543 0.4430 sec/batch\n",
      "Epoch 199/2000  Iteration 398/4000 Training loss: 0.8146 0.4434 sec/batch\n",
      "Epoch 200/2000  Iteration 399/4000 Training loss: 0.8454 0.4434 sec/batch\n",
      "Epoch 200/2000  Iteration 400/4000 Training loss: 0.7953 0.4434 sec/batch\n",
      "Validation loss: 2.89799 Saving checkpoint!\n",
      "Epoch 201/2000  Iteration 401/4000 Training loss: 0.8101 0.4432 sec/batch\n",
      "Epoch 201/2000  Iteration 402/4000 Training loss: 0.7577 0.4430 sec/batch\n",
      "Epoch 202/2000  Iteration 403/4000 Training loss: 0.7869 0.4429 sec/batch\n",
      "Epoch 202/2000  Iteration 404/4000 Training loss: 0.7323 0.4435 sec/batch\n",
      "Epoch 203/2000  Iteration 405/4000 Training loss: 0.7538 0.4429 sec/batch\n",
      "Epoch 203/2000  Iteration 406/4000 Training loss: 0.7092 0.4431 sec/batch\n",
      "Epoch 204/2000  Iteration 407/4000 Training loss: 0.7330 0.4430 sec/batch\n",
      "Epoch 204/2000  Iteration 408/4000 Training loss: 0.6899 0.4429 sec/batch\n",
      "Epoch 205/2000  Iteration 409/4000 Training loss: 0.7365 0.4430 sec/batch\n",
      "Epoch 205/2000  Iteration 410/4000 Training loss: 0.6776 0.4437 sec/batch\n",
      "Epoch 206/2000  Iteration 411/4000 Training loss: 0.7121 0.4429 sec/batch\n",
      "Epoch 206/2000  Iteration 412/4000 Training loss: 0.6639 0.4435 sec/batch\n",
      "Epoch 207/2000  Iteration 413/4000 Training loss: 0.6869 0.4430 sec/batch\n",
      "Epoch 207/2000  Iteration 414/4000 Training loss: 0.6442 0.4430 sec/batch\n",
      "Epoch 208/2000  Iteration 415/4000 Training loss: 0.6683 0.4428 sec/batch\n",
      "Epoch 208/2000  Iteration 416/4000 Training loss: 0.6169 0.4430 sec/batch\n",
      "Epoch 209/2000  Iteration 417/4000 Training loss: 0.6574 0.4427 sec/batch\n",
      "Epoch 209/2000  Iteration 418/4000 Training loss: 0.6157 0.4431 sec/batch\n",
      "Epoch 210/2000  Iteration 419/4000 Training loss: 0.6418 0.4428 sec/batch\n",
      "Epoch 210/2000  Iteration 420/4000 Training loss: 0.5947 0.4434 sec/batch\n",
      "Epoch 211/2000  Iteration 421/4000 Training loss: 0.6186 0.4435 sec/batch\n",
      "Epoch 211/2000  Iteration 422/4000 Training loss: 0.5699 0.4437 sec/batch\n",
      "Epoch 212/2000  Iteration 423/4000 Training loss: 0.5951 0.4433 sec/batch\n",
      "Epoch 212/2000  Iteration 424/4000 Training loss: 0.5563 0.4437 sec/batch\n",
      "Epoch 213/2000  Iteration 425/4000 Training loss: 0.5947 0.4431 sec/batch\n",
      "Epoch 213/2000  Iteration 426/4000 Training loss: 0.5444 0.4433 sec/batch\n",
      "Epoch 214/2000  Iteration 427/4000 Training loss: 0.5750 0.4430 sec/batch\n",
      "Epoch 214/2000  Iteration 428/4000 Training loss: 0.5249 0.4442 sec/batch\n",
      "Epoch 215/2000  Iteration 429/4000 Training loss: 0.5657 0.4432 sec/batch\n",
      "Epoch 215/2000  Iteration 430/4000 Training loss: 0.5146 0.4433 sec/batch\n",
      "Epoch 216/2000  Iteration 431/4000 Training loss: 0.5496 0.4436 sec/batch\n",
      "Epoch 216/2000  Iteration 432/4000 Training loss: 0.4950 0.4440 sec/batch\n",
      "Epoch 217/2000  Iteration 433/4000 Training loss: 0.5222 0.4433 sec/batch\n",
      "Epoch 217/2000  Iteration 434/4000 Training loss: 0.4780 0.4436 sec/batch\n",
      "Epoch 218/2000  Iteration 435/4000 Training loss: 0.5050 0.4428 sec/batch\n",
      "Epoch 218/2000  Iteration 436/4000 Training loss: 0.4522 0.4431 sec/batch\n",
      "Epoch 219/2000  Iteration 437/4000 Training loss: 0.4982 0.4432 sec/batch\n",
      "Epoch 219/2000  Iteration 438/4000 Training loss: 0.4432 0.4432 sec/batch\n",
      "Epoch 220/2000  Iteration 439/4000 Training loss: 0.4936 0.4433 sec/batch\n",
      "Epoch 220/2000  Iteration 440/4000 Training loss: 0.4341 0.4436 sec/batch\n",
      "Epoch 221/2000  Iteration 441/4000 Training loss: 0.4821 0.4434 sec/batch\n",
      "Epoch 221/2000  Iteration 442/4000 Training loss: 0.4247 0.4437 sec/batch\n",
      "Epoch 222/2000  Iteration 443/4000 Training loss: 0.4619 0.4428 sec/batch\n",
      "Epoch 222/2000  Iteration 444/4000 Training loss: 0.4120 0.4436 sec/batch\n",
      "Epoch 223/2000  Iteration 445/4000 Training loss: 0.4647 0.4434 sec/batch\n",
      "Epoch 223/2000  Iteration 446/4000 Training loss: 0.4138 0.4432 sec/batch\n",
      "Epoch 224/2000  Iteration 447/4000 Training loss: 0.4612 0.4431 sec/batch\n",
      "Epoch 224/2000  Iteration 448/4000 Training loss: 0.4055 0.4433 sec/batch\n",
      "Epoch 225/2000  Iteration 449/4000 Training loss: 0.4427 0.4431 sec/batch\n",
      "Epoch 225/2000  Iteration 450/4000 Training loss: 0.3906 0.4432 sec/batch\n",
      "Epoch 226/2000  Iteration 451/4000 Training loss: 0.4278 0.4432 sec/batch\n",
      "Epoch 226/2000  Iteration 452/4000 Training loss: 0.3837 0.4433 sec/batch\n",
      "Epoch 227/2000  Iteration 453/4000 Training loss: 0.4153 0.4429 sec/batch\n",
      "Epoch 227/2000  Iteration 454/4000 Training loss: 0.3610 0.4435 sec/batch\n",
      "Epoch 228/2000  Iteration 455/4000 Training loss: 0.4054 0.4435 sec/batch\n",
      "Epoch 228/2000  Iteration 456/4000 Training loss: 0.3577 0.4429 sec/batch\n",
      "Epoch 229/2000  Iteration 457/4000 Training loss: 0.3969 0.4429 sec/batch\n",
      "Epoch 229/2000  Iteration 458/4000 Training loss: 0.3455 0.4433 sec/batch\n",
      "Epoch 230/2000  Iteration 459/4000 Training loss: 0.3785 0.4430 sec/batch\n",
      "Epoch 230/2000  Iteration 460/4000 Training loss: 0.3274 0.4431 sec/batch\n",
      "Epoch 231/2000  Iteration 461/4000 Training loss: 0.3760 0.4432 sec/batch\n",
      "Epoch 231/2000  Iteration 462/4000 Training loss: 0.3258 0.4435 sec/batch\n",
      "Epoch 232/2000  Iteration 463/4000 Training loss: 0.3750 0.4427 sec/batch\n",
      "Epoch 232/2000  Iteration 464/4000 Training loss: 0.3175 0.4435 sec/batch\n",
      "Epoch 233/2000  Iteration 465/4000 Training loss: 0.3592 0.4442 sec/batch\n",
      "Epoch 233/2000  Iteration 466/4000 Training loss: 0.3066 0.4438 sec/batch\n",
      "Epoch 234/2000  Iteration 467/4000 Training loss: 0.3623 0.4436 sec/batch\n",
      "Epoch 234/2000  Iteration 468/4000 Training loss: 0.3018 0.4434 sec/batch\n",
      "Epoch 235/2000  Iteration 469/4000 Training loss: 0.3416 0.4428 sec/batch\n",
      "Epoch 235/2000  Iteration 470/4000 Training loss: 0.2928 0.4435 sec/batch\n",
      "Epoch 236/2000  Iteration 471/4000 Training loss: 0.3355 0.4431 sec/batch\n",
      "Epoch 236/2000  Iteration 472/4000 Training loss: 0.2833 0.4432 sec/batch\n",
      "Epoch 237/2000  Iteration 473/4000 Training loss: 0.3347 0.4432 sec/batch\n",
      "Epoch 237/2000  Iteration 474/4000 Training loss: 0.2811 0.4431 sec/batch\n",
      "Epoch 238/2000  Iteration 475/4000 Training loss: 0.3229 0.4432 sec/batch\n",
      "Epoch 238/2000  Iteration 476/4000 Training loss: 0.2731 0.4433 sec/batch\n",
      "Epoch 239/2000  Iteration 477/4000 Training loss: 0.3154 0.4431 sec/batch\n",
      "Epoch 239/2000  Iteration 478/4000 Training loss: 0.2650 0.4434 sec/batch\n",
      "Epoch 240/2000  Iteration 479/4000 Training loss: 0.3062 0.4429 sec/batch\n",
      "Epoch 240/2000  Iteration 480/4000 Training loss: 0.2585 0.4435 sec/batch\n",
      "Epoch 241/2000  Iteration 481/4000 Training loss: 0.2995 0.4430 sec/batch\n",
      "Epoch 241/2000  Iteration 482/4000 Training loss: 0.2524 0.4433 sec/batch\n",
      "Epoch 242/2000  Iteration 483/4000 Training loss: 0.2917 0.4428 sec/batch\n",
      "Epoch 242/2000  Iteration 484/4000 Training loss: 0.2448 0.4433 sec/batch\n",
      "Epoch 243/2000  Iteration 485/4000 Training loss: 0.2904 0.4434 sec/batch\n",
      "Epoch 243/2000  Iteration 486/4000 Training loss: 0.2400 0.4440 sec/batch\n",
      "Epoch 244/2000  Iteration 487/4000 Training loss: 0.2879 0.4431 sec/batch\n",
      "Epoch 244/2000  Iteration 488/4000 Training loss: 0.2366 0.4431 sec/batch\n",
      "Epoch 245/2000  Iteration 489/4000 Training loss: 0.2838 0.4435 sec/batch\n",
      "Epoch 245/2000  Iteration 490/4000 Training loss: 0.2311 0.4431 sec/batch\n",
      "Epoch 246/2000  Iteration 491/4000 Training loss: 0.2723 0.4428 sec/batch\n",
      "Epoch 246/2000  Iteration 492/4000 Training loss: 0.2253 0.4435 sec/batch\n",
      "Epoch 247/2000  Iteration 493/4000 Training loss: 0.2682 0.4428 sec/batch\n",
      "Epoch 247/2000  Iteration 494/4000 Training loss: 0.2187 0.4433 sec/batch\n",
      "Epoch 248/2000  Iteration 495/4000 Training loss: 0.2664 0.4445 sec/batch\n",
      "Epoch 248/2000  Iteration 496/4000 Training loss: 0.2149 0.4436 sec/batch\n",
      "Epoch 249/2000  Iteration 497/4000 Training loss: 0.2625 0.4436 sec/batch\n",
      "Epoch 249/2000  Iteration 498/4000 Training loss: 0.2119 0.4435 sec/batch\n",
      "Epoch 250/2000  Iteration 499/4000 Training loss: 0.2638 0.4436 sec/batch\n",
      "Epoch 250/2000  Iteration 500/4000 Training loss: 0.2120 0.4435 sec/batch\n",
      "Epoch 251/2000  Iteration 501/4000 Training loss: 0.2533 0.4431 sec/batch\n",
      "Epoch 251/2000  Iteration 502/4000 Training loss: 0.2042 0.4436 sec/batch\n",
      "Epoch 252/2000  Iteration 503/4000 Training loss: 0.2479 0.4434 sec/batch\n",
      "Epoch 252/2000  Iteration 504/4000 Training loss: 0.2037 0.4437 sec/batch\n",
      "Epoch 253/2000  Iteration 505/4000 Training loss: 0.2427 0.4434 sec/batch\n",
      "Epoch 253/2000  Iteration 506/4000 Training loss: 0.1951 0.4436 sec/batch\n",
      "Epoch 254/2000  Iteration 507/4000 Training loss: 0.2461 0.4436 sec/batch\n",
      "Epoch 254/2000  Iteration 508/4000 Training loss: 0.1956 0.4435 sec/batch\n",
      "Epoch 255/2000  Iteration 509/4000 Training loss: 0.2372 0.4431 sec/batch\n",
      "Epoch 255/2000  Iteration 510/4000 Training loss: 0.1905 0.4461 sec/batch\n",
      "Epoch 256/2000  Iteration 511/4000 Training loss: 0.2369 0.4430 sec/batch\n",
      "Epoch 256/2000  Iteration 512/4000 Training loss: 0.1878 0.4434 sec/batch\n",
      "Epoch 257/2000  Iteration 513/4000 Training loss: 0.2320 0.4431 sec/batch\n",
      "Epoch 257/2000  Iteration 514/4000 Training loss: 0.1830 0.4452 sec/batch\n",
      "Epoch 258/2000  Iteration 515/4000 Training loss: 0.2246 0.4437 sec/batch\n",
      "Epoch 258/2000  Iteration 516/4000 Training loss: 0.1759 0.4431 sec/batch\n",
      "Epoch 259/2000  Iteration 517/4000 Training loss: 0.2295 0.4431 sec/batch\n",
      "Epoch 259/2000  Iteration 518/4000 Training loss: 0.1777 0.4433 sec/batch\n",
      "Epoch 260/2000  Iteration 519/4000 Training loss: 0.2254 0.4431 sec/batch\n",
      "Epoch 260/2000  Iteration 520/4000 Training loss: 0.1786 0.4432 sec/batch\n",
      "Epoch 261/2000  Iteration 521/4000 Training loss: 0.2210 0.4430 sec/batch\n",
      "Epoch 261/2000  Iteration 522/4000 Training loss: 0.1728 0.4429 sec/batch\n",
      "Epoch 262/2000  Iteration 523/4000 Training loss: 0.2167 0.4430 sec/batch\n",
      "Epoch 262/2000  Iteration 524/4000 Training loss: 0.1686 0.4431 sec/batch\n",
      "Epoch 263/2000  Iteration 525/4000 Training loss: 0.2184 0.4432 sec/batch\n",
      "Epoch 263/2000  Iteration 526/4000 Training loss: 0.1696 0.4433 sec/batch\n",
      "Epoch 264/2000  Iteration 527/4000 Training loss: 0.2148 0.4429 sec/batch\n",
      "Epoch 264/2000  Iteration 528/4000 Training loss: 0.1659 0.4432 sec/batch\n",
      "Epoch 265/2000  Iteration 529/4000 Training loss: 0.2072 0.4434 sec/batch\n",
      "Epoch 265/2000  Iteration 530/4000 Training loss: 0.1639 0.4437 sec/batch\n",
      "Epoch 266/2000  Iteration 531/4000 Training loss: 0.2100 0.4432 sec/batch\n",
      "Epoch 266/2000  Iteration 532/4000 Training loss: 0.1626 0.4429 sec/batch\n",
      "Epoch 267/2000  Iteration 533/4000 Training loss: 0.2052 0.4433 sec/batch\n",
      "Epoch 267/2000  Iteration 534/4000 Training loss: 0.1582 0.4436 sec/batch\n",
      "Epoch 268/2000  Iteration 535/4000 Training loss: 0.2039 0.4429 sec/batch\n",
      "Epoch 268/2000  Iteration 536/4000 Training loss: 0.1577 0.4431 sec/batch\n",
      "Epoch 269/2000  Iteration 537/4000 Training loss: 0.2012 0.4431 sec/batch\n",
      "Epoch 269/2000  Iteration 538/4000 Training loss: 0.1539 0.4430 sec/batch\n",
      "Epoch 270/2000  Iteration 539/4000 Training loss: 0.2009 0.4430 sec/batch\n",
      "Epoch 270/2000  Iteration 540/4000 Training loss: 0.1551 0.4431 sec/batch\n",
      "Epoch 271/2000  Iteration 541/4000 Training loss: 0.2004 0.4432 sec/batch\n",
      "Epoch 271/2000  Iteration 542/4000 Training loss: 0.1550 0.4436 sec/batch\n",
      "Epoch 272/2000  Iteration 543/4000 Training loss: 0.1974 0.4430 sec/batch\n",
      "Epoch 272/2000  Iteration 544/4000 Training loss: 0.1525 0.4429 sec/batch\n",
      "Epoch 273/2000  Iteration 545/4000 Training loss: 0.1970 0.4432 sec/batch\n",
      "Epoch 273/2000  Iteration 546/4000 Training loss: 0.1505 0.4435 sec/batch\n",
      "Epoch 274/2000  Iteration 547/4000 Training loss: 0.1918 0.4433 sec/batch\n",
      "Epoch 274/2000  Iteration 548/4000 Training loss: 0.1466 0.4436 sec/batch\n",
      "Epoch 275/2000  Iteration 549/4000 Training loss: 0.1877 0.4429 sec/batch\n",
      "Epoch 275/2000  Iteration 550/4000 Training loss: 0.1437 0.4437 sec/batch\n",
      "Epoch 276/2000  Iteration 551/4000 Training loss: 0.1849 0.4431 sec/batch\n",
      "Epoch 276/2000  Iteration 552/4000 Training loss: 0.1404 0.4434 sec/batch\n",
      "Epoch 277/2000  Iteration 553/4000 Training loss: 0.1856 0.4433 sec/batch\n",
      "Epoch 277/2000  Iteration 554/4000 Training loss: 0.1396 0.4433 sec/batch\n",
      "Epoch 278/2000  Iteration 555/4000 Training loss: 0.1839 0.4438 sec/batch\n",
      "Epoch 278/2000  Iteration 556/4000 Training loss: 0.1386 0.4437 sec/batch\n",
      "Epoch 279/2000  Iteration 557/4000 Training loss: 0.1823 0.4438 sec/batch\n",
      "Epoch 279/2000  Iteration 558/4000 Training loss: 0.1382 0.4435 sec/batch\n",
      "Epoch 280/2000  Iteration 559/4000 Training loss: 0.1810 0.4433 sec/batch\n",
      "Epoch 280/2000  Iteration 560/4000 Training loss: 0.1352 0.4436 sec/batch\n",
      "Epoch 281/2000  Iteration 561/4000 Training loss: 0.1780 0.4433 sec/batch\n",
      "Epoch 281/2000  Iteration 562/4000 Training loss: 0.1340 0.4434 sec/batch\n",
      "Epoch 282/2000  Iteration 563/4000 Training loss: 0.1754 0.4435 sec/batch\n",
      "Epoch 282/2000  Iteration 564/4000 Training loss: 0.1316 0.4434 sec/batch\n",
      "Epoch 283/2000  Iteration 565/4000 Training loss: 0.1755 0.4429 sec/batch\n",
      "Epoch 283/2000  Iteration 566/4000 Training loss: 0.1302 0.4436 sec/batch\n",
      "Epoch 284/2000  Iteration 567/4000 Training loss: 0.1749 0.4434 sec/batch\n",
      "Epoch 284/2000  Iteration 568/4000 Training loss: 0.1293 0.4433 sec/batch\n",
      "Epoch 285/2000  Iteration 569/4000 Training loss: 0.1712 0.4432 sec/batch\n",
      "Epoch 285/2000  Iteration 570/4000 Training loss: 0.1283 0.4434 sec/batch\n",
      "Epoch 286/2000  Iteration 571/4000 Training loss: 0.1705 0.4436 sec/batch\n",
      "Epoch 286/2000  Iteration 572/4000 Training loss: 0.1271 0.4440 sec/batch\n",
      "Epoch 287/2000  Iteration 573/4000 Training loss: 0.1656 0.4433 sec/batch\n",
      "Epoch 287/2000  Iteration 574/4000 Training loss: 0.1230 0.4429 sec/batch\n",
      "Epoch 288/2000  Iteration 575/4000 Training loss: 0.1704 0.4436 sec/batch\n",
      "Epoch 288/2000  Iteration 576/4000 Training loss: 0.1264 0.4431 sec/batch\n",
      "Epoch 289/2000  Iteration 577/4000 Training loss: 0.1687 0.4427 sec/batch\n",
      "Epoch 289/2000  Iteration 578/4000 Training loss: 0.1260 0.4435 sec/batch\n",
      "Epoch 290/2000  Iteration 579/4000 Training loss: 0.1654 0.4428 sec/batch\n",
      "Epoch 290/2000  Iteration 580/4000 Training loss: 0.1218 0.4436 sec/batch\n",
      "Epoch 291/2000  Iteration 581/4000 Training loss: 0.1631 0.4428 sec/batch\n",
      "Epoch 291/2000  Iteration 582/4000 Training loss: 0.1195 0.4431 sec/batch\n",
      "Epoch 292/2000  Iteration 583/4000 Training loss: 0.1664 0.4431 sec/batch\n",
      "Epoch 292/2000  Iteration 584/4000 Training loss: 0.1197 0.4433 sec/batch\n",
      "Epoch 293/2000  Iteration 585/4000 Training loss: 0.1596 0.4430 sec/batch\n",
      "Epoch 293/2000  Iteration 586/4000 Training loss: 0.1171 0.4431 sec/batch\n",
      "Epoch 294/2000  Iteration 587/4000 Training loss: 0.1600 0.4429 sec/batch\n",
      "Epoch 294/2000  Iteration 588/4000 Training loss: 0.1180 0.4434 sec/batch\n",
      "Epoch 295/2000  Iteration 589/4000 Training loss: 0.1584 0.4430 sec/batch\n",
      "Epoch 295/2000  Iteration 590/4000 Training loss: 0.1141 0.4435 sec/batch\n",
      "Epoch 296/2000  Iteration 591/4000 Training loss: 0.1563 0.4429 sec/batch\n",
      "Epoch 296/2000  Iteration 592/4000 Training loss: 0.1144 0.4435 sec/batch\n",
      "Epoch 297/2000  Iteration 593/4000 Training loss: 0.1534 0.4434 sec/batch\n",
      "Epoch 297/2000  Iteration 594/4000 Training loss: 0.1128 0.4435 sec/batch\n",
      "Epoch 298/2000  Iteration 595/4000 Training loss: 0.1522 0.4434 sec/batch\n",
      "Epoch 298/2000  Iteration 596/4000 Training loss: 0.1096 0.4432 sec/batch\n",
      "Epoch 299/2000  Iteration 597/4000 Training loss: 0.1520 0.4436 sec/batch\n",
      "Epoch 299/2000  Iteration 598/4000 Training loss: 0.1107 0.4439 sec/batch\n",
      "Epoch 300/2000  Iteration 599/4000 Training loss: 0.1484 0.4438 sec/batch\n",
      "Epoch 300/2000  Iteration 600/4000 Training loss: 0.1075 0.4434 sec/batch\n",
      "Validation loss: 4.5078 Saving checkpoint!\n",
      "Epoch 301/2000  Iteration 601/4000 Training loss: 0.1484 0.4434 sec/batch\n",
      "Epoch 301/2000  Iteration 602/4000 Training loss: 0.1078 0.4438 sec/batch\n",
      "Epoch 302/2000  Iteration 603/4000 Training loss: 0.1505 0.4432 sec/batch\n",
      "Epoch 302/2000  Iteration 604/4000 Training loss: 0.1068 0.4434 sec/batch\n",
      "Epoch 303/2000  Iteration 605/4000 Training loss: 0.1472 0.4433 sec/batch\n",
      "Epoch 303/2000  Iteration 606/4000 Training loss: 0.1061 0.4437 sec/batch\n",
      "Epoch 304/2000  Iteration 607/4000 Training loss: 0.1468 0.4436 sec/batch\n",
      "Epoch 304/2000  Iteration 608/4000 Training loss: 0.1045 0.4442 sec/batch\n",
      "Epoch 305/2000  Iteration 609/4000 Training loss: 0.1459 0.4433 sec/batch\n",
      "Epoch 305/2000  Iteration 610/4000 Training loss: 0.1051 0.4436 sec/batch\n",
      "Epoch 306/2000  Iteration 611/4000 Training loss: 0.1411 0.4433 sec/batch\n",
      "Epoch 306/2000  Iteration 612/4000 Training loss: 0.1012 0.4440 sec/batch\n",
      "Epoch 307/2000  Iteration 613/4000 Training loss: 0.1428 0.4436 sec/batch\n",
      "Epoch 307/2000  Iteration 614/4000 Training loss: 0.1027 0.4441 sec/batch\n",
      "Epoch 308/2000  Iteration 615/4000 Training loss: 0.1419 0.4442 sec/batch\n",
      "Epoch 308/2000  Iteration 616/4000 Training loss: 0.1021 0.4437 sec/batch\n",
      "Epoch 309/2000  Iteration 617/4000 Training loss: 0.1379 0.4441 sec/batch\n",
      "Epoch 309/2000  Iteration 618/4000 Training loss: 0.0992 0.4438 sec/batch\n",
      "Epoch 310/2000  Iteration 619/4000 Training loss: 0.1405 0.4437 sec/batch\n",
      "Epoch 310/2000  Iteration 620/4000 Training loss: 0.0994 0.4439 sec/batch\n",
      "Epoch 311/2000  Iteration 621/4000 Training loss: 0.1381 0.4445 sec/batch\n",
      "Epoch 311/2000  Iteration 622/4000 Training loss: 0.0972 0.4438 sec/batch\n",
      "Epoch 312/2000  Iteration 623/4000 Training loss: 0.1352 0.4433 sec/batch\n",
      "Epoch 312/2000  Iteration 624/4000 Training loss: 0.0959 0.4432 sec/batch\n",
      "Epoch 313/2000  Iteration 625/4000 Training loss: 0.1369 0.4429 sec/batch\n",
      "Epoch 313/2000  Iteration 626/4000 Training loss: 0.0975 0.4434 sec/batch\n",
      "Epoch 314/2000  Iteration 627/4000 Training loss: 0.1336 0.4433 sec/batch\n",
      "Epoch 314/2000  Iteration 628/4000 Training loss: 0.0943 0.4432 sec/batch\n",
      "Epoch 315/2000  Iteration 629/4000 Training loss: 0.1329 0.4431 sec/batch\n",
      "Epoch 315/2000  Iteration 630/4000 Training loss: 0.0922 0.4433 sec/batch\n",
      "Epoch 316/2000  Iteration 631/4000 Training loss: 0.1313 0.4432 sec/batch\n",
      "Epoch 316/2000  Iteration 632/4000 Training loss: 0.0930 0.4437 sec/batch\n",
      "Epoch 317/2000  Iteration 633/4000 Training loss: 0.1329 0.4430 sec/batch\n",
      "Epoch 317/2000  Iteration 634/4000 Training loss: 0.0941 0.4436 sec/batch\n",
      "Epoch 318/2000  Iteration 635/4000 Training loss: 0.1264 0.4433 sec/batch\n",
      "Epoch 318/2000  Iteration 636/4000 Training loss: 0.0882 0.4429 sec/batch\n",
      "Epoch 319/2000  Iteration 637/4000 Training loss: 0.1297 0.4429 sec/batch\n",
      "Epoch 319/2000  Iteration 638/4000 Training loss: 0.0908 0.4433 sec/batch\n",
      "Epoch 320/2000  Iteration 639/4000 Training loss: 0.1263 0.4431 sec/batch\n",
      "Epoch 320/2000  Iteration 640/4000 Training loss: 0.0877 0.4434 sec/batch\n",
      "Epoch 321/2000  Iteration 641/4000 Training loss: 0.1277 0.4434 sec/batch\n",
      "Epoch 321/2000  Iteration 642/4000 Training loss: 0.0894 0.4431 sec/batch\n",
      "Epoch 322/2000  Iteration 643/4000 Training loss: 0.1325 0.4432 sec/batch\n",
      "Epoch 322/2000  Iteration 644/4000 Training loss: 0.0916 0.4430 sec/batch\n",
      "Epoch 323/2000  Iteration 645/4000 Training loss: 0.1272 0.4432 sec/batch\n",
      "Epoch 323/2000  Iteration 646/4000 Training loss: 0.0882 0.4430 sec/batch\n",
      "Epoch 324/2000  Iteration 647/4000 Training loss: 0.1262 0.4426 sec/batch\n",
      "Epoch 324/2000  Iteration 648/4000 Training loss: 0.0870 0.4436 sec/batch\n",
      "Epoch 325/2000  Iteration 649/4000 Training loss: 0.1282 0.4428 sec/batch\n",
      "Epoch 325/2000  Iteration 650/4000 Training loss: 0.0882 0.4433 sec/batch\n",
      "Epoch 326/2000  Iteration 651/4000 Training loss: 0.1239 0.4430 sec/batch\n",
      "Epoch 326/2000  Iteration 652/4000 Training loss: 0.0852 0.4434 sec/batch\n",
      "Epoch 327/2000  Iteration 653/4000 Training loss: 0.1265 0.4429 sec/batch\n",
      "Epoch 327/2000  Iteration 654/4000 Training loss: 0.0860 0.4434 sec/batch\n",
      "Epoch 328/2000  Iteration 655/4000 Training loss: 0.1224 0.4430 sec/batch\n",
      "Epoch 328/2000  Iteration 656/4000 Training loss: 0.0837 0.4435 sec/batch\n",
      "Epoch 329/2000  Iteration 657/4000 Training loss: 0.1200 0.4432 sec/batch\n",
      "Epoch 329/2000  Iteration 658/4000 Training loss: 0.0833 0.4433 sec/batch\n",
      "Epoch 330/2000  Iteration 659/4000 Training loss: 0.1235 0.4429 sec/batch\n",
      "Epoch 330/2000  Iteration 660/4000 Training loss: 0.0840 0.4433 sec/batch\n",
      "Epoch 331/2000  Iteration 661/4000 Training loss: 0.1183 0.4430 sec/batch\n",
      "Epoch 331/2000  Iteration 662/4000 Training loss: 0.0828 0.4433 sec/batch\n",
      "Epoch 332/2000  Iteration 663/4000 Training loss: 0.1211 0.4432 sec/batch\n",
      "Epoch 332/2000  Iteration 664/4000 Training loss: 0.0831 0.4435 sec/batch\n",
      "Epoch 333/2000  Iteration 665/4000 Training loss: 0.1194 0.4432 sec/batch\n",
      "Epoch 333/2000  Iteration 666/4000 Training loss: 0.0823 0.4436 sec/batch\n",
      "Epoch 334/2000  Iteration 667/4000 Training loss: 0.1185 0.4430 sec/batch\n",
      "Epoch 334/2000  Iteration 668/4000 Training loss: 0.0810 0.4435 sec/batch\n",
      "Epoch 335/2000  Iteration 669/4000 Training loss: 0.1168 0.4436 sec/batch\n",
      "Epoch 335/2000  Iteration 670/4000 Training loss: 0.0810 0.4440 sec/batch\n",
      "Epoch 336/2000  Iteration 671/4000 Training loss: 0.1175 0.4436 sec/batch\n",
      "Epoch 336/2000  Iteration 672/4000 Training loss: 0.0799 0.4436 sec/batch\n",
      "Epoch 337/2000  Iteration 673/4000 Training loss: 0.1197 0.4436 sec/batch\n",
      "Epoch 337/2000  Iteration 674/4000 Training loss: 0.0802 0.4437 sec/batch\n",
      "Epoch 338/2000  Iteration 675/4000 Training loss: 0.1172 0.4434 sec/batch\n",
      "Epoch 338/2000  Iteration 676/4000 Training loss: 0.0789 0.4438 sec/batch\n",
      "Epoch 339/2000  Iteration 677/4000 Training loss: 0.1181 0.4434 sec/batch\n",
      "Epoch 339/2000  Iteration 678/4000 Training loss: 0.0806 0.4440 sec/batch\n",
      "Epoch 340/2000  Iteration 679/4000 Training loss: 0.1157 0.4437 sec/batch\n",
      "Epoch 340/2000  Iteration 680/4000 Training loss: 0.0801 0.4435 sec/batch\n",
      "Epoch 341/2000  Iteration 681/4000 Training loss: 0.1132 0.4435 sec/batch\n",
      "Epoch 341/2000  Iteration 682/4000 Training loss: 0.0769 0.4437 sec/batch\n",
      "Epoch 342/2000  Iteration 683/4000 Training loss: 0.1134 0.4436 sec/batch\n",
      "Epoch 342/2000  Iteration 684/4000 Training loss: 0.0776 0.4439 sec/batch\n",
      "Epoch 343/2000  Iteration 685/4000 Training loss: 0.1144 0.4436 sec/batch\n",
      "Epoch 343/2000  Iteration 686/4000 Training loss: 0.0779 0.4436 sec/batch\n",
      "Epoch 344/2000  Iteration 687/4000 Training loss: 0.1150 0.4436 sec/batch\n",
      "Epoch 344/2000  Iteration 688/4000 Training loss: 0.0769 0.4436 sec/batch\n",
      "Epoch 345/2000  Iteration 689/4000 Training loss: 0.1100 0.4434 sec/batch\n",
      "Epoch 345/2000  Iteration 690/4000 Training loss: 0.0761 0.4434 sec/batch\n",
      "Epoch 346/2000  Iteration 691/4000 Training loss: 0.1122 0.4433 sec/batch\n",
      "Epoch 346/2000  Iteration 692/4000 Training loss: 0.0772 0.4441 sec/batch\n",
      "Epoch 347/2000  Iteration 693/4000 Training loss: 0.1147 0.4436 sec/batch\n",
      "Epoch 347/2000  Iteration 694/4000 Training loss: 0.0771 0.4444 sec/batch\n",
      "Epoch 348/2000  Iteration 695/4000 Training loss: 0.1085 0.4436 sec/batch\n",
      "Epoch 348/2000  Iteration 696/4000 Training loss: 0.0758 0.4437 sec/batch\n",
      "Epoch 349/2000  Iteration 697/4000 Training loss: 0.1101 0.4433 sec/batch\n",
      "Epoch 349/2000  Iteration 698/4000 Training loss: 0.0725 0.4439 sec/batch\n",
      "Epoch 350/2000  Iteration 699/4000 Training loss: 0.1095 0.4432 sec/batch\n",
      "Epoch 350/2000  Iteration 700/4000 Training loss: 0.0746 0.4438 sec/batch\n",
      "Epoch 351/2000  Iteration 701/4000 Training loss: 0.1110 0.4430 sec/batch\n",
      "Epoch 351/2000  Iteration 702/4000 Training loss: 0.0743 0.4438 sec/batch\n",
      "Epoch 352/2000  Iteration 703/4000 Training loss: 0.1050 0.4439 sec/batch\n",
      "Epoch 352/2000  Iteration 704/4000 Training loss: 0.0722 0.4434 sec/batch\n",
      "Epoch 353/2000  Iteration 705/4000 Training loss: 0.1060 0.4431 sec/batch\n",
      "Epoch 353/2000  Iteration 706/4000 Training loss: 0.0727 0.4435 sec/batch\n",
      "Epoch 354/2000  Iteration 707/4000 Training loss: 0.1065 0.4431 sec/batch\n",
      "Epoch 354/2000  Iteration 708/4000 Training loss: 0.0706 0.4429 sec/batch\n",
      "Epoch 355/2000  Iteration 709/4000 Training loss: 0.1045 0.4432 sec/batch\n",
      "Epoch 355/2000  Iteration 710/4000 Training loss: 0.0708 0.4431 sec/batch\n",
      "Epoch 356/2000  Iteration 711/4000 Training loss: 0.1053 0.4430 sec/batch\n",
      "Epoch 356/2000  Iteration 712/4000 Training loss: 0.0721 0.4431 sec/batch\n",
      "Epoch 357/2000  Iteration 713/4000 Training loss: 0.1077 0.4430 sec/batch\n",
      "Epoch 357/2000  Iteration 714/4000 Training loss: 0.0724 0.4436 sec/batch\n",
      "Epoch 358/2000  Iteration 715/4000 Training loss: 0.1040 0.4429 sec/batch\n",
      "Epoch 358/2000  Iteration 716/4000 Training loss: 0.0698 0.4434 sec/batch\n",
      "Epoch 359/2000  Iteration 717/4000 Training loss: 0.1037 0.4435 sec/batch\n",
      "Epoch 359/2000  Iteration 718/4000 Training loss: 0.0705 0.4435 sec/batch\n",
      "Epoch 360/2000  Iteration 719/4000 Training loss: 0.1051 0.4434 sec/batch\n",
      "Epoch 360/2000  Iteration 720/4000 Training loss: 0.0709 0.4434 sec/batch\n",
      "Epoch 361/2000  Iteration 721/4000 Training loss: 0.1018 0.4430 sec/batch\n",
      "Epoch 361/2000  Iteration 722/4000 Training loss: 0.0671 0.4437 sec/batch\n",
      "Epoch 362/2000  Iteration 723/4000 Training loss: 0.1033 0.4429 sec/batch\n",
      "Epoch 362/2000  Iteration 724/4000 Training loss: 0.0694 0.4431 sec/batch\n",
      "Epoch 363/2000  Iteration 725/4000 Training loss: 0.1012 0.4431 sec/batch\n",
      "Epoch 363/2000  Iteration 726/4000 Training loss: 0.0679 0.4434 sec/batch\n",
      "Epoch 364/2000  Iteration 727/4000 Training loss: 0.1016 0.4434 sec/batch\n",
      "Epoch 364/2000  Iteration 728/4000 Training loss: 0.0672 0.4439 sec/batch\n",
      "Epoch 365/2000  Iteration 729/4000 Training loss: 0.1019 0.4431 sec/batch\n",
      "Epoch 365/2000  Iteration 730/4000 Training loss: 0.0672 0.4434 sec/batch\n",
      "Epoch 366/2000  Iteration 731/4000 Training loss: 0.1009 0.4431 sec/batch\n",
      "Epoch 366/2000  Iteration 732/4000 Training loss: 0.0666 0.4435 sec/batch\n",
      "Epoch 367/2000  Iteration 733/4000 Training loss: 0.0989 0.4426 sec/batch\n",
      "Epoch 367/2000  Iteration 734/4000 Training loss: 0.0653 0.4434 sec/batch\n",
      "Epoch 368/2000  Iteration 735/4000 Training loss: 0.0990 0.4433 sec/batch\n",
      "Epoch 368/2000  Iteration 736/4000 Training loss: 0.0657 0.4435 sec/batch\n",
      "Epoch 369/2000  Iteration 737/4000 Training loss: 0.1009 0.4433 sec/batch\n",
      "Epoch 369/2000  Iteration 738/4000 Training loss: 0.0663 0.4433 sec/batch\n",
      "Epoch 370/2000  Iteration 739/4000 Training loss: 0.0982 0.4431 sec/batch\n",
      "Epoch 370/2000  Iteration 740/4000 Training loss: 0.0645 0.4432 sec/batch\n",
      "Epoch 371/2000  Iteration 741/4000 Training loss: 0.0991 0.4435 sec/batch\n",
      "Epoch 371/2000  Iteration 742/4000 Training loss: 0.0656 0.4431 sec/batch\n",
      "Epoch 372/2000  Iteration 743/4000 Training loss: 0.0979 0.4431 sec/batch\n",
      "Epoch 372/2000  Iteration 744/4000 Training loss: 0.0646 0.4433 sec/batch\n",
      "Epoch 373/2000  Iteration 745/4000 Training loss: 0.0969 0.4430 sec/batch\n",
      "Epoch 373/2000  Iteration 746/4000 Training loss: 0.0638 0.4433 sec/batch\n",
      "Epoch 374/2000  Iteration 747/4000 Training loss: 0.0986 0.4435 sec/batch\n",
      "Epoch 374/2000  Iteration 748/4000 Training loss: 0.0642 0.4435 sec/batch\n",
      "Epoch 375/2000  Iteration 749/4000 Training loss: 0.0964 0.4432 sec/batch\n",
      "Epoch 375/2000  Iteration 750/4000 Training loss: 0.0641 0.4433 sec/batch\n",
      "Epoch 376/2000  Iteration 751/4000 Training loss: 0.0983 0.4434 sec/batch\n",
      "Epoch 376/2000  Iteration 752/4000 Training loss: 0.0654 0.4431 sec/batch\n",
      "Epoch 377/2000  Iteration 753/4000 Training loss: 0.0956 0.4432 sec/batch\n",
      "Epoch 377/2000  Iteration 754/4000 Training loss: 0.0641 0.4435 sec/batch\n",
      "Epoch 378/2000  Iteration 755/4000 Training loss: 0.0958 0.4432 sec/batch\n",
      "Epoch 378/2000  Iteration 756/4000 Training loss: 0.0628 0.4430 sec/batch\n",
      "Epoch 379/2000  Iteration 757/4000 Training loss: 0.0954 0.4430 sec/batch\n",
      "Epoch 379/2000  Iteration 758/4000 Training loss: 0.0629 0.4431 sec/batch\n",
      "Epoch 380/2000  Iteration 759/4000 Training loss: 0.0943 0.4428 sec/batch\n",
      "Epoch 380/2000  Iteration 760/4000 Training loss: 0.0622 0.4432 sec/batch\n",
      "Epoch 381/2000  Iteration 761/4000 Training loss: 0.0920 0.4430 sec/batch\n",
      "Epoch 381/2000  Iteration 762/4000 Training loss: 0.0598 0.4432 sec/batch\n",
      "Epoch 382/2000  Iteration 763/4000 Training loss: 0.0932 0.4438 sec/batch\n",
      "Epoch 382/2000  Iteration 764/4000 Training loss: 0.0612 0.4443 sec/batch\n",
      "Epoch 383/2000  Iteration 765/4000 Training loss: 0.0925 0.4434 sec/batch\n",
      "Epoch 383/2000  Iteration 766/4000 Training loss: 0.0611 0.4436 sec/batch\n",
      "Epoch 384/2000  Iteration 767/4000 Training loss: 0.0918 0.4431 sec/batch\n",
      "Epoch 384/2000  Iteration 768/4000 Training loss: 0.0613 0.4436 sec/batch\n",
      "Epoch 385/2000  Iteration 769/4000 Training loss: 0.0900 0.4430 sec/batch\n",
      "Epoch 385/2000  Iteration 770/4000 Training loss: 0.0594 0.4440 sec/batch\n",
      "Epoch 386/2000  Iteration 771/4000 Training loss: 0.0894 0.4433 sec/batch\n",
      "Epoch 386/2000  Iteration 772/4000 Training loss: 0.0582 0.4435 sec/batch\n",
      "Epoch 387/2000  Iteration 773/4000 Training loss: 0.0904 0.4435 sec/batch\n",
      "Epoch 387/2000  Iteration 774/4000 Training loss: 0.0605 0.4438 sec/batch\n",
      "Epoch 388/2000  Iteration 775/4000 Training loss: 0.0906 0.4430 sec/batch\n",
      "Epoch 388/2000  Iteration 776/4000 Training loss: 0.0588 0.4435 sec/batch\n",
      "Epoch 389/2000  Iteration 777/4000 Training loss: 0.0884 0.4433 sec/batch\n",
      "Epoch 389/2000  Iteration 778/4000 Training loss: 0.0583 0.4433 sec/batch\n",
      "Epoch 390/2000  Iteration 779/4000 Training loss: 0.0870 0.4432 sec/batch\n",
      "Epoch 390/2000  Iteration 780/4000 Training loss: 0.0568 0.4432 sec/batch\n",
      "Epoch 391/2000  Iteration 781/4000 Training loss: 0.0875 0.4430 sec/batch\n",
      "Epoch 391/2000  Iteration 782/4000 Training loss: 0.0566 0.4433 sec/batch\n",
      "Epoch 392/2000  Iteration 783/4000 Training loss: 0.0892 0.4433 sec/batch\n",
      "Epoch 392/2000  Iteration 784/4000 Training loss: 0.0572 0.4431 sec/batch\n",
      "Epoch 393/2000  Iteration 785/4000 Training loss: 0.0897 0.4431 sec/batch\n",
      "Epoch 393/2000  Iteration 786/4000 Training loss: 0.0581 0.4432 sec/batch\n",
      "Epoch 394/2000  Iteration 787/4000 Training loss: 0.0909 0.4429 sec/batch\n",
      "Epoch 394/2000  Iteration 788/4000 Training loss: 0.0584 0.4430 sec/batch\n",
      "Epoch 395/2000  Iteration 789/4000 Training loss: 0.0878 0.4434 sec/batch\n",
      "Epoch 395/2000  Iteration 790/4000 Training loss: 0.0566 0.4434 sec/batch\n",
      "Epoch 396/2000  Iteration 791/4000 Training loss: 0.0856 0.4429 sec/batch\n",
      "Epoch 396/2000  Iteration 792/4000 Training loss: 0.0552 0.4435 sec/batch\n",
      "Epoch 397/2000  Iteration 793/4000 Training loss: 0.0881 0.4431 sec/batch\n",
      "Epoch 397/2000  Iteration 794/4000 Training loss: 0.0566 0.4430 sec/batch\n",
      "Epoch 398/2000  Iteration 795/4000 Training loss: 0.0866 0.4434 sec/batch\n",
      "Epoch 398/2000  Iteration 796/4000 Training loss: 0.0556 0.4435 sec/batch\n",
      "Epoch 399/2000  Iteration 797/4000 Training loss: 0.0844 0.4431 sec/batch\n",
      "Epoch 399/2000  Iteration 798/4000 Training loss: 0.0548 0.4433 sec/batch\n",
      "Epoch 400/2000  Iteration 799/4000 Training loss: 0.0844 0.4435 sec/batch\n",
      "Epoch 400/2000  Iteration 800/4000 Training loss: 0.0549 0.4434 sec/batch\n",
      "Validation loss: 5.09056 Saving checkpoint!\n",
      "Epoch 401/2000  Iteration 801/4000 Training loss: 0.0851 0.4438 sec/batch\n",
      "Epoch 401/2000  Iteration 802/4000 Training loss: 0.0552 0.4432 sec/batch\n",
      "Epoch 402/2000  Iteration 803/4000 Training loss: 0.0846 0.4433 sec/batch\n",
      "Epoch 402/2000  Iteration 804/4000 Training loss: 0.0537 0.4434 sec/batch\n",
      "Epoch 403/2000  Iteration 805/4000 Training loss: 0.0857 0.4432 sec/batch\n",
      "Epoch 403/2000  Iteration 806/4000 Training loss: 0.0554 0.4431 sec/batch\n",
      "Epoch 404/2000  Iteration 807/4000 Training loss: 0.0845 0.4429 sec/batch\n",
      "Epoch 404/2000  Iteration 808/4000 Training loss: 0.0549 0.4435 sec/batch\n",
      "Epoch 405/2000  Iteration 809/4000 Training loss: 0.0851 0.4427 sec/batch\n",
      "Epoch 405/2000  Iteration 810/4000 Training loss: 0.0531 0.4431 sec/batch\n",
      "Epoch 406/2000  Iteration 811/4000 Training loss: 0.0837 0.4428 sec/batch\n",
      "Epoch 406/2000  Iteration 812/4000 Training loss: 0.0540 0.4431 sec/batch\n",
      "Epoch 407/2000  Iteration 813/4000 Training loss: 0.0851 0.4428 sec/batch\n",
      "Epoch 407/2000  Iteration 814/4000 Training loss: 0.0531 0.4434 sec/batch\n",
      "Epoch 408/2000  Iteration 815/4000 Training loss: 0.0835 0.4427 sec/batch\n",
      "Epoch 408/2000  Iteration 816/4000 Training loss: 0.0537 0.4432 sec/batch\n",
      "Epoch 409/2000  Iteration 817/4000 Training loss: 0.0837 0.4432 sec/batch\n",
      "Epoch 409/2000  Iteration 818/4000 Training loss: 0.0532 0.4429 sec/batch\n",
      "Epoch 410/2000  Iteration 819/4000 Training loss: 0.0831 0.4427 sec/batch\n",
      "Epoch 410/2000  Iteration 820/4000 Training loss: 0.0527 0.4433 sec/batch\n",
      "Epoch 411/2000  Iteration 821/4000 Training loss: 0.0836 0.4434 sec/batch\n",
      "Epoch 411/2000  Iteration 822/4000 Training loss: 0.0529 0.4430 sec/batch\n",
      "Epoch 412/2000  Iteration 823/4000 Training loss: 0.0819 0.4431 sec/batch\n",
      "Epoch 412/2000  Iteration 824/4000 Training loss: 0.0524 0.4435 sec/batch\n",
      "Epoch 413/2000  Iteration 825/4000 Training loss: 0.0806 0.4432 sec/batch\n",
      "Epoch 413/2000  Iteration 826/4000 Training loss: 0.0514 0.4435 sec/batch\n",
      "Epoch 414/2000  Iteration 827/4000 Training loss: 0.0823 0.4433 sec/batch\n",
      "Epoch 414/2000  Iteration 828/4000 Training loss: 0.0524 0.4448 sec/batch\n",
      "Epoch 415/2000  Iteration 829/4000 Training loss: 0.0830 0.4434 sec/batch\n",
      "Epoch 415/2000  Iteration 830/4000 Training loss: 0.0521 0.4432 sec/batch\n",
      "Epoch 416/2000  Iteration 831/4000 Training loss: 0.0824 0.4429 sec/batch\n",
      "Epoch 416/2000  Iteration 832/4000 Training loss: 0.0522 0.4434 sec/batch\n",
      "Epoch 417/2000  Iteration 833/4000 Training loss: 0.0811 0.4428 sec/batch\n",
      "Epoch 417/2000  Iteration 834/4000 Training loss: 0.0519 0.4431 sec/batch\n",
      "Epoch 418/2000  Iteration 835/4000 Training loss: 0.0812 0.4429 sec/batch\n",
      "Epoch 418/2000  Iteration 836/4000 Training loss: 0.0517 0.4434 sec/batch\n",
      "Epoch 419/2000  Iteration 837/4000 Training loss: 0.0815 0.4428 sec/batch\n",
      "Epoch 419/2000  Iteration 838/4000 Training loss: 0.0516 0.4431 sec/batch\n",
      "Epoch 420/2000  Iteration 839/4000 Training loss: 0.0813 0.4433 sec/batch\n",
      "Epoch 420/2000  Iteration 840/4000 Training loss: 0.0511 0.4460 sec/batch\n",
      "Epoch 421/2000  Iteration 841/4000 Training loss: 0.0813 0.4431 sec/batch\n",
      "Epoch 421/2000  Iteration 842/4000 Training loss: 0.0523 0.4431 sec/batch\n",
      "Epoch 422/2000  Iteration 843/4000 Training loss: 0.0798 0.4433 sec/batch\n",
      "Epoch 422/2000  Iteration 844/4000 Training loss: 0.0511 0.4435 sec/batch\n",
      "Epoch 423/2000  Iteration 845/4000 Training loss: 0.0780 0.4478 sec/batch\n",
      "Epoch 423/2000  Iteration 846/4000 Training loss: 0.0500 0.4430 sec/batch\n",
      "Epoch 424/2000  Iteration 847/4000 Training loss: 0.0795 0.4431 sec/batch\n",
      "Epoch 424/2000  Iteration 848/4000 Training loss: 0.0493 0.4430 sec/batch\n",
      "Epoch 425/2000  Iteration 849/4000 Training loss: 0.0794 0.4427 sec/batch\n",
      "Epoch 425/2000  Iteration 850/4000 Training loss: 0.0499 0.4430 sec/batch\n",
      "Epoch 426/2000  Iteration 851/4000 Training loss: 0.0788 0.4434 sec/batch\n",
      "Epoch 426/2000  Iteration 852/4000 Training loss: 0.0493 0.4436 sec/batch\n",
      "Epoch 427/2000  Iteration 853/4000 Training loss: 0.0773 0.4433 sec/batch\n",
      "Epoch 427/2000  Iteration 854/4000 Training loss: 0.0487 0.4449 sec/batch\n",
      "Epoch 428/2000  Iteration 855/4000 Training loss: 0.0771 0.4434 sec/batch\n",
      "Epoch 428/2000  Iteration 856/4000 Training loss: 0.0486 0.4436 sec/batch\n",
      "Epoch 429/2000  Iteration 857/4000 Training loss: 0.0782 0.4431 sec/batch\n",
      "Epoch 429/2000  Iteration 858/4000 Training loss: 0.0490 0.4438 sec/batch\n",
      "Epoch 430/2000  Iteration 859/4000 Training loss: 0.0774 0.4437 sec/batch\n",
      "Epoch 430/2000  Iteration 860/4000 Training loss: 0.0491 0.4432 sec/batch\n",
      "Epoch 431/2000  Iteration 861/4000 Training loss: 0.0770 0.4432 sec/batch\n",
      "Epoch 431/2000  Iteration 862/4000 Training loss: 0.0483 0.4436 sec/batch\n",
      "Epoch 432/2000  Iteration 863/4000 Training loss: 0.0767 0.4434 sec/batch\n",
      "Epoch 432/2000  Iteration 864/4000 Training loss: 0.0488 0.4436 sec/batch\n",
      "Epoch 433/2000  Iteration 865/4000 Training loss: 0.0751 0.4430 sec/batch\n",
      "Epoch 433/2000  Iteration 866/4000 Training loss: 0.0472 0.4440 sec/batch\n",
      "Epoch 434/2000  Iteration 867/4000 Training loss: 0.0762 0.4432 sec/batch\n",
      "Epoch 434/2000  Iteration 868/4000 Training loss: 0.0476 0.4436 sec/batch\n",
      "Epoch 435/2000  Iteration 869/4000 Training loss: 0.0764 0.4429 sec/batch\n",
      "Epoch 435/2000  Iteration 870/4000 Training loss: 0.0480 0.4435 sec/batch\n",
      "Epoch 436/2000  Iteration 871/4000 Training loss: 0.0768 0.4433 sec/batch\n",
      "Epoch 436/2000  Iteration 872/4000 Training loss: 0.0472 0.4432 sec/batch\n",
      "Epoch 437/2000  Iteration 873/4000 Training loss: 0.0756 0.4433 sec/batch\n",
      "Epoch 437/2000  Iteration 874/4000 Training loss: 0.0481 0.4432 sec/batch\n",
      "Epoch 438/2000  Iteration 875/4000 Training loss: 0.0758 0.4430 sec/batch\n",
      "Epoch 438/2000  Iteration 876/4000 Training loss: 0.0476 0.4434 sec/batch\n",
      "Epoch 439/2000  Iteration 877/4000 Training loss: 0.0745 0.4428 sec/batch\n",
      "Epoch 439/2000  Iteration 878/4000 Training loss: 0.0461 0.4428 sec/batch\n",
      "Epoch 440/2000  Iteration 879/4000 Training loss: 0.0757 0.4433 sec/batch\n",
      "Epoch 440/2000  Iteration 880/4000 Training loss: 0.0467 0.4434 sec/batch\n",
      "Epoch 441/2000  Iteration 881/4000 Training loss: 0.0748 0.4431 sec/batch\n",
      "Epoch 441/2000  Iteration 882/4000 Training loss: 0.0463 0.4436 sec/batch\n",
      "Epoch 442/2000  Iteration 883/4000 Training loss: 0.0735 0.4434 sec/batch\n",
      "Epoch 442/2000  Iteration 884/4000 Training loss: 0.0453 0.4430 sec/batch\n",
      "Epoch 443/2000  Iteration 885/4000 Training loss: 0.0737 0.4433 sec/batch\n",
      "Epoch 443/2000  Iteration 886/4000 Training loss: 0.0459 0.4435 sec/batch\n",
      "Epoch 444/2000  Iteration 887/4000 Training loss: 0.0724 0.4433 sec/batch\n",
      "Epoch 444/2000  Iteration 888/4000 Training loss: 0.0458 0.4430 sec/batch\n",
      "Epoch 445/2000  Iteration 889/4000 Training loss: 0.0722 0.4430 sec/batch\n",
      "Epoch 445/2000  Iteration 890/4000 Training loss: 0.0450 0.4435 sec/batch\n",
      "Epoch 446/2000  Iteration 891/4000 Training loss: 0.0726 0.4434 sec/batch\n",
      "Epoch 446/2000  Iteration 892/4000 Training loss: 0.0453 0.4435 sec/batch\n",
      "Epoch 447/2000  Iteration 893/4000 Training loss: 0.0751 0.4432 sec/batch\n",
      "Epoch 447/2000  Iteration 894/4000 Training loss: 0.0463 0.4433 sec/batch\n",
      "Epoch 448/2000  Iteration 895/4000 Training loss: 0.0740 0.4436 sec/batch\n",
      "Epoch 448/2000  Iteration 896/4000 Training loss: 0.0452 0.4436 sec/batch\n",
      "Epoch 449/2000  Iteration 897/4000 Training loss: 0.0734 0.4434 sec/batch\n",
      "Epoch 449/2000  Iteration 898/4000 Training loss: 0.0448 0.4434 sec/batch\n",
      "Epoch 450/2000  Iteration 899/4000 Training loss: 0.0710 0.4430 sec/batch\n",
      "Epoch 450/2000  Iteration 900/4000 Training loss: 0.0444 0.4432 sec/batch\n",
      "Epoch 451/2000  Iteration 901/4000 Training loss: 0.0717 0.4431 sec/batch\n",
      "Epoch 451/2000  Iteration 902/4000 Training loss: 0.0446 0.4432 sec/batch\n",
      "Epoch 452/2000  Iteration 903/4000 Training loss: 0.0696 0.4430 sec/batch\n",
      "Epoch 452/2000  Iteration 904/4000 Training loss: 0.0436 0.4429 sec/batch\n",
      "Epoch 453/2000  Iteration 905/4000 Training loss: 0.0708 0.4432 sec/batch\n",
      "Epoch 453/2000  Iteration 906/4000 Training loss: 0.0447 0.4432 sec/batch\n",
      "Epoch 454/2000  Iteration 907/4000 Training loss: 0.0719 0.4431 sec/batch\n",
      "Epoch 454/2000  Iteration 908/4000 Training loss: 0.0441 0.4432 sec/batch\n",
      "Epoch 455/2000  Iteration 909/4000 Training loss: 0.0697 0.4428 sec/batch\n",
      "Epoch 455/2000  Iteration 910/4000 Training loss: 0.0436 0.4428 sec/batch\n",
      "Epoch 456/2000  Iteration 911/4000 Training loss: 0.0703 0.4433 sec/batch\n",
      "Epoch 456/2000  Iteration 912/4000 Training loss: 0.0429 0.4431 sec/batch\n",
      "Epoch 457/2000  Iteration 913/4000 Training loss: 0.0683 0.4429 sec/batch\n",
      "Epoch 457/2000  Iteration 914/4000 Training loss: 0.0422 0.4432 sec/batch\n",
      "Epoch 458/2000  Iteration 915/4000 Training loss: 0.0704 0.4430 sec/batch\n",
      "Epoch 458/2000  Iteration 916/4000 Training loss: 0.0438 0.4433 sec/batch\n",
      "Epoch 459/2000  Iteration 917/4000 Training loss: 0.0715 0.4438 sec/batch\n",
      "Epoch 459/2000  Iteration 918/4000 Training loss: 0.0443 0.4437 sec/batch\n",
      "Epoch 460/2000  Iteration 919/4000 Training loss: 0.0696 0.4431 sec/batch\n",
      "Epoch 460/2000  Iteration 920/4000 Training loss: 0.0438 0.4437 sec/batch\n",
      "Epoch 461/2000  Iteration 921/4000 Training loss: 0.0689 0.4433 sec/batch\n",
      "Epoch 461/2000  Iteration 922/4000 Training loss: 0.0434 0.4437 sec/batch\n",
      "Epoch 462/2000  Iteration 923/4000 Training loss: 0.0707 0.4438 sec/batch\n",
      "Epoch 462/2000  Iteration 924/4000 Training loss: 0.0443 0.4439 sec/batch\n",
      "Epoch 463/2000  Iteration 925/4000 Training loss: 0.0695 0.4435 sec/batch\n",
      "Epoch 463/2000  Iteration 926/4000 Training loss: 0.0441 0.4436 sec/batch\n",
      "Epoch 464/2000  Iteration 927/4000 Training loss: 0.0697 0.4437 sec/batch\n",
      "Epoch 464/2000  Iteration 928/4000 Training loss: 0.0430 0.4433 sec/batch\n",
      "Epoch 465/2000  Iteration 929/4000 Training loss: 0.0695 0.4434 sec/batch\n",
      "Epoch 465/2000  Iteration 930/4000 Training loss: 0.0435 0.4436 sec/batch\n",
      "Epoch 466/2000  Iteration 931/4000 Training loss: 0.0680 0.4429 sec/batch\n",
      "Epoch 466/2000  Iteration 932/4000 Training loss: 0.0421 0.4434 sec/batch\n",
      "Epoch 467/2000  Iteration 933/4000 Training loss: 0.0677 0.4432 sec/batch\n",
      "Epoch 467/2000  Iteration 934/4000 Training loss: 0.0421 0.4439 sec/batch\n",
      "Epoch 468/2000  Iteration 935/4000 Training loss: 0.0689 0.4434 sec/batch\n",
      "Epoch 468/2000  Iteration 936/4000 Training loss: 0.0412 0.4434 sec/batch\n",
      "Epoch 469/2000  Iteration 937/4000 Training loss: 0.0684 0.4432 sec/batch\n",
      "Epoch 469/2000  Iteration 938/4000 Training loss: 0.0421 0.4436 sec/batch\n",
      "Epoch 470/2000  Iteration 939/4000 Training loss: 0.0688 0.4434 sec/batch\n",
      "Epoch 470/2000  Iteration 940/4000 Training loss: 0.0426 0.4435 sec/batch\n",
      "Epoch 471/2000  Iteration 941/4000 Training loss: 0.0690 0.4431 sec/batch\n",
      "Epoch 471/2000  Iteration 942/4000 Training loss: 0.0421 0.4430 sec/batch\n",
      "Epoch 472/2000  Iteration 943/4000 Training loss: 0.0689 0.4431 sec/batch\n",
      "Epoch 472/2000  Iteration 944/4000 Training loss: 0.0422 0.4437 sec/batch\n",
      "Epoch 473/2000  Iteration 945/4000 Training loss: 0.0662 0.4430 sec/batch\n",
      "Epoch 473/2000  Iteration 946/4000 Training loss: 0.0398 0.4436 sec/batch\n",
      "Epoch 474/2000  Iteration 947/4000 Training loss: 0.0663 0.4432 sec/batch\n",
      "Epoch 474/2000  Iteration 948/4000 Training loss: 0.0407 0.4432 sec/batch\n",
      "Epoch 475/2000  Iteration 949/4000 Training loss: 0.0660 0.4431 sec/batch\n",
      "Epoch 475/2000  Iteration 950/4000 Training loss: 0.0402 0.4435 sec/batch\n",
      "Epoch 476/2000  Iteration 951/4000 Training loss: 0.0661 0.4433 sec/batch\n",
      "Epoch 476/2000  Iteration 952/4000 Training loss: 0.0401 0.4433 sec/batch\n",
      "Epoch 477/2000  Iteration 953/4000 Training loss: 0.0668 0.4429 sec/batch\n",
      "Epoch 477/2000  Iteration 954/4000 Training loss: 0.0401 0.4435 sec/batch\n",
      "Epoch 478/2000  Iteration 955/4000 Training loss: 0.0656 0.4427 sec/batch\n",
      "Epoch 478/2000  Iteration 956/4000 Training loss: 0.0402 0.4428 sec/batch\n",
      "Epoch 479/2000  Iteration 957/4000 Training loss: 0.0676 0.4430 sec/batch\n",
      "Epoch 479/2000  Iteration 958/4000 Training loss: 0.0412 0.4433 sec/batch\n",
      "Epoch 480/2000  Iteration 959/4000 Training loss: 0.0655 0.4432 sec/batch\n",
      "Epoch 480/2000  Iteration 960/4000 Training loss: 0.0397 0.4432 sec/batch\n",
      "Epoch 481/2000  Iteration 961/4000 Training loss: 0.0667 0.4430 sec/batch\n",
      "Epoch 481/2000  Iteration 962/4000 Training loss: 0.0400 0.4432 sec/batch\n",
      "Epoch 482/2000  Iteration 963/4000 Training loss: 0.0665 0.4434 sec/batch\n",
      "Epoch 482/2000  Iteration 964/4000 Training loss: 0.0403 0.4432 sec/batch\n",
      "Epoch 483/2000  Iteration 965/4000 Training loss: 0.0655 0.4432 sec/batch\n",
      "Epoch 483/2000  Iteration 966/4000 Training loss: 0.0400 0.4432 sec/batch\n",
      "Epoch 484/2000  Iteration 967/4000 Training loss: 0.0647 0.4429 sec/batch\n",
      "Epoch 484/2000  Iteration 968/4000 Training loss: 0.0397 0.4436 sec/batch\n",
      "Epoch 485/2000  Iteration 969/4000 Training loss: 0.0642 0.4429 sec/batch\n",
      "Epoch 485/2000  Iteration 970/4000 Training loss: 0.0392 0.4433 sec/batch\n",
      "Epoch 486/2000  Iteration 971/4000 Training loss: 0.0643 0.4432 sec/batch\n",
      "Epoch 486/2000  Iteration 972/4000 Training loss: 0.0402 0.4429 sec/batch\n",
      "Epoch 487/2000  Iteration 973/4000 Training loss: 0.0653 0.4429 sec/batch\n",
      "Epoch 487/2000  Iteration 974/4000 Training loss: 0.0391 0.4436 sec/batch\n",
      "Epoch 488/2000  Iteration 975/4000 Training loss: 0.0652 0.4436 sec/batch\n",
      "Epoch 488/2000  Iteration 976/4000 Training loss: 0.0399 0.4435 sec/batch\n",
      "Epoch 489/2000  Iteration 977/4000 Training loss: 0.0632 0.4431 sec/batch\n",
      "Epoch 489/2000  Iteration 978/4000 Training loss: 0.0388 0.4431 sec/batch\n",
      "Epoch 490/2000  Iteration 979/4000 Training loss: 0.0649 0.4430 sec/batch\n",
      "Epoch 490/2000  Iteration 980/4000 Training loss: 0.0402 0.4433 sec/batch\n",
      "Epoch 491/2000  Iteration 981/4000 Training loss: 0.0644 0.4434 sec/batch\n",
      "Epoch 491/2000  Iteration 982/4000 Training loss: 0.0394 0.4434 sec/batch\n",
      "Epoch 492/2000  Iteration 983/4000 Training loss: 0.0641 0.4428 sec/batch\n",
      "Epoch 492/2000  Iteration 984/4000 Training loss: 0.0392 0.4432 sec/batch\n",
      "Epoch 493/2000  Iteration 985/4000 Training loss: 0.0638 0.4429 sec/batch\n",
      "Epoch 493/2000  Iteration 986/4000 Training loss: 0.0392 0.4433 sec/batch\n",
      "Epoch 494/2000  Iteration 987/4000 Training loss: 0.0636 0.4427 sec/batch\n",
      "Epoch 494/2000  Iteration 988/4000 Training loss: 0.0384 0.4434 sec/batch\n",
      "Epoch 495/2000  Iteration 989/4000 Training loss: 0.0653 0.4428 sec/batch\n",
      "Epoch 495/2000  Iteration 990/4000 Training loss: 0.0394 0.4429 sec/batch\n",
      "Epoch 496/2000  Iteration 991/4000 Training loss: 0.0639 0.4433 sec/batch\n",
      "Epoch 496/2000  Iteration 992/4000 Training loss: 0.0389 0.4442 sec/batch\n",
      "Epoch 497/2000  Iteration 993/4000 Training loss: 0.0647 0.4432 sec/batch\n",
      "Epoch 497/2000  Iteration 994/4000 Training loss: 0.0389 0.4431 sec/batch\n",
      "Epoch 498/2000  Iteration 995/4000 Training loss: 0.0638 0.4429 sec/batch\n",
      "Epoch 498/2000  Iteration 996/4000 Training loss: 0.0381 0.4438 sec/batch\n",
      "Epoch 499/2000  Iteration 997/4000 Training loss: 0.0628 0.4433 sec/batch\n",
      "Epoch 499/2000  Iteration 998/4000 Training loss: 0.0381 0.4431 sec/batch\n",
      "Epoch 500/2000  Iteration 999/4000 Training loss: 0.0636 0.4429 sec/batch\n",
      "Epoch 500/2000  Iteration 1000/4000 Training loss: 0.0386 0.4438 sec/batch\n",
      "Validation loss: 5.43665 Saving checkpoint!\n",
      "Epoch 501/2000  Iteration 1001/4000 Training loss: 0.0628 0.4435 sec/batch\n",
      "Epoch 501/2000  Iteration 1002/4000 Training loss: 0.0389 0.4436 sec/batch\n",
      "Epoch 502/2000  Iteration 1003/4000 Training loss: 0.0633 0.4433 sec/batch\n",
      "Epoch 502/2000  Iteration 1004/4000 Training loss: 0.0383 0.4433 sec/batch\n",
      "Epoch 503/2000  Iteration 1005/4000 Training loss: 0.0626 0.4429 sec/batch\n",
      "Epoch 503/2000  Iteration 1006/4000 Training loss: 0.0381 0.4433 sec/batch\n",
      "Epoch 504/2000  Iteration 1007/4000 Training loss: 0.0616 0.4432 sec/batch\n",
      "Epoch 504/2000  Iteration 1008/4000 Training loss: 0.0372 0.4434 sec/batch\n",
      "Epoch 505/2000  Iteration 1009/4000 Training loss: 0.0615 0.4431 sec/batch\n",
      "Epoch 505/2000  Iteration 1010/4000 Training loss: 0.0367 0.4432 sec/batch\n",
      "Epoch 506/2000  Iteration 1011/4000 Training loss: 0.0614 0.4433 sec/batch\n",
      "Epoch 506/2000  Iteration 1012/4000 Training loss: 0.0377 0.4433 sec/batch\n",
      "Epoch 507/2000  Iteration 1013/4000 Training loss: 0.0632 0.4428 sec/batch\n",
      "Epoch 507/2000  Iteration 1014/4000 Training loss: 0.0380 0.4434 sec/batch\n",
      "Epoch 508/2000  Iteration 1015/4000 Training loss: 0.0611 0.4432 sec/batch\n",
      "Epoch 508/2000  Iteration 1016/4000 Training loss: 0.0374 0.4434 sec/batch\n",
      "Epoch 509/2000  Iteration 1017/4000 Training loss: 0.0609 0.4429 sec/batch\n",
      "Epoch 509/2000  Iteration 1018/4000 Training loss: 0.0367 0.4437 sec/batch\n",
      "Epoch 510/2000  Iteration 1019/4000 Training loss: 0.0598 0.4429 sec/batch\n",
      "Epoch 510/2000  Iteration 1020/4000 Training loss: 0.0359 0.4434 sec/batch\n",
      "Epoch 511/2000  Iteration 1021/4000 Training loss: 0.0624 0.4430 sec/batch\n",
      "Epoch 511/2000  Iteration 1022/4000 Training loss: 0.0383 0.4430 sec/batch\n",
      "Epoch 512/2000  Iteration 1023/4000 Training loss: 0.0619 0.4432 sec/batch\n",
      "Epoch 512/2000  Iteration 1024/4000 Training loss: 0.0379 0.4436 sec/batch\n",
      "Epoch 513/2000  Iteration 1025/4000 Training loss: 0.0590 0.4432 sec/batch\n",
      "Epoch 513/2000  Iteration 1026/4000 Training loss: 0.0361 0.4433 sec/batch\n",
      "Epoch 514/2000  Iteration 1027/4000 Training loss: 0.0625 0.4429 sec/batch\n",
      "Epoch 514/2000  Iteration 1028/4000 Training loss: 0.0387 0.4428 sec/batch\n",
      "Epoch 515/2000  Iteration 1029/4000 Training loss: 0.0603 0.4429 sec/batch\n",
      "Epoch 515/2000  Iteration 1030/4000 Training loss: 0.0371 0.4433 sec/batch\n",
      "Epoch 516/2000  Iteration 1031/4000 Training loss: 0.0592 0.4432 sec/batch\n",
      "Epoch 516/2000  Iteration 1032/4000 Training loss: 0.0359 0.4430 sec/batch\n",
      "Epoch 517/2000  Iteration 1033/4000 Training loss: 0.0600 0.4430 sec/batch\n",
      "Epoch 517/2000  Iteration 1034/4000 Training loss: 0.0369 0.4435 sec/batch\n",
      "Epoch 518/2000  Iteration 1035/4000 Training loss: 0.0612 0.4429 sec/batch\n",
      "Epoch 518/2000  Iteration 1036/4000 Training loss: 0.0365 0.4432 sec/batch\n",
      "Epoch 519/2000  Iteration 1037/4000 Training loss: 0.0587 0.4434 sec/batch\n",
      "Epoch 519/2000  Iteration 1038/4000 Training loss: 0.0358 0.4437 sec/batch\n",
      "Epoch 520/2000  Iteration 1039/4000 Training loss: 0.0620 0.4436 sec/batch\n",
      "Epoch 520/2000  Iteration 1040/4000 Training loss: 0.0371 0.4434 sec/batch\n",
      "Epoch 521/2000  Iteration 1041/4000 Training loss: 0.0594 0.4439 sec/batch\n",
      "Epoch 521/2000  Iteration 1042/4000 Training loss: 0.0358 0.4441 sec/batch\n",
      "Epoch 522/2000  Iteration 1043/4000 Training loss: 0.0588 0.4434 sec/batch\n",
      "Epoch 522/2000  Iteration 1044/4000 Training loss: 0.0360 0.4434 sec/batch\n",
      "Epoch 523/2000  Iteration 1045/4000 Training loss: 0.0588 0.4427 sec/batch\n",
      "Epoch 523/2000  Iteration 1046/4000 Training loss: 0.0358 0.4437 sec/batch\n",
      "Epoch 524/2000  Iteration 1047/4000 Training loss: 0.0600 0.4431 sec/batch\n",
      "Epoch 524/2000  Iteration 1048/4000 Training loss: 0.0360 0.4433 sec/batch\n",
      "Epoch 525/2000  Iteration 1049/4000 Training loss: 0.0592 0.4432 sec/batch\n",
      "Epoch 525/2000  Iteration 1050/4000 Training loss: 0.0352 0.4440 sec/batch\n",
      "Epoch 526/2000  Iteration 1051/4000 Training loss: 0.0592 0.4431 sec/batch\n",
      "Epoch 526/2000  Iteration 1052/4000 Training loss: 0.0350 0.4439 sec/batch\n",
      "Epoch 527/2000  Iteration 1053/4000 Training loss: 0.0581 0.4426 sec/batch\n",
      "Epoch 527/2000  Iteration 1054/4000 Training loss: 0.0354 0.4431 sec/batch\n",
      "Epoch 528/2000  Iteration 1055/4000 Training loss: 0.0610 0.4427 sec/batch\n",
      "Epoch 528/2000  Iteration 1056/4000 Training loss: 0.0362 0.4437 sec/batch\n",
      "Epoch 529/2000  Iteration 1057/4000 Training loss: 0.0580 0.4432 sec/batch\n",
      "Epoch 529/2000  Iteration 1058/4000 Training loss: 0.0351 0.4432 sec/batch\n",
      "Epoch 530/2000  Iteration 1059/4000 Training loss: 0.0581 0.4426 sec/batch\n",
      "Epoch 530/2000  Iteration 1060/4000 Training loss: 0.0342 0.4429 sec/batch\n",
      "Epoch 531/2000  Iteration 1061/4000 Training loss: 0.0589 0.4429 sec/batch\n",
      "Epoch 531/2000  Iteration 1062/4000 Training loss: 0.0349 0.4434 sec/batch\n",
      "Epoch 532/2000  Iteration 1063/4000 Training loss: 0.0563 0.4435 sec/batch\n",
      "Epoch 532/2000  Iteration 1064/4000 Training loss: 0.0332 0.4430 sec/batch\n",
      "Epoch 533/2000  Iteration 1065/4000 Training loss: 0.0575 0.4431 sec/batch\n",
      "Epoch 533/2000  Iteration 1066/4000 Training loss: 0.0339 0.4435 sec/batch\n",
      "Epoch 534/2000  Iteration 1067/4000 Training loss: 0.0555 0.4431 sec/batch\n",
      "Epoch 534/2000  Iteration 1068/4000 Training loss: 0.0328 0.4433 sec/batch\n",
      "Epoch 535/2000  Iteration 1069/4000 Training loss: 0.0575 0.4434 sec/batch\n",
      "Epoch 535/2000  Iteration 1070/4000 Training loss: 0.0337 0.4437 sec/batch\n",
      "Epoch 536/2000  Iteration 1071/4000 Training loss: 0.0567 0.4435 sec/batch\n",
      "Epoch 536/2000  Iteration 1072/4000 Training loss: 0.0335 0.4438 sec/batch\n",
      "Epoch 537/2000  Iteration 1073/4000 Training loss: 0.0573 0.4435 sec/batch\n",
      "Epoch 537/2000  Iteration 1074/4000 Training loss: 0.0339 0.4433 sec/batch\n",
      "Epoch 538/2000  Iteration 1075/4000 Training loss: 0.0572 0.4432 sec/batch\n",
      "Epoch 538/2000  Iteration 1076/4000 Training loss: 0.0341 0.4437 sec/batch\n",
      "Epoch 539/2000  Iteration 1077/4000 Training loss: 0.0559 0.4438 sec/batch\n",
      "Epoch 539/2000  Iteration 1078/4000 Training loss: 0.0325 0.4444 sec/batch\n",
      "Epoch 540/2000  Iteration 1079/4000 Training loss: 0.0571 0.4439 sec/batch\n",
      "Epoch 540/2000  Iteration 1080/4000 Training loss: 0.0344 0.4439 sec/batch\n",
      "Epoch 541/2000  Iteration 1081/4000 Training loss: 0.0564 0.4434 sec/batch\n",
      "Epoch 541/2000  Iteration 1082/4000 Training loss: 0.0335 0.4440 sec/batch\n",
      "Epoch 542/2000  Iteration 1083/4000 Training loss: 0.0568 0.4435 sec/batch\n",
      "Epoch 542/2000  Iteration 1084/4000 Training loss: 0.0336 0.4435 sec/batch\n",
      "Epoch 543/2000  Iteration 1085/4000 Training loss: 0.0564 0.4433 sec/batch\n",
      "Epoch 543/2000  Iteration 1086/4000 Training loss: 0.0334 0.4433 sec/batch\n",
      "Epoch 544/2000  Iteration 1087/4000 Training loss: 0.0568 0.4434 sec/batch\n",
      "Epoch 544/2000  Iteration 1088/4000 Training loss: 0.0339 0.4433 sec/batch\n",
      "Epoch 545/2000  Iteration 1089/4000 Training loss: 0.0564 0.4432 sec/batch\n",
      "Epoch 545/2000  Iteration 1090/4000 Training loss: 0.0330 0.4432 sec/batch\n",
      "Epoch 546/2000  Iteration 1091/4000 Training loss: 0.0562 0.4433 sec/batch\n",
      "Epoch 546/2000  Iteration 1092/4000 Training loss: 0.0338 0.4438 sec/batch\n",
      "Epoch 547/2000  Iteration 1093/4000 Training loss: 0.0555 0.4430 sec/batch\n",
      "Epoch 547/2000  Iteration 1094/4000 Training loss: 0.0342 0.4436 sec/batch\n",
      "Epoch 548/2000  Iteration 1095/4000 Training loss: 0.0563 0.4428 sec/batch\n",
      "Epoch 548/2000  Iteration 1096/4000 Training loss: 0.0339 0.4442 sec/batch\n",
      "Epoch 549/2000  Iteration 1097/4000 Training loss: 0.0557 0.4426 sec/batch\n",
      "Epoch 549/2000  Iteration 1098/4000 Training loss: 0.0339 0.4435 sec/batch\n",
      "Epoch 550/2000  Iteration 1099/4000 Training loss: 0.0562 0.4429 sec/batch\n",
      "Epoch 550/2000  Iteration 1100/4000 Training loss: 0.0332 0.4430 sec/batch\n",
      "Epoch 551/2000  Iteration 1101/4000 Training loss: 0.0557 0.4434 sec/batch\n",
      "Epoch 551/2000  Iteration 1102/4000 Training loss: 0.0336 0.4433 sec/batch\n",
      "Epoch 552/2000  Iteration 1103/4000 Training loss: 0.0547 0.4432 sec/batch\n",
      "Epoch 552/2000  Iteration 1104/4000 Training loss: 0.0326 0.4435 sec/batch\n",
      "Epoch 553/2000  Iteration 1105/4000 Training loss: 0.0556 0.4434 sec/batch\n",
      "Epoch 553/2000  Iteration 1106/4000 Training loss: 0.0332 0.4435 sec/batch\n",
      "Epoch 554/2000  Iteration 1107/4000 Training loss: 0.0556 0.4431 sec/batch\n",
      "Epoch 554/2000  Iteration 1108/4000 Training loss: 0.0340 0.4432 sec/batch\n",
      "Epoch 555/2000  Iteration 1109/4000 Training loss: 0.0543 0.4433 sec/batch\n",
      "Epoch 555/2000  Iteration 1110/4000 Training loss: 0.0331 0.4435 sec/batch\n",
      "Epoch 556/2000  Iteration 1111/4000 Training loss: 0.0550 0.4432 sec/batch\n",
      "Epoch 556/2000  Iteration 1112/4000 Training loss: 0.0328 0.4436 sec/batch\n",
      "Epoch 557/2000  Iteration 1113/4000 Training loss: 0.0554 0.4430 sec/batch\n",
      "Epoch 557/2000  Iteration 1114/4000 Training loss: 0.0331 0.4432 sec/batch\n",
      "Epoch 558/2000  Iteration 1115/4000 Training loss: 0.0537 0.4431 sec/batch\n",
      "Epoch 558/2000  Iteration 1116/4000 Training loss: 0.0321 0.4434 sec/batch\n",
      "Epoch 559/2000  Iteration 1117/4000 Training loss: 0.0571 0.4429 sec/batch\n",
      "Epoch 559/2000  Iteration 1118/4000 Training loss: 0.0342 0.4431 sec/batch\n",
      "Epoch 560/2000  Iteration 1119/4000 Training loss: 0.0570 0.4428 sec/batch\n",
      "Epoch 560/2000  Iteration 1120/4000 Training loss: 0.0338 0.4437 sec/batch\n",
      "Epoch 561/2000  Iteration 1121/4000 Training loss: 0.0542 0.4429 sec/batch\n",
      "Epoch 561/2000  Iteration 1122/4000 Training loss: 0.0320 0.4436 sec/batch\n",
      "Epoch 562/2000  Iteration 1123/4000 Training loss: 0.0554 0.4432 sec/batch\n",
      "Epoch 562/2000  Iteration 1124/4000 Training loss: 0.0334 0.4434 sec/batch\n",
      "Epoch 563/2000  Iteration 1125/4000 Training loss: 0.0576 0.4431 sec/batch\n",
      "Epoch 563/2000  Iteration 1126/4000 Training loss: 0.0345 0.4440 sec/batch\n",
      "Epoch 564/2000  Iteration 1127/4000 Training loss: 0.0543 0.4437 sec/batch\n",
      "Epoch 564/2000  Iteration 1128/4000 Training loss: 0.0324 0.4439 sec/batch\n",
      "Epoch 565/2000  Iteration 1129/4000 Training loss: 0.0553 0.4441 sec/batch\n",
      "Epoch 565/2000  Iteration 1130/4000 Training loss: 0.0331 0.4433 sec/batch\n",
      "Epoch 566/2000  Iteration 1131/4000 Training loss: 0.0544 0.4434 sec/batch\n",
      "Epoch 566/2000  Iteration 1132/4000 Training loss: 0.0328 0.4433 sec/batch\n",
      "Epoch 567/2000  Iteration 1133/4000 Training loss: 0.0544 0.4429 sec/batch\n",
      "Epoch 567/2000  Iteration 1134/4000 Training loss: 0.0320 0.4434 sec/batch\n",
      "Epoch 568/2000  Iteration 1135/4000 Training loss: 0.0541 0.4430 sec/batch\n",
      "Epoch 568/2000  Iteration 1136/4000 Training loss: 0.0327 0.4431 sec/batch\n",
      "Epoch 569/2000  Iteration 1137/4000 Training loss: 0.0528 0.4430 sec/batch\n",
      "Epoch 569/2000  Iteration 1138/4000 Training loss: 0.0325 0.4437 sec/batch\n",
      "Epoch 570/2000  Iteration 1139/4000 Training loss: 0.0540 0.4427 sec/batch\n",
      "Epoch 570/2000  Iteration 1140/4000 Training loss: 0.0321 0.4431 sec/batch\n",
      "Epoch 571/2000  Iteration 1141/4000 Training loss: 0.0552 0.4430 sec/batch\n",
      "Epoch 571/2000  Iteration 1142/4000 Training loss: 0.0329 0.4434 sec/batch\n",
      "Epoch 572/2000  Iteration 1143/4000 Training loss: 0.0538 0.4431 sec/batch\n",
      "Epoch 572/2000  Iteration 1144/4000 Training loss: 0.0314 0.4432 sec/batch\n",
      "Epoch 573/2000  Iteration 1145/4000 Training loss: 0.0535 0.4434 sec/batch\n",
      "Epoch 573/2000  Iteration 1146/4000 Training loss: 0.0316 0.4435 sec/batch\n",
      "Epoch 574/2000  Iteration 1147/4000 Training loss: 0.0527 0.4432 sec/batch\n",
      "Epoch 574/2000  Iteration 1148/4000 Training loss: 0.0309 0.4436 sec/batch\n",
      "Epoch 575/2000  Iteration 1149/4000 Training loss: 0.0541 0.4435 sec/batch\n",
      "Epoch 575/2000  Iteration 1150/4000 Training loss: 0.0320 0.4434 sec/batch\n",
      "Epoch 576/2000  Iteration 1151/4000 Training loss: 0.0533 0.4434 sec/batch\n",
      "Epoch 576/2000  Iteration 1152/4000 Training loss: 0.0313 0.4438 sec/batch\n",
      "Epoch 577/2000  Iteration 1153/4000 Training loss: 0.0529 0.4429 sec/batch\n",
      "Epoch 577/2000  Iteration 1154/4000 Training loss: 0.0314 0.4429 sec/batch\n",
      "Epoch 578/2000  Iteration 1155/4000 Training loss: 0.0531 0.4436 sec/batch\n",
      "Epoch 578/2000  Iteration 1156/4000 Training loss: 0.0316 0.4433 sec/batch\n",
      "Epoch 579/2000  Iteration 1157/4000 Training loss: 0.0516 0.4432 sec/batch\n",
      "Epoch 579/2000  Iteration 1158/4000 Training loss: 0.0306 0.4437 sec/batch\n",
      "Epoch 580/2000  Iteration 1159/4000 Training loss: 0.0531 0.4428 sec/batch\n",
      "Epoch 580/2000  Iteration 1160/4000 Training loss: 0.0315 0.4434 sec/batch\n",
      "Epoch 581/2000  Iteration 1161/4000 Training loss: 0.0522 0.4432 sec/batch\n",
      "Epoch 581/2000  Iteration 1162/4000 Training loss: 0.0305 0.4435 sec/batch\n",
      "Epoch 582/2000  Iteration 1163/4000 Training loss: 0.0511 0.4431 sec/batch\n",
      "Epoch 582/2000  Iteration 1164/4000 Training loss: 0.0302 0.4433 sec/batch\n",
      "Epoch 583/2000  Iteration 1165/4000 Training loss: 0.0511 0.4431 sec/batch\n",
      "Epoch 583/2000  Iteration 1166/4000 Training loss: 0.0306 0.4433 sec/batch\n",
      "Epoch 584/2000  Iteration 1167/4000 Training loss: 0.0518 0.4430 sec/batch\n",
      "Epoch 584/2000  Iteration 1168/4000 Training loss: 0.0311 0.4432 sec/batch\n",
      "Epoch 585/2000  Iteration 1169/4000 Training loss: 0.0528 0.4426 sec/batch\n",
      "Epoch 585/2000  Iteration 1170/4000 Training loss: 0.0312 0.4432 sec/batch\n",
      "Epoch 586/2000  Iteration 1171/4000 Training loss: 0.0513 0.4428 sec/batch\n",
      "Epoch 586/2000  Iteration 1172/4000 Training loss: 0.0306 0.4436 sec/batch\n",
      "Epoch 587/2000  Iteration 1173/4000 Training loss: 0.0512 0.4432 sec/batch\n",
      "Epoch 587/2000  Iteration 1174/4000 Training loss: 0.0301 0.4433 sec/batch\n",
      "Epoch 588/2000  Iteration 1175/4000 Training loss: 0.0514 0.4427 sec/batch\n",
      "Epoch 588/2000  Iteration 1176/4000 Training loss: 0.0301 0.4434 sec/batch\n",
      "Epoch 589/2000  Iteration 1177/4000 Training loss: 0.0515 0.4435 sec/batch\n",
      "Epoch 589/2000  Iteration 1178/4000 Training loss: 0.0302 0.4432 sec/batch\n",
      "Epoch 590/2000  Iteration 1179/4000 Training loss: 0.0502 0.4430 sec/batch\n",
      "Epoch 590/2000  Iteration 1180/4000 Training loss: 0.0298 0.4432 sec/batch\n",
      "Epoch 591/2000  Iteration 1181/4000 Training loss: 0.0503 0.4430 sec/batch\n",
      "Epoch 591/2000  Iteration 1182/4000 Training loss: 0.0302 0.4434 sec/batch\n",
      "Epoch 592/2000  Iteration 1183/4000 Training loss: 0.0508 0.4431 sec/batch\n",
      "Epoch 592/2000  Iteration 1184/4000 Training loss: 0.0300 0.4436 sec/batch\n",
      "Epoch 593/2000  Iteration 1185/4000 Training loss: 0.0512 0.4429 sec/batch\n",
      "Epoch 593/2000  Iteration 1186/4000 Training loss: 0.0302 0.4433 sec/batch\n",
      "Epoch 594/2000  Iteration 1187/4000 Training loss: 0.0503 0.4433 sec/batch\n",
      "Epoch 594/2000  Iteration 1188/4000 Training loss: 0.0296 0.4432 sec/batch\n",
      "Epoch 595/2000  Iteration 1189/4000 Training loss: 0.0510 0.4431 sec/batch\n",
      "Epoch 595/2000  Iteration 1190/4000 Training loss: 0.0298 0.4433 sec/batch\n",
      "Epoch 596/2000  Iteration 1191/4000 Training loss: 0.0511 0.4432 sec/batch\n",
      "Epoch 596/2000  Iteration 1192/4000 Training loss: 0.0297 0.4432 sec/batch\n",
      "Epoch 597/2000  Iteration 1193/4000 Training loss: 0.0502 0.4430 sec/batch\n",
      "Epoch 597/2000  Iteration 1194/4000 Training loss: 0.0299 0.4429 sec/batch\n",
      "Epoch 598/2000  Iteration 1195/4000 Training loss: 0.0490 0.4430 sec/batch\n",
      "Epoch 598/2000  Iteration 1196/4000 Training loss: 0.0291 0.4434 sec/batch\n",
      "Epoch 599/2000  Iteration 1197/4000 Training loss: 0.0495 0.4430 sec/batch\n",
      "Epoch 599/2000  Iteration 1198/4000 Training loss: 0.0293 0.4433 sec/batch\n",
      "Epoch 600/2000  Iteration 1199/4000 Training loss: 0.0490 0.4432 sec/batch\n",
      "Epoch 600/2000  Iteration 1200/4000 Training loss: 0.0290 0.4434 sec/batch\n",
      "Validation loss: 5.69397 Saving checkpoint!\n",
      "Epoch 601/2000  Iteration 1201/4000 Training loss: 0.0496 0.4432 sec/batch\n",
      "Epoch 601/2000  Iteration 1202/4000 Training loss: 0.0290 0.4434 sec/batch\n",
      "Epoch 602/2000  Iteration 1203/4000 Training loss: 0.0504 0.4427 sec/batch\n",
      "Epoch 602/2000  Iteration 1204/4000 Training loss: 0.0296 0.4429 sec/batch\n",
      "Epoch 603/2000  Iteration 1205/4000 Training loss: 0.0508 0.4430 sec/batch\n",
      "Epoch 603/2000  Iteration 1206/4000 Training loss: 0.0295 0.4435 sec/batch\n",
      "Epoch 604/2000  Iteration 1207/4000 Training loss: 0.0496 0.4429 sec/batch\n",
      "Epoch 604/2000  Iteration 1208/4000 Training loss: 0.0290 0.4433 sec/batch\n",
      "Epoch 605/2000  Iteration 1209/4000 Training loss: 0.0500 0.4425 sec/batch\n",
      "Epoch 605/2000  Iteration 1210/4000 Training loss: 0.0299 0.4432 sec/batch\n",
      "Epoch 606/2000  Iteration 1211/4000 Training loss: 0.0496 0.4432 sec/batch\n",
      "Epoch 606/2000  Iteration 1212/4000 Training loss: 0.0292 0.4433 sec/batch\n",
      "Epoch 607/2000  Iteration 1213/4000 Training loss: 0.0499 0.4431 sec/batch\n",
      "Epoch 607/2000  Iteration 1214/4000 Training loss: 0.0290 0.4430 sec/batch\n",
      "Epoch 608/2000  Iteration 1215/4000 Training loss: 0.0490 0.4428 sec/batch\n",
      "Epoch 608/2000  Iteration 1216/4000 Training loss: 0.0291 0.4430 sec/batch\n",
      "Epoch 609/2000  Iteration 1217/4000 Training loss: 0.0502 0.4428 sec/batch\n",
      "Epoch 609/2000  Iteration 1218/4000 Training loss: 0.0297 0.4434 sec/batch\n",
      "Epoch 610/2000  Iteration 1219/4000 Training loss: 0.0494 0.4434 sec/batch\n",
      "Epoch 610/2000  Iteration 1220/4000 Training loss: 0.0291 0.4431 sec/batch\n",
      "Epoch 611/2000  Iteration 1221/4000 Training loss: 0.0485 0.4428 sec/batch\n",
      "Epoch 611/2000  Iteration 1222/4000 Training loss: 0.0287 0.4431 sec/batch\n",
      "Epoch 612/2000  Iteration 1223/4000 Training loss: 0.0484 0.4430 sec/batch\n",
      "Epoch 612/2000  Iteration 1224/4000 Training loss: 0.0285 0.4433 sec/batch\n",
      "Epoch 613/2000  Iteration 1225/4000 Training loss: 0.0488 0.4426 sec/batch\n",
      "Epoch 613/2000  Iteration 1226/4000 Training loss: 0.0287 0.4434 sec/batch\n",
      "Epoch 614/2000  Iteration 1227/4000 Training loss: 0.0480 0.4432 sec/batch\n",
      "Epoch 614/2000  Iteration 1228/4000 Training loss: 0.0285 0.4437 sec/batch\n",
      "Epoch 615/2000  Iteration 1229/4000 Training loss: 0.0489 0.4430 sec/batch\n",
      "Epoch 615/2000  Iteration 1230/4000 Training loss: 0.0287 0.4432 sec/batch\n",
      "Epoch 616/2000  Iteration 1231/4000 Training loss: 0.0485 0.4429 sec/batch\n",
      "Epoch 616/2000  Iteration 1232/4000 Training loss: 0.0282 0.4430 sec/batch\n",
      "Epoch 617/2000  Iteration 1233/4000 Training loss: 0.0488 0.4430 sec/batch\n",
      "Epoch 617/2000  Iteration 1234/4000 Training loss: 0.0300 0.4436 sec/batch\n",
      "Epoch 618/2000  Iteration 1235/4000 Training loss: 0.0488 0.4434 sec/batch\n",
      "Epoch 618/2000  Iteration 1236/4000 Training loss: 0.0289 0.4431 sec/batch\n",
      "Epoch 619/2000  Iteration 1237/4000 Training loss: 0.0513 0.4433 sec/batch\n",
      "Epoch 619/2000  Iteration 1238/4000 Training loss: 0.0301 0.4434 sec/batch\n",
      "Epoch 620/2000  Iteration 1239/4000 Training loss: 0.0478 0.4430 sec/batch\n",
      "Epoch 620/2000  Iteration 1240/4000 Training loss: 0.0288 0.4432 sec/batch\n",
      "Epoch 621/2000  Iteration 1241/4000 Training loss: 0.0479 0.4427 sec/batch\n",
      "Epoch 621/2000  Iteration 1242/4000 Training loss: 0.0281 0.4432 sec/batch\n",
      "Epoch 622/2000  Iteration 1243/4000 Training loss: 0.0483 0.4429 sec/batch\n",
      "Epoch 622/2000  Iteration 1244/4000 Training loss: 0.0289 0.4436 sec/batch\n",
      "Epoch 623/2000  Iteration 1245/4000 Training loss: 0.0485 0.4430 sec/batch\n",
      "Epoch 623/2000  Iteration 1246/4000 Training loss: 0.0288 0.4432 sec/batch\n",
      "Epoch 624/2000  Iteration 1247/4000 Training loss: 0.0489 0.4429 sec/batch\n",
      "Epoch 624/2000  Iteration 1248/4000 Training loss: 0.0291 0.4436 sec/batch\n",
      "Epoch 625/2000  Iteration 1249/4000 Training loss: 0.0480 0.4430 sec/batch\n",
      "Epoch 625/2000  Iteration 1250/4000 Training loss: 0.0285 0.4434 sec/batch\n",
      "Epoch 626/2000  Iteration 1251/4000 Training loss: 0.0486 0.4431 sec/batch\n",
      "Epoch 626/2000  Iteration 1252/4000 Training loss: 0.0296 0.4432 sec/batch\n",
      "Epoch 627/2000  Iteration 1253/4000 Training loss: 0.0464 0.4430 sec/batch\n",
      "Epoch 627/2000  Iteration 1254/4000 Training loss: 0.0273 0.4435 sec/batch\n",
      "Epoch 628/2000  Iteration 1255/4000 Training loss: 0.0474 0.4430 sec/batch\n",
      "Epoch 628/2000  Iteration 1256/4000 Training loss: 0.0283 0.4433 sec/batch\n",
      "Epoch 629/2000  Iteration 1257/4000 Training loss: 0.0499 0.4459 sec/batch\n",
      "Epoch 629/2000  Iteration 1258/4000 Training loss: 0.0293 0.4436 sec/batch\n",
      "Epoch 630/2000  Iteration 1259/4000 Training loss: 0.0486 0.4431 sec/batch\n",
      "Epoch 630/2000  Iteration 1260/4000 Training loss: 0.0283 0.4434 sec/batch\n",
      "Epoch 631/2000  Iteration 1261/4000 Training loss: 0.0478 0.4432 sec/batch\n",
      "Epoch 631/2000  Iteration 1262/4000 Training loss: 0.0283 0.4436 sec/batch\n",
      "Epoch 632/2000  Iteration 1263/4000 Training loss: 0.0463 0.4432 sec/batch\n",
      "Epoch 632/2000  Iteration 1264/4000 Training loss: 0.0272 0.4434 sec/batch\n",
      "Epoch 633/2000  Iteration 1265/4000 Training loss: 0.0471 0.4436 sec/batch\n",
      "Epoch 633/2000  Iteration 1266/4000 Training loss: 0.0271 0.4432 sec/batch\n",
      "Epoch 634/2000  Iteration 1267/4000 Training loss: 0.0475 0.4432 sec/batch\n",
      "Epoch 634/2000  Iteration 1268/4000 Training loss: 0.0283 0.4436 sec/batch\n",
      "Epoch 635/2000  Iteration 1269/4000 Training loss: 0.0477 0.4433 sec/batch\n",
      "Epoch 635/2000  Iteration 1270/4000 Training loss: 0.0280 0.4434 sec/batch\n",
      "Epoch 636/2000  Iteration 1271/4000 Training loss: 0.0472 0.4432 sec/batch\n",
      "Epoch 636/2000  Iteration 1272/4000 Training loss: 0.0277 0.4432 sec/batch\n",
      "Epoch 637/2000  Iteration 1273/4000 Training loss: 0.0456 0.4428 sec/batch\n",
      "Epoch 637/2000  Iteration 1274/4000 Training loss: 0.0270 0.4435 sec/batch\n",
      "Epoch 638/2000  Iteration 1275/4000 Training loss: 0.0460 0.4432 sec/batch\n",
      "Epoch 638/2000  Iteration 1276/4000 Training loss: 0.0270 0.4432 sec/batch\n",
      "Epoch 639/2000  Iteration 1277/4000 Training loss: 0.0472 0.4432 sec/batch\n",
      "Epoch 639/2000  Iteration 1278/4000 Training loss: 0.0281 0.4434 sec/batch\n",
      "Epoch 640/2000  Iteration 1279/4000 Training loss: 0.0459 0.4429 sec/batch\n",
      "Epoch 640/2000  Iteration 1280/4000 Training loss: 0.0273 0.4433 sec/batch\n",
      "Epoch 641/2000  Iteration 1281/4000 Training loss: 0.0464 0.4437 sec/batch\n",
      "Epoch 641/2000  Iteration 1282/4000 Training loss: 0.0278 0.4432 sec/batch\n",
      "Epoch 642/2000  Iteration 1283/4000 Training loss: 0.0458 0.4433 sec/batch\n",
      "Epoch 642/2000  Iteration 1284/4000 Training loss: 0.0267 0.4435 sec/batch\n",
      "Epoch 643/2000  Iteration 1285/4000 Training loss: 0.0458 0.4427 sec/batch\n",
      "Epoch 643/2000  Iteration 1286/4000 Training loss: 0.0272 0.4429 sec/batch\n",
      "Epoch 644/2000  Iteration 1287/4000 Training loss: 0.0469 0.4429 sec/batch\n",
      "Epoch 644/2000  Iteration 1288/4000 Training loss: 0.0282 0.4431 sec/batch\n",
      "Epoch 645/2000  Iteration 1289/4000 Training loss: 0.0470 0.4430 sec/batch\n",
      "Epoch 645/2000  Iteration 1290/4000 Training loss: 0.0271 0.4437 sec/batch\n",
      "Epoch 646/2000  Iteration 1291/4000 Training loss: 0.0461 0.4434 sec/batch\n",
      "Epoch 646/2000  Iteration 1292/4000 Training loss: 0.0269 0.4434 sec/batch\n",
      "Epoch 647/2000  Iteration 1293/4000 Training loss: 0.0462 0.4433 sec/batch\n",
      "Epoch 647/2000  Iteration 1294/4000 Training loss: 0.0272 0.4431 sec/batch\n",
      "Epoch 648/2000  Iteration 1295/4000 Training loss: 0.0456 0.4431 sec/batch\n",
      "Epoch 648/2000  Iteration 1296/4000 Training loss: 0.0268 0.4431 sec/batch\n",
      "Epoch 649/2000  Iteration 1297/4000 Training loss: 0.0457 0.4429 sec/batch\n",
      "Epoch 649/2000  Iteration 1298/4000 Training loss: 0.0261 0.4435 sec/batch\n",
      "Epoch 650/2000  Iteration 1299/4000 Training loss: 0.0457 0.4427 sec/batch\n",
      "Epoch 650/2000  Iteration 1300/4000 Training loss: 0.0269 0.4434 sec/batch\n",
      "Epoch 651/2000  Iteration 1301/4000 Training loss: 0.0456 0.4435 sec/batch\n",
      "Epoch 651/2000  Iteration 1302/4000 Training loss: 0.0275 0.4436 sec/batch\n",
      "Epoch 652/2000  Iteration 1303/4000 Training loss: 0.0449 0.4431 sec/batch\n",
      "Epoch 652/2000  Iteration 1304/4000 Training loss: 0.0264 0.4439 sec/batch\n",
      "Epoch 653/2000  Iteration 1305/4000 Training loss: 0.0450 0.4435 sec/batch\n",
      "Epoch 653/2000  Iteration 1306/4000 Training loss: 0.0265 0.4434 sec/batch\n",
      "Epoch 654/2000  Iteration 1307/4000 Training loss: 0.0469 0.4429 sec/batch\n",
      "Epoch 654/2000  Iteration 1308/4000 Training loss: 0.0276 0.4437 sec/batch\n",
      "Epoch 655/2000  Iteration 1309/4000 Training loss: 0.0443 0.4433 sec/batch\n",
      "Epoch 655/2000  Iteration 1310/4000 Training loss: 0.0265 0.4433 sec/batch\n",
      "Epoch 656/2000  Iteration 1311/4000 Training loss: 0.0450 0.4434 sec/batch\n",
      "Epoch 656/2000  Iteration 1312/4000 Training loss: 0.0269 0.4435 sec/batch\n",
      "Epoch 657/2000  Iteration 1313/4000 Training loss: 0.0462 0.4433 sec/batch\n",
      "Epoch 657/2000  Iteration 1314/4000 Training loss: 0.0269 0.4433 sec/batch\n",
      "Epoch 658/2000  Iteration 1315/4000 Training loss: 0.0463 0.4436 sec/batch\n",
      "Epoch 658/2000  Iteration 1316/4000 Training loss: 0.0272 0.4434 sec/batch\n",
      "Epoch 659/2000  Iteration 1317/4000 Training loss: 0.0458 0.4430 sec/batch\n",
      "Epoch 659/2000  Iteration 1318/4000 Training loss: 0.0268 0.4432 sec/batch\n",
      "Epoch 660/2000  Iteration 1319/4000 Training loss: 0.0447 0.4433 sec/batch\n",
      "Epoch 660/2000  Iteration 1320/4000 Training loss: 0.0266 0.4432 sec/batch\n",
      "Epoch 661/2000  Iteration 1321/4000 Training loss: 0.0444 0.4431 sec/batch\n",
      "Epoch 661/2000  Iteration 1322/4000 Training loss: 0.0262 0.4437 sec/batch\n",
      "Epoch 662/2000  Iteration 1323/4000 Training loss: 0.0467 0.4431 sec/batch\n",
      "Epoch 662/2000  Iteration 1324/4000 Training loss: 0.0269 0.4430 sec/batch\n",
      "Epoch 663/2000  Iteration 1325/4000 Training loss: 0.0446 0.4435 sec/batch\n",
      "Epoch 663/2000  Iteration 1326/4000 Training loss: 0.0258 0.4429 sec/batch\n",
      "Epoch 664/2000  Iteration 1327/4000 Training loss: 0.0441 0.4429 sec/batch\n",
      "Epoch 664/2000  Iteration 1328/4000 Training loss: 0.0266 0.4432 sec/batch\n",
      "Epoch 665/2000  Iteration 1329/4000 Training loss: 0.0439 0.4427 sec/batch\n",
      "Epoch 665/2000  Iteration 1330/4000 Training loss: 0.0258 0.4434 sec/batch\n",
      "Epoch 666/2000  Iteration 1331/4000 Training loss: 0.0457 0.4427 sec/batch\n",
      "Epoch 666/2000  Iteration 1332/4000 Training loss: 0.0267 0.4435 sec/batch\n",
      "Epoch 667/2000  Iteration 1333/4000 Training loss: 0.0452 0.4435 sec/batch\n",
      "Epoch 667/2000  Iteration 1334/4000 Training loss: 0.0263 0.4428 sec/batch\n",
      "Epoch 668/2000  Iteration 1335/4000 Training loss: 0.0441 0.4431 sec/batch\n",
      "Epoch 668/2000  Iteration 1336/4000 Training loss: 0.0257 0.4432 sec/batch\n",
      "Epoch 669/2000  Iteration 1337/4000 Training loss: 0.0450 0.4432 sec/batch\n",
      "Epoch 669/2000  Iteration 1338/4000 Training loss: 0.0265 0.4432 sec/batch\n",
      "Epoch 670/2000  Iteration 1339/4000 Training loss: 0.0438 0.4433 sec/batch\n",
      "Epoch 670/2000  Iteration 1340/4000 Training loss: 0.0252 0.4425 sec/batch\n",
      "Epoch 671/2000  Iteration 1341/4000 Training loss: 0.0444 0.4433 sec/batch\n",
      "Epoch 671/2000  Iteration 1342/4000 Training loss: 0.0263 0.4431 sec/batch\n",
      "Epoch 672/2000  Iteration 1343/4000 Training loss: 0.0453 0.4430 sec/batch\n",
      "Epoch 672/2000  Iteration 1344/4000 Training loss: 0.0262 0.4431 sec/batch\n",
      "Epoch 673/2000  Iteration 1345/4000 Training loss: 0.0437 0.4431 sec/batch\n",
      "Epoch 673/2000  Iteration 1346/4000 Training loss: 0.0253 0.4435 sec/batch\n",
      "Epoch 674/2000  Iteration 1347/4000 Training loss: 0.0439 0.4449 sec/batch\n",
      "Epoch 674/2000  Iteration 1348/4000 Training loss: 0.0257 0.4437 sec/batch\n",
      "Epoch 675/2000  Iteration 1349/4000 Training loss: 0.0449 0.4432 sec/batch\n",
      "Epoch 675/2000  Iteration 1350/4000 Training loss: 0.0259 0.4433 sec/batch\n",
      "Epoch 676/2000  Iteration 1351/4000 Training loss: 0.0446 0.4428 sec/batch\n",
      "Epoch 676/2000  Iteration 1352/4000 Training loss: 0.0261 0.4430 sec/batch\n",
      "Epoch 677/2000  Iteration 1353/4000 Training loss: 0.0449 0.4426 sec/batch\n",
      "Epoch 677/2000  Iteration 1354/4000 Training loss: 0.0263 0.4433 sec/batch\n",
      "Epoch 678/2000  Iteration 1355/4000 Training loss: 0.0441 0.4433 sec/batch\n",
      "Epoch 678/2000  Iteration 1356/4000 Training loss: 0.0258 0.4437 sec/batch\n",
      "Epoch 679/2000  Iteration 1357/4000 Training loss: 0.0425 0.4429 sec/batch\n",
      "Epoch 679/2000  Iteration 1358/4000 Training loss: 0.0249 0.4432 sec/batch\n",
      "Epoch 680/2000  Iteration 1359/4000 Training loss: 0.0429 0.4431 sec/batch\n",
      "Epoch 680/2000  Iteration 1360/4000 Training loss: 0.0250 0.4436 sec/batch\n",
      "Epoch 681/2000  Iteration 1361/4000 Training loss: 0.0443 0.4433 sec/batch\n",
      "Epoch 681/2000  Iteration 1362/4000 Training loss: 0.0263 0.4433 sec/batch\n",
      "Epoch 682/2000  Iteration 1363/4000 Training loss: 0.0426 0.4429 sec/batch\n",
      "Epoch 682/2000  Iteration 1364/4000 Training loss: 0.0247 0.4435 sec/batch\n",
      "Epoch 683/2000  Iteration 1365/4000 Training loss: 0.0430 0.4429 sec/batch\n",
      "Epoch 683/2000  Iteration 1366/4000 Training loss: 0.0249 0.4434 sec/batch\n",
      "Epoch 684/2000  Iteration 1367/4000 Training loss: 0.0414 0.4430 sec/batch\n",
      "Epoch 684/2000  Iteration 1368/4000 Training loss: 0.0245 0.4431 sec/batch\n",
      "Epoch 685/2000  Iteration 1369/4000 Training loss: 0.0440 0.4436 sec/batch\n",
      "Epoch 685/2000  Iteration 1370/4000 Training loss: 0.0254 0.4434 sec/batch\n",
      "Epoch 686/2000  Iteration 1371/4000 Training loss: 0.0446 0.4440 sec/batch\n",
      "Epoch 686/2000  Iteration 1372/4000 Training loss: 0.0256 0.4436 sec/batch\n",
      "Epoch 687/2000  Iteration 1373/4000 Training loss: 0.0437 0.4430 sec/batch\n",
      "Epoch 687/2000  Iteration 1374/4000 Training loss: 0.0255 0.4437 sec/batch\n",
      "Epoch 688/2000  Iteration 1375/4000 Training loss: 0.0429 0.4433 sec/batch\n",
      "Epoch 688/2000  Iteration 1376/4000 Training loss: 0.0247 0.4439 sec/batch\n",
      "Epoch 689/2000  Iteration 1377/4000 Training loss: 0.0419 0.4434 sec/batch\n",
      "Epoch 689/2000  Iteration 1378/4000 Training loss: 0.0245 0.4436 sec/batch\n",
      "Epoch 690/2000  Iteration 1379/4000 Training loss: 0.0419 0.4432 sec/batch\n",
      "Epoch 690/2000  Iteration 1380/4000 Training loss: 0.0243 0.4437 sec/batch\n",
      "Epoch 691/2000  Iteration 1381/4000 Training loss: 0.0433 0.4430 sec/batch\n",
      "Epoch 691/2000  Iteration 1382/4000 Training loss: 0.0252 0.4439 sec/batch\n",
      "Epoch 692/2000  Iteration 1383/4000 Training loss: 0.0407 0.4433 sec/batch\n",
      "Epoch 692/2000  Iteration 1384/4000 Training loss: 0.0239 0.4449 sec/batch\n",
      "Epoch 693/2000  Iteration 1385/4000 Training loss: 0.0418 0.4435 sec/batch\n",
      "Epoch 693/2000  Iteration 1386/4000 Training loss: 0.0241 0.4441 sec/batch\n",
      "Epoch 694/2000  Iteration 1387/4000 Training loss: 0.0429 0.4433 sec/batch\n",
      "Epoch 694/2000  Iteration 1388/4000 Training loss: 0.0249 0.4437 sec/batch\n",
      "Epoch 695/2000  Iteration 1389/4000 Training loss: 0.0423 0.4438 sec/batch\n",
      "Epoch 695/2000  Iteration 1390/4000 Training loss: 0.0246 0.4440 sec/batch\n",
      "Epoch 696/2000  Iteration 1391/4000 Training loss: 0.0423 0.4433 sec/batch\n",
      "Epoch 696/2000  Iteration 1392/4000 Training loss: 0.0244 0.4432 sec/batch\n",
      "Epoch 697/2000  Iteration 1393/4000 Training loss: 0.0419 0.4439 sec/batch\n",
      "Epoch 697/2000  Iteration 1394/4000 Training loss: 0.0243 0.4433 sec/batch\n",
      "Epoch 698/2000  Iteration 1395/4000 Training loss: 0.0417 0.4441 sec/batch\n",
      "Epoch 698/2000  Iteration 1396/4000 Training loss: 0.0243 0.4438 sec/batch\n",
      "Epoch 699/2000  Iteration 1397/4000 Training loss: 0.0413 0.4431 sec/batch\n",
      "Epoch 699/2000  Iteration 1398/4000 Training loss: 0.0235 0.4435 sec/batch\n",
      "Epoch 700/2000  Iteration 1399/4000 Training loss: 0.0412 0.4437 sec/batch\n",
      "Epoch 700/2000  Iteration 1400/4000 Training loss: 0.0241 0.4430 sec/batch\n",
      "Validation loss: 5.84443 Saving checkpoint!\n",
      "Epoch 701/2000  Iteration 1401/4000 Training loss: 0.0408 0.4437 sec/batch\n",
      "Epoch 701/2000  Iteration 1402/4000 Training loss: 0.0237 0.4434 sec/batch\n",
      "Epoch 702/2000  Iteration 1403/4000 Training loss: 0.0424 0.4430 sec/batch\n",
      "Epoch 702/2000  Iteration 1404/4000 Training loss: 0.0245 0.4440 sec/batch\n",
      "Epoch 703/2000  Iteration 1405/4000 Training loss: 0.0407 0.4437 sec/batch\n",
      "Epoch 703/2000  Iteration 1406/4000 Training loss: 0.0234 0.4431 sec/batch\n",
      "Epoch 704/2000  Iteration 1407/4000 Training loss: 0.0410 0.4427 sec/batch\n",
      "Epoch 704/2000  Iteration 1408/4000 Training loss: 0.0239 0.4432 sec/batch\n",
      "Epoch 705/2000  Iteration 1409/4000 Training loss: 0.0404 0.4431 sec/batch\n",
      "Epoch 705/2000  Iteration 1410/4000 Training loss: 0.0231 0.4434 sec/batch\n",
      "Epoch 706/2000  Iteration 1411/4000 Training loss: 0.0393 0.4428 sec/batch\n",
      "Epoch 706/2000  Iteration 1412/4000 Training loss: 0.0225 0.4432 sec/batch\n",
      "Epoch 707/2000  Iteration 1413/4000 Training loss: 0.0399 0.4434 sec/batch\n",
      "Epoch 707/2000  Iteration 1414/4000 Training loss: 0.0227 0.4433 sec/batch\n",
      "Epoch 708/2000  Iteration 1415/4000 Training loss: 0.0408 0.4428 sec/batch\n",
      "Epoch 708/2000  Iteration 1416/4000 Training loss: 0.0241 0.4431 sec/batch\n",
      "Epoch 709/2000  Iteration 1417/4000 Training loss: 0.0411 0.4427 sec/batch\n",
      "Epoch 709/2000  Iteration 1418/4000 Training loss: 0.0236 0.4445 sec/batch\n",
      "Epoch 710/2000  Iteration 1419/4000 Training loss: 0.0418 0.4430 sec/batch\n",
      "Epoch 710/2000  Iteration 1420/4000 Training loss: 0.0238 0.4431 sec/batch\n",
      "Epoch 711/2000  Iteration 1421/4000 Training loss: 0.0409 0.4427 sec/batch\n",
      "Epoch 711/2000  Iteration 1422/4000 Training loss: 0.0230 0.4435 sec/batch\n",
      "Epoch 712/2000  Iteration 1423/4000 Training loss: 0.0420 0.4432 sec/batch\n",
      "Epoch 712/2000  Iteration 1424/4000 Training loss: 0.0240 0.4432 sec/batch\n",
      "Epoch 713/2000  Iteration 1425/4000 Training loss: 0.0407 0.4432 sec/batch\n",
      "Epoch 713/2000  Iteration 1426/4000 Training loss: 0.0233 0.4438 sec/batch\n",
      "Epoch 714/2000  Iteration 1427/4000 Training loss: 0.0405 0.4428 sec/batch\n",
      "Epoch 714/2000  Iteration 1428/4000 Training loss: 0.0231 0.4435 sec/batch\n",
      "Epoch 715/2000  Iteration 1429/4000 Training loss: 0.0416 0.4431 sec/batch\n",
      "Epoch 715/2000  Iteration 1430/4000 Training loss: 0.0241 0.4431 sec/batch\n",
      "Epoch 716/2000  Iteration 1431/4000 Training loss: 0.0405 0.4432 sec/batch\n",
      "Epoch 716/2000  Iteration 1432/4000 Training loss: 0.0237 0.4433 sec/batch\n",
      "Epoch 717/2000  Iteration 1433/4000 Training loss: 0.0416 0.4435 sec/batch\n",
      "Epoch 717/2000  Iteration 1434/4000 Training loss: 0.0237 0.4433 sec/batch\n",
      "Epoch 718/2000  Iteration 1435/4000 Training loss: 0.0402 0.4430 sec/batch\n",
      "Epoch 718/2000  Iteration 1436/4000 Training loss: 0.0231 0.4431 sec/batch\n",
      "Epoch 719/2000  Iteration 1437/4000 Training loss: 0.0401 0.4432 sec/batch\n",
      "Epoch 719/2000  Iteration 1438/4000 Training loss: 0.0232 0.4432 sec/batch\n",
      "Epoch 720/2000  Iteration 1439/4000 Training loss: 0.0409 0.4430 sec/batch\n",
      "Epoch 720/2000  Iteration 1440/4000 Training loss: 0.0235 0.4435 sec/batch\n",
      "Epoch 721/2000  Iteration 1441/4000 Training loss: 0.0408 0.4434 sec/batch\n",
      "Epoch 721/2000  Iteration 1442/4000 Training loss: 0.0240 0.4434 sec/batch\n",
      "Epoch 722/2000  Iteration 1443/4000 Training loss: 0.0401 0.4435 sec/batch\n",
      "Epoch 722/2000  Iteration 1444/4000 Training loss: 0.0232 0.4436 sec/batch\n",
      "Epoch 723/2000  Iteration 1445/4000 Training loss: 0.0407 0.4438 sec/batch\n",
      "Epoch 723/2000  Iteration 1446/4000 Training loss: 0.0232 0.4439 sec/batch\n",
      "Epoch 724/2000  Iteration 1447/4000 Training loss: 0.0409 0.4430 sec/batch\n",
      "Epoch 724/2000  Iteration 1448/4000 Training loss: 0.0234 0.4433 sec/batch\n",
      "Epoch 725/2000  Iteration 1449/4000 Training loss: 0.0394 0.4434 sec/batch\n",
      "Epoch 725/2000  Iteration 1450/4000 Training loss: 0.0230 0.4430 sec/batch\n",
      "Epoch 726/2000  Iteration 1451/4000 Training loss: 0.0405 0.4433 sec/batch\n",
      "Epoch 726/2000  Iteration 1452/4000 Training loss: 0.0232 0.4437 sec/batch\n",
      "Epoch 727/2000  Iteration 1453/4000 Training loss: 0.0390 0.4431 sec/batch\n",
      "Epoch 727/2000  Iteration 1454/4000 Training loss: 0.0224 0.4434 sec/batch\n",
      "Epoch 728/2000  Iteration 1455/4000 Training loss: 0.0403 0.4426 sec/batch\n",
      "Epoch 728/2000  Iteration 1456/4000 Training loss: 0.0227 0.4435 sec/batch\n",
      "Epoch 729/2000  Iteration 1457/4000 Training loss: 0.0389 0.4428 sec/batch\n",
      "Epoch 729/2000  Iteration 1458/4000 Training loss: 0.0224 0.4433 sec/batch\n",
      "Epoch 730/2000  Iteration 1459/4000 Training loss: 0.0400 0.4433 sec/batch\n",
      "Epoch 730/2000  Iteration 1460/4000 Training loss: 0.0232 0.4432 sec/batch\n",
      "Epoch 731/2000  Iteration 1461/4000 Training loss: 0.0395 0.4462 sec/batch\n",
      "Epoch 731/2000  Iteration 1462/4000 Training loss: 0.0228 0.4464 sec/batch\n",
      "Epoch 732/2000  Iteration 1463/4000 Training loss: 0.0401 0.4432 sec/batch\n",
      "Epoch 732/2000  Iteration 1464/4000 Training loss: 0.0227 0.4430 sec/batch\n",
      "Epoch 733/2000  Iteration 1465/4000 Training loss: 0.0391 0.4429 sec/batch\n",
      "Epoch 733/2000  Iteration 1466/4000 Training loss: 0.0228 0.4434 sec/batch\n",
      "Epoch 734/2000  Iteration 1467/4000 Training loss: 0.0403 0.4427 sec/batch\n",
      "Epoch 734/2000  Iteration 1468/4000 Training loss: 0.0234 0.4433 sec/batch\n",
      "Epoch 735/2000  Iteration 1469/4000 Training loss: 0.0393 0.4430 sec/batch\n",
      "Epoch 735/2000  Iteration 1470/4000 Training loss: 0.0229 0.4433 sec/batch\n",
      "Epoch 736/2000  Iteration 1471/4000 Training loss: 0.0398 0.4428 sec/batch\n",
      "Epoch 736/2000  Iteration 1472/4000 Training loss: 0.0231 0.4430 sec/batch\n",
      "Epoch 737/2000  Iteration 1473/4000 Training loss: 0.0389 0.4430 sec/batch\n",
      "Epoch 737/2000  Iteration 1474/4000 Training loss: 0.0223 0.4431 sec/batch\n",
      "Epoch 738/2000  Iteration 1475/4000 Training loss: 0.0394 0.4431 sec/batch\n",
      "Epoch 738/2000  Iteration 1476/4000 Training loss: 0.0228 0.4437 sec/batch\n",
      "Epoch 739/2000  Iteration 1477/4000 Training loss: 0.0392 0.4433 sec/batch\n",
      "Epoch 739/2000  Iteration 1478/4000 Training loss: 0.0226 0.4434 sec/batch\n",
      "Epoch 740/2000  Iteration 1479/4000 Training loss: 0.0393 0.4430 sec/batch\n",
      "Epoch 740/2000  Iteration 1480/4000 Training loss: 0.0224 0.4432 sec/batch\n",
      "Epoch 741/2000  Iteration 1481/4000 Training loss: 0.0379 0.4429 sec/batch\n",
      "Epoch 741/2000  Iteration 1482/4000 Training loss: 0.0219 0.4432 sec/batch\n",
      "Epoch 742/2000  Iteration 1483/4000 Training loss: 0.0397 0.4435 sec/batch\n",
      "Epoch 742/2000  Iteration 1484/4000 Training loss: 0.0234 0.4432 sec/batch\n",
      "Epoch 743/2000  Iteration 1485/4000 Training loss: 0.0402 0.4428 sec/batch\n",
      "Epoch 743/2000  Iteration 1486/4000 Training loss: 0.0233 0.4432 sec/batch\n",
      "Epoch 744/2000  Iteration 1487/4000 Training loss: 0.0388 0.4432 sec/batch\n",
      "Epoch 744/2000  Iteration 1488/4000 Training loss: 0.0227 0.4429 sec/batch\n",
      "Epoch 745/2000  Iteration 1489/4000 Training loss: 0.0393 0.4431 sec/batch\n",
      "Epoch 745/2000  Iteration 1490/4000 Training loss: 0.0231 0.4447 sec/batch\n",
      "Epoch 746/2000  Iteration 1491/4000 Training loss: 0.0399 0.4433 sec/batch\n",
      "Epoch 746/2000  Iteration 1492/4000 Training loss: 0.0231 0.4436 sec/batch\n",
      "Epoch 747/2000  Iteration 1493/4000 Training loss: 0.0391 0.4428 sec/batch\n",
      "Epoch 747/2000  Iteration 1494/4000 Training loss: 0.0228 0.4430 sec/batch\n",
      "Epoch 748/2000  Iteration 1495/4000 Training loss: 0.0386 0.4430 sec/batch\n",
      "Epoch 748/2000  Iteration 1496/4000 Training loss: 0.0227 0.4432 sec/batch\n",
      "Epoch 749/2000  Iteration 1497/4000 Training loss: 0.0397 0.4430 sec/batch\n",
      "Epoch 749/2000  Iteration 1498/4000 Training loss: 0.0234 0.4436 sec/batch\n",
      "Epoch 750/2000  Iteration 1499/4000 Training loss: 0.0400 0.4431 sec/batch\n",
      "Epoch 750/2000  Iteration 1500/4000 Training loss: 0.0233 0.4435 sec/batch\n",
      "Epoch 751/2000  Iteration 1501/4000 Training loss: 0.0400 0.4433 sec/batch\n",
      "Epoch 751/2000  Iteration 1502/4000 Training loss: 0.0234 0.4434 sec/batch\n",
      "Epoch 752/2000  Iteration 1503/4000 Training loss: 0.0387 0.4431 sec/batch\n",
      "Epoch 752/2000  Iteration 1504/4000 Training loss: 0.0230 0.4438 sec/batch\n",
      "Epoch 753/2000  Iteration 1505/4000 Training loss: 0.0397 0.4431 sec/batch\n",
      "Epoch 753/2000  Iteration 1506/4000 Training loss: 0.0237 0.4431 sec/batch\n",
      "Epoch 754/2000  Iteration 1507/4000 Training loss: 0.0397 0.4432 sec/batch\n",
      "Epoch 754/2000  Iteration 1508/4000 Training loss: 0.0238 0.4434 sec/batch\n",
      "Epoch 755/2000  Iteration 1509/4000 Training loss: 0.0395 0.4431 sec/batch\n",
      "Epoch 755/2000  Iteration 1510/4000 Training loss: 0.0228 0.4431 sec/batch\n",
      "Epoch 756/2000  Iteration 1511/4000 Training loss: 0.0399 0.4429 sec/batch\n",
      "Epoch 756/2000  Iteration 1512/4000 Training loss: 0.0238 0.4430 sec/batch\n",
      "Epoch 757/2000  Iteration 1513/4000 Training loss: 0.0388 0.4427 sec/batch\n",
      "Epoch 757/2000  Iteration 1514/4000 Training loss: 0.0226 0.4433 sec/batch\n",
      "Epoch 758/2000  Iteration 1515/4000 Training loss: 0.0399 0.4435 sec/batch\n",
      "Epoch 758/2000  Iteration 1516/4000 Training loss: 0.0236 0.4443 sec/batch\n",
      "Epoch 759/2000  Iteration 1517/4000 Training loss: 0.0386 0.4432 sec/batch\n",
      "Epoch 759/2000  Iteration 1518/4000 Training loss: 0.0235 0.4433 sec/batch\n",
      "Epoch 760/2000  Iteration 1519/4000 Training loss: 0.0380 0.4430 sec/batch\n",
      "Epoch 760/2000  Iteration 1520/4000 Training loss: 0.0219 0.4438 sec/batch\n",
      "Epoch 761/2000  Iteration 1521/4000 Training loss: 0.0393 0.4436 sec/batch\n",
      "Epoch 761/2000  Iteration 1522/4000 Training loss: 0.0228 0.4436 sec/batch\n",
      "Epoch 762/2000  Iteration 1523/4000 Training loss: 0.0390 0.4447 sec/batch\n",
      "Epoch 762/2000  Iteration 1524/4000 Training loss: 0.0229 0.4443 sec/batch\n",
      "Epoch 763/2000  Iteration 1525/4000 Training loss: 0.0398 0.4432 sec/batch\n",
      "Epoch 763/2000  Iteration 1526/4000 Training loss: 0.0234 0.4431 sec/batch\n",
      "Epoch 764/2000  Iteration 1527/4000 Training loss: 0.0374 0.4432 sec/batch\n",
      "Epoch 764/2000  Iteration 1528/4000 Training loss: 0.0221 0.4436 sec/batch\n",
      "Epoch 765/2000  Iteration 1529/4000 Training loss: 0.0392 0.4429 sec/batch\n",
      "Epoch 765/2000  Iteration 1530/4000 Training loss: 0.0228 0.4435 sec/batch\n",
      "Epoch 766/2000  Iteration 1531/4000 Training loss: 0.0402 0.4429 sec/batch\n",
      "Epoch 766/2000  Iteration 1532/4000 Training loss: 0.0233 0.4434 sec/batch\n",
      "Epoch 767/2000  Iteration 1533/4000 Training loss: 0.0386 0.4433 sec/batch\n",
      "Epoch 767/2000  Iteration 1534/4000 Training loss: 0.0225 0.4425 sec/batch\n",
      "Epoch 768/2000  Iteration 1535/4000 Training loss: 0.0382 0.4430 sec/batch\n",
      "Epoch 768/2000  Iteration 1536/4000 Training loss: 0.0226 0.4432 sec/batch\n",
      "Epoch 769/2000  Iteration 1537/4000 Training loss: 0.0384 0.4431 sec/batch\n",
      "Epoch 769/2000  Iteration 1538/4000 Training loss: 0.0223 0.4437 sec/batch\n",
      "Epoch 770/2000  Iteration 1539/4000 Training loss: 0.0386 0.4435 sec/batch\n",
      "Epoch 770/2000  Iteration 1540/4000 Training loss: 0.0230 0.4432 sec/batch\n",
      "Epoch 771/2000  Iteration 1541/4000 Training loss: 0.0387 0.4436 sec/batch\n",
      "Epoch 771/2000  Iteration 1542/4000 Training loss: 0.0226 0.4438 sec/batch\n",
      "Epoch 772/2000  Iteration 1543/4000 Training loss: 0.0371 0.4436 sec/batch\n",
      "Epoch 772/2000  Iteration 1544/4000 Training loss: 0.0224 0.4436 sec/batch\n",
      "Epoch 773/2000  Iteration 1545/4000 Training loss: 0.0387 0.4429 sec/batch\n",
      "Epoch 773/2000  Iteration 1546/4000 Training loss: 0.0227 0.4434 sec/batch\n",
      "Epoch 774/2000  Iteration 1547/4000 Training loss: 0.0380 0.4433 sec/batch\n",
      "Epoch 774/2000  Iteration 1548/4000 Training loss: 0.0220 0.4433 sec/batch\n",
      "Epoch 775/2000  Iteration 1549/4000 Training loss: 0.0377 0.4427 sec/batch\n",
      "Epoch 775/2000  Iteration 1550/4000 Training loss: 0.0222 0.4440 sec/batch\n",
      "Epoch 776/2000  Iteration 1551/4000 Training loss: 0.0377 0.4434 sec/batch\n",
      "Epoch 776/2000  Iteration 1552/4000 Training loss: 0.0222 0.4433 sec/batch\n",
      "Epoch 777/2000  Iteration 1553/4000 Training loss: 0.0379 0.4429 sec/batch\n",
      "Epoch 777/2000  Iteration 1554/4000 Training loss: 0.0224 0.4434 sec/batch\n",
      "Epoch 778/2000  Iteration 1555/4000 Training loss: 0.0380 0.4431 sec/batch\n",
      "Epoch 778/2000  Iteration 1556/4000 Training loss: 0.0224 0.4436 sec/batch\n",
      "Epoch 779/2000  Iteration 1557/4000 Training loss: 0.0375 0.4432 sec/batch\n",
      "Epoch 779/2000  Iteration 1558/4000 Training loss: 0.0223 0.4437 sec/batch\n",
      "Epoch 780/2000  Iteration 1559/4000 Training loss: 0.0389 0.4434 sec/batch\n",
      "Epoch 780/2000  Iteration 1560/4000 Training loss: 0.0226 0.4429 sec/batch\n",
      "Epoch 781/2000  Iteration 1561/4000 Training loss: 0.0379 0.4430 sec/batch\n",
      "Epoch 781/2000  Iteration 1562/4000 Training loss: 0.0221 0.4429 sec/batch\n",
      "Epoch 782/2000  Iteration 1563/4000 Training loss: 0.0373 0.4429 sec/batch\n",
      "Epoch 782/2000  Iteration 1564/4000 Training loss: 0.0218 0.4438 sec/batch\n",
      "Epoch 783/2000  Iteration 1565/4000 Training loss: 0.0370 0.4435 sec/batch\n",
      "Epoch 783/2000  Iteration 1566/4000 Training loss: 0.0216 0.4434 sec/batch\n",
      "Epoch 784/2000  Iteration 1567/4000 Training loss: 0.0380 0.4435 sec/batch\n",
      "Epoch 784/2000  Iteration 1568/4000 Training loss: 0.0222 0.4437 sec/batch\n",
      "Epoch 785/2000  Iteration 1569/4000 Training loss: 0.0370 0.4430 sec/batch\n",
      "Epoch 785/2000  Iteration 1570/4000 Training loss: 0.0219 0.4432 sec/batch\n",
      "Epoch 786/2000  Iteration 1571/4000 Training loss: 0.0372 0.4432 sec/batch\n",
      "Epoch 786/2000  Iteration 1572/4000 Training loss: 0.0221 0.4431 sec/batch\n",
      "Epoch 787/2000  Iteration 1573/4000 Training loss: 0.0365 0.4432 sec/batch\n",
      "Epoch 787/2000  Iteration 1574/4000 Training loss: 0.0209 0.4433 sec/batch\n",
      "Epoch 788/2000  Iteration 1575/4000 Training loss: 0.0378 0.4435 sec/batch\n",
      "Epoch 788/2000  Iteration 1576/4000 Training loss: 0.0217 0.4433 sec/batch\n",
      "Epoch 789/2000  Iteration 1577/4000 Training loss: 0.0370 0.4437 sec/batch\n",
      "Epoch 789/2000  Iteration 1578/4000 Training loss: 0.0212 0.4434 sec/batch\n",
      "Epoch 790/2000  Iteration 1579/4000 Training loss: 0.0363 0.4434 sec/batch\n",
      "Epoch 790/2000  Iteration 1580/4000 Training loss: 0.0212 0.4436 sec/batch\n",
      "Epoch 791/2000  Iteration 1581/4000 Training loss: 0.0373 0.4432 sec/batch\n",
      "Epoch 791/2000  Iteration 1582/4000 Training loss: 0.0212 0.4437 sec/batch\n",
      "Epoch 792/2000  Iteration 1583/4000 Training loss: 0.0358 0.4435 sec/batch\n",
      "Epoch 792/2000  Iteration 1584/4000 Training loss: 0.0208 0.4432 sec/batch\n",
      "Epoch 793/2000  Iteration 1585/4000 Training loss: 0.0370 0.4433 sec/batch\n",
      "Epoch 793/2000  Iteration 1586/4000 Training loss: 0.0215 0.4437 sec/batch\n",
      "Epoch 794/2000  Iteration 1587/4000 Training loss: 0.0381 0.4437 sec/batch\n",
      "Epoch 794/2000  Iteration 1588/4000 Training loss: 0.0221 0.4435 sec/batch\n",
      "Epoch 795/2000  Iteration 1589/4000 Training loss: 0.0382 0.4435 sec/batch\n",
      "Epoch 795/2000  Iteration 1590/4000 Training loss: 0.0218 0.4438 sec/batch\n",
      "Epoch 796/2000  Iteration 1591/4000 Training loss: 0.0365 0.4433 sec/batch\n",
      "Epoch 796/2000  Iteration 1592/4000 Training loss: 0.0210 0.4431 sec/batch\n",
      "Epoch 797/2000  Iteration 1593/4000 Training loss: 0.0363 0.4430 sec/batch\n",
      "Epoch 797/2000  Iteration 1594/4000 Training loss: 0.0211 0.4433 sec/batch\n",
      "Epoch 798/2000  Iteration 1595/4000 Training loss: 0.0365 0.4429 sec/batch\n",
      "Epoch 798/2000  Iteration 1596/4000 Training loss: 0.0214 0.4434 sec/batch\n",
      "Epoch 799/2000  Iteration 1597/4000 Training loss: 0.0356 0.4464 sec/batch\n",
      "Epoch 799/2000  Iteration 1598/4000 Training loss: 0.0207 0.4435 sec/batch\n",
      "Epoch 800/2000  Iteration 1599/4000 Training loss: 0.0353 0.4432 sec/batch\n",
      "Epoch 800/2000  Iteration 1600/4000 Training loss: 0.0211 0.4431 sec/batch\n",
      "Validation loss: 6.01822 Saving checkpoint!\n",
      "Epoch 801/2000  Iteration 1601/4000 Training loss: 0.0373 0.4431 sec/batch\n",
      "Epoch 801/2000  Iteration 1602/4000 Training loss: 0.0214 0.4433 sec/batch\n",
      "Epoch 802/2000  Iteration 1603/4000 Training loss: 0.0372 0.4430 sec/batch\n",
      "Epoch 802/2000  Iteration 1604/4000 Training loss: 0.0211 0.4438 sec/batch\n",
      "Epoch 803/2000  Iteration 1605/4000 Training loss: 0.0373 0.4431 sec/batch\n",
      "Epoch 803/2000  Iteration 1606/4000 Training loss: 0.0217 0.4435 sec/batch\n",
      "Epoch 804/2000  Iteration 1607/4000 Training loss: 0.0364 0.4435 sec/batch\n",
      "Epoch 804/2000  Iteration 1608/4000 Training loss: 0.0211 0.4432 sec/batch\n",
      "Epoch 805/2000  Iteration 1609/4000 Training loss: 0.0361 0.4430 sec/batch\n",
      "Epoch 805/2000  Iteration 1610/4000 Training loss: 0.0206 0.4434 sec/batch\n",
      "Epoch 806/2000  Iteration 1611/4000 Training loss: 0.0364 0.4434 sec/batch\n",
      "Epoch 806/2000  Iteration 1612/4000 Training loss: 0.0211 0.4434 sec/batch\n",
      "Epoch 807/2000  Iteration 1613/4000 Training loss: 0.0355 0.4435 sec/batch\n",
      "Epoch 807/2000  Iteration 1614/4000 Training loss: 0.0204 0.4436 sec/batch\n",
      "Epoch 808/2000  Iteration 1615/4000 Training loss: 0.0361 0.4432 sec/batch\n",
      "Epoch 808/2000  Iteration 1616/4000 Training loss: 0.0213 0.4428 sec/batch\n",
      "Epoch 809/2000  Iteration 1617/4000 Training loss: 0.0352 0.4433 sec/batch\n",
      "Epoch 809/2000  Iteration 1618/4000 Training loss: 0.0204 0.4432 sec/batch\n",
      "Epoch 810/2000  Iteration 1619/4000 Training loss: 0.0356 0.4431 sec/batch\n",
      "Epoch 810/2000  Iteration 1620/4000 Training loss: 0.0206 0.4429 sec/batch\n",
      "Epoch 811/2000  Iteration 1621/4000 Training loss: 0.0354 0.4431 sec/batch\n",
      "Epoch 811/2000  Iteration 1622/4000 Training loss: 0.0204 0.4437 sec/batch\n",
      "Epoch 812/2000  Iteration 1623/4000 Training loss: 0.0369 0.4431 sec/batch\n",
      "Epoch 812/2000  Iteration 1624/4000 Training loss: 0.0211 0.4436 sec/batch\n",
      "Epoch 813/2000  Iteration 1625/4000 Training loss: 0.0372 0.4428 sec/batch\n",
      "Epoch 813/2000  Iteration 1626/4000 Training loss: 0.0213 0.4434 sec/batch\n",
      "Epoch 814/2000  Iteration 1627/4000 Training loss: 0.0352 0.4433 sec/batch\n",
      "Epoch 814/2000  Iteration 1628/4000 Training loss: 0.0203 0.4438 sec/batch\n",
      "Epoch 815/2000  Iteration 1629/4000 Training loss: 0.0388 0.4430 sec/batch\n",
      "Epoch 815/2000  Iteration 1630/4000 Training loss: 0.0219 0.4434 sec/batch\n",
      "Epoch 816/2000  Iteration 1631/4000 Training loss: 0.0350 0.4437 sec/batch\n",
      "Epoch 816/2000  Iteration 1632/4000 Training loss: 0.0207 0.4439 sec/batch\n",
      "Epoch 817/2000  Iteration 1633/4000 Training loss: 0.0370 0.4434 sec/batch\n",
      "Epoch 817/2000  Iteration 1634/4000 Training loss: 0.0213 0.4440 sec/batch\n",
      "Epoch 818/2000  Iteration 1635/4000 Training loss: 0.0359 0.4431 sec/batch\n",
      "Epoch 818/2000  Iteration 1636/4000 Training loss: 0.0210 0.4441 sec/batch\n",
      "Epoch 819/2000  Iteration 1637/4000 Training loss: 0.0359 0.4440 sec/batch\n",
      "Epoch 819/2000  Iteration 1638/4000 Training loss: 0.0215 0.4437 sec/batch\n",
      "Epoch 820/2000  Iteration 1639/4000 Training loss: 0.0366 0.4437 sec/batch\n",
      "Epoch 820/2000  Iteration 1640/4000 Training loss: 0.0215 0.4434 sec/batch\n",
      "Epoch 821/2000  Iteration 1641/4000 Training loss: 0.0357 0.4434 sec/batch\n",
      "Epoch 821/2000  Iteration 1642/4000 Training loss: 0.0212 0.4436 sec/batch\n",
      "Epoch 822/2000  Iteration 1643/4000 Training loss: 0.0365 0.4432 sec/batch\n",
      "Epoch 822/2000  Iteration 1644/4000 Training loss: 0.0211 0.4435 sec/batch\n",
      "Epoch 823/2000  Iteration 1645/4000 Training loss: 0.0354 0.4430 sec/batch\n",
      "Epoch 823/2000  Iteration 1646/4000 Training loss: 0.0205 0.4433 sec/batch\n",
      "Epoch 824/2000  Iteration 1647/4000 Training loss: 0.0345 0.4433 sec/batch\n",
      "Epoch 824/2000  Iteration 1648/4000 Training loss: 0.0200 0.4437 sec/batch\n",
      "Epoch 825/2000  Iteration 1649/4000 Training loss: 0.0366 0.4436 sec/batch\n",
      "Epoch 825/2000  Iteration 1650/4000 Training loss: 0.0213 0.4441 sec/batch\n",
      "Epoch 826/2000  Iteration 1651/4000 Training loss: 0.0366 0.4434 sec/batch\n",
      "Epoch 826/2000  Iteration 1652/4000 Training loss: 0.0207 0.4442 sec/batch\n",
      "Epoch 827/2000  Iteration 1653/4000 Training loss: 0.0352 0.4433 sec/batch\n",
      "Epoch 827/2000  Iteration 1654/4000 Training loss: 0.0206 0.4442 sec/batch\n",
      "Epoch 828/2000  Iteration 1655/4000 Training loss: 0.0354 0.4437 sec/batch\n",
      "Epoch 828/2000  Iteration 1656/4000 Training loss: 0.0203 0.4434 sec/batch\n",
      "Epoch 829/2000  Iteration 1657/4000 Training loss: 0.0349 0.4437 sec/batch\n",
      "Epoch 829/2000  Iteration 1658/4000 Training loss: 0.0200 0.4436 sec/batch\n",
      "Epoch 830/2000  Iteration 1659/4000 Training loss: 0.0345 0.4438 sec/batch\n",
      "Epoch 830/2000  Iteration 1660/4000 Training loss: 0.0200 0.4440 sec/batch\n",
      "Epoch 831/2000  Iteration 1661/4000 Training loss: 0.0346 0.4431 sec/batch\n",
      "Epoch 831/2000  Iteration 1662/4000 Training loss: 0.0199 0.4440 sec/batch\n",
      "Epoch 832/2000  Iteration 1663/4000 Training loss: 0.0349 0.4435 sec/batch\n",
      "Epoch 832/2000  Iteration 1664/4000 Training loss: 0.0200 0.4442 sec/batch\n",
      "Epoch 833/2000  Iteration 1665/4000 Training loss: 0.0354 0.4434 sec/batch\n",
      "Epoch 833/2000  Iteration 1666/4000 Training loss: 0.0203 0.4437 sec/batch\n",
      "Epoch 834/2000  Iteration 1667/4000 Training loss: 0.0352 0.4434 sec/batch\n",
      "Epoch 834/2000  Iteration 1668/4000 Training loss: 0.0205 0.4443 sec/batch\n",
      "Epoch 835/2000  Iteration 1669/4000 Training loss: 0.0333 0.4443 sec/batch\n",
      "Epoch 835/2000  Iteration 1670/4000 Training loss: 0.0188 0.4433 sec/batch\n",
      "Epoch 836/2000  Iteration 1671/4000 Training loss: 0.0359 0.4434 sec/batch\n",
      "Epoch 836/2000  Iteration 1672/4000 Training loss: 0.0201 0.4439 sec/batch\n",
      "Epoch 837/2000  Iteration 1673/4000 Training loss: 0.0362 0.4429 sec/batch\n",
      "Epoch 837/2000  Iteration 1674/4000 Training loss: 0.0206 0.4434 sec/batch\n",
      "Epoch 838/2000  Iteration 1675/4000 Training loss: 0.0350 0.4435 sec/batch\n",
      "Epoch 838/2000  Iteration 1676/4000 Training loss: 0.0202 0.4437 sec/batch\n",
      "Epoch 839/2000  Iteration 1677/4000 Training loss: 0.0347 0.4431 sec/batch\n",
      "Epoch 839/2000  Iteration 1678/4000 Training loss: 0.0197 0.4434 sec/batch\n",
      "Epoch 840/2000  Iteration 1679/4000 Training loss: 0.0350 0.4428 sec/batch\n",
      "Epoch 840/2000  Iteration 1680/4000 Training loss: 0.0201 0.4435 sec/batch\n",
      "Epoch 841/2000  Iteration 1681/4000 Training loss: 0.0350 0.4429 sec/batch\n",
      "Epoch 841/2000  Iteration 1682/4000 Training loss: 0.0204 0.4434 sec/batch\n",
      "Epoch 842/2000  Iteration 1683/4000 Training loss: 0.0348 0.4432 sec/batch\n",
      "Epoch 842/2000  Iteration 1684/4000 Training loss: 0.0201 0.4435 sec/batch\n",
      "Epoch 843/2000  Iteration 1685/4000 Training loss: 0.0355 0.4429 sec/batch\n",
      "Epoch 843/2000  Iteration 1686/4000 Training loss: 0.0209 0.4431 sec/batch\n",
      "Epoch 844/2000  Iteration 1687/4000 Training loss: 0.0348 0.4431 sec/batch\n",
      "Epoch 844/2000  Iteration 1688/4000 Training loss: 0.0203 0.4432 sec/batch\n",
      "Epoch 845/2000  Iteration 1689/4000 Training loss: 0.0343 0.4431 sec/batch\n",
      "Epoch 845/2000  Iteration 1690/4000 Training loss: 0.0200 0.4434 sec/batch\n",
      "Epoch 846/2000  Iteration 1691/4000 Training loss: 0.0338 0.4431 sec/batch\n",
      "Epoch 846/2000  Iteration 1692/4000 Training loss: 0.0199 0.4430 sec/batch\n",
      "Epoch 847/2000  Iteration 1693/4000 Training loss: 0.0337 0.4429 sec/batch\n",
      "Epoch 847/2000  Iteration 1694/4000 Training loss: 0.0194 0.4436 sec/batch\n",
      "Epoch 848/2000  Iteration 1695/4000 Training loss: 0.0346 0.4433 sec/batch\n",
      "Epoch 848/2000  Iteration 1696/4000 Training loss: 0.0199 0.4444 sec/batch\n",
      "Epoch 849/2000  Iteration 1697/4000 Training loss: 0.0351 0.4438 sec/batch\n",
      "Epoch 849/2000  Iteration 1698/4000 Training loss: 0.0203 0.4436 sec/batch\n",
      "Epoch 850/2000  Iteration 1699/4000 Training loss: 0.0342 0.4435 sec/batch\n",
      "Epoch 850/2000  Iteration 1700/4000 Training loss: 0.0197 0.4437 sec/batch\n",
      "Epoch 851/2000  Iteration 1701/4000 Training loss: 0.0351 0.4435 sec/batch\n",
      "Epoch 851/2000  Iteration 1702/4000 Training loss: 0.0204 0.4439 sec/batch\n",
      "Epoch 852/2000  Iteration 1703/4000 Training loss: 0.0345 0.4433 sec/batch\n",
      "Epoch 852/2000  Iteration 1704/4000 Training loss: 0.0204 0.4437 sec/batch\n",
      "Epoch 853/2000  Iteration 1705/4000 Training loss: 0.0349 0.4435 sec/batch\n",
      "Epoch 853/2000  Iteration 1706/4000 Training loss: 0.0202 0.4436 sec/batch\n",
      "Epoch 854/2000  Iteration 1707/4000 Training loss: 0.0349 0.4433 sec/batch\n",
      "Epoch 854/2000  Iteration 1708/4000 Training loss: 0.0198 0.4438 sec/batch\n",
      "Epoch 855/2000  Iteration 1709/4000 Training loss: 0.0335 0.4433 sec/batch\n",
      "Epoch 855/2000  Iteration 1710/4000 Training loss: 0.0192 0.4441 sec/batch\n",
      "Epoch 856/2000  Iteration 1711/4000 Training loss: 0.0348 0.4433 sec/batch\n",
      "Epoch 856/2000  Iteration 1712/4000 Training loss: 0.0200 0.4436 sec/batch\n",
      "Epoch 857/2000  Iteration 1713/4000 Training loss: 0.0356 0.4432 sec/batch\n",
      "Epoch 857/2000  Iteration 1714/4000 Training loss: 0.0201 0.4435 sec/batch\n",
      "Epoch 858/2000  Iteration 1715/4000 Training loss: 0.0360 0.4433 sec/batch\n",
      "Epoch 858/2000  Iteration 1716/4000 Training loss: 0.0206 0.4434 sec/batch\n",
      "Epoch 859/2000  Iteration 1717/4000 Training loss: 0.0338 0.4437 sec/batch\n",
      "Epoch 859/2000  Iteration 1718/4000 Training loss: 0.0196 0.4434 sec/batch\n",
      "Epoch 860/2000  Iteration 1719/4000 Training loss: 0.0341 0.4430 sec/batch\n",
      "Epoch 860/2000  Iteration 1720/4000 Training loss: 0.0198 0.4437 sec/batch\n",
      "Epoch 861/2000  Iteration 1721/4000 Training loss: 0.0348 0.4434 sec/batch\n",
      "Epoch 861/2000  Iteration 1722/4000 Training loss: 0.0201 0.4439 sec/batch\n",
      "Epoch 862/2000  Iteration 1723/4000 Training loss: 0.0344 0.4443 sec/batch\n",
      "Epoch 862/2000  Iteration 1724/4000 Training loss: 0.0200 0.4434 sec/batch\n",
      "Epoch 863/2000  Iteration 1725/4000 Training loss: 0.0333 0.4436 sec/batch\n",
      "Epoch 863/2000  Iteration 1726/4000 Training loss: 0.0197 0.4435 sec/batch\n",
      "Epoch 864/2000  Iteration 1727/4000 Training loss: 0.0357 0.4442 sec/batch\n",
      "Epoch 864/2000  Iteration 1728/4000 Training loss: 0.0206 0.4431 sec/batch\n",
      "Epoch 865/2000  Iteration 1729/4000 Training loss: 0.0342 0.4434 sec/batch\n",
      "Epoch 865/2000  Iteration 1730/4000 Training loss: 0.0197 0.4440 sec/batch\n",
      "Epoch 866/2000  Iteration 1731/4000 Training loss: 0.0344 0.4433 sec/batch\n",
      "Epoch 866/2000  Iteration 1732/4000 Training loss: 0.0196 0.4435 sec/batch\n",
      "Epoch 867/2000  Iteration 1733/4000 Training loss: 0.0344 0.4436 sec/batch\n",
      "Epoch 867/2000  Iteration 1734/4000 Training loss: 0.0201 0.4441 sec/batch\n",
      "Epoch 868/2000  Iteration 1735/4000 Training loss: 0.0355 0.4437 sec/batch\n",
      "Epoch 868/2000  Iteration 1736/4000 Training loss: 0.0207 0.4444 sec/batch\n",
      "Epoch 869/2000  Iteration 1737/4000 Training loss: 0.0332 0.4436 sec/batch\n",
      "Epoch 869/2000  Iteration 1738/4000 Training loss: 0.0190 0.4449 sec/batch\n",
      "Epoch 870/2000  Iteration 1739/4000 Training loss: 0.0337 0.4439 sec/batch\n",
      "Epoch 870/2000  Iteration 1740/4000 Training loss: 0.0200 0.4439 sec/batch\n",
      "Epoch 871/2000  Iteration 1741/4000 Training loss: 0.0335 0.4445 sec/batch\n",
      "Epoch 871/2000  Iteration 1742/4000 Training loss: 0.0190 0.4436 sec/batch\n",
      "Epoch 872/2000  Iteration 1743/4000 Training loss: 0.0345 0.4430 sec/batch\n",
      "Epoch 872/2000  Iteration 1744/4000 Training loss: 0.0202 0.4441 sec/batch\n",
      "Epoch 873/2000  Iteration 1745/4000 Training loss: 0.0337 0.4433 sec/batch\n",
      "Epoch 873/2000  Iteration 1746/4000 Training loss: 0.0197 0.4435 sec/batch\n",
      "Epoch 874/2000  Iteration 1747/4000 Training loss: 0.0330 0.4431 sec/batch\n",
      "Epoch 874/2000  Iteration 1748/4000 Training loss: 0.0190 0.4437 sec/batch\n",
      "Epoch 875/2000  Iteration 1749/4000 Training loss: 0.0339 0.4440 sec/batch\n",
      "Epoch 875/2000  Iteration 1750/4000 Training loss: 0.0193 0.4439 sec/batch\n",
      "Epoch 876/2000  Iteration 1751/4000 Training loss: 0.0331 0.4436 sec/batch\n",
      "Epoch 876/2000  Iteration 1752/4000 Training loss: 0.0191 0.4439 sec/batch\n",
      "Epoch 877/2000  Iteration 1753/4000 Training loss: 0.0330 0.4435 sec/batch\n",
      "Epoch 877/2000  Iteration 1754/4000 Training loss: 0.0190 0.4437 sec/batch\n",
      "Epoch 878/2000  Iteration 1755/4000 Training loss: 0.0339 0.4437 sec/batch\n",
      "Epoch 878/2000  Iteration 1756/4000 Training loss: 0.0194 0.4436 sec/batch\n",
      "Epoch 879/2000  Iteration 1757/4000 Training loss: 0.0331 0.4431 sec/batch\n",
      "Epoch 879/2000  Iteration 1758/4000 Training loss: 0.0193 0.4435 sec/batch\n",
      "Epoch 880/2000  Iteration 1759/4000 Training loss: 0.0339 0.4434 sec/batch\n",
      "Epoch 880/2000  Iteration 1760/4000 Training loss: 0.0194 0.4439 sec/batch\n",
      "Epoch 881/2000  Iteration 1761/4000 Training loss: 0.0330 0.4428 sec/batch\n",
      "Epoch 881/2000  Iteration 1762/4000 Training loss: 0.0190 0.4438 sec/batch\n",
      "Epoch 882/2000  Iteration 1763/4000 Training loss: 0.0347 0.4433 sec/batch\n",
      "Epoch 882/2000  Iteration 1764/4000 Training loss: 0.0198 0.4435 sec/batch\n",
      "Epoch 883/2000  Iteration 1765/4000 Training loss: 0.0334 0.4432 sec/batch\n",
      "Epoch 883/2000  Iteration 1766/4000 Training loss: 0.0191 0.4429 sec/batch\n",
      "Epoch 884/2000  Iteration 1767/4000 Training loss: 0.0342 0.4439 sec/batch\n",
      "Epoch 884/2000  Iteration 1768/4000 Training loss: 0.0201 0.4435 sec/batch\n",
      "Epoch 885/2000  Iteration 1769/4000 Training loss: 0.0334 0.4433 sec/batch\n",
      "Epoch 885/2000  Iteration 1770/4000 Training loss: 0.0193 0.4437 sec/batch\n",
      "Epoch 886/2000  Iteration 1771/4000 Training loss: 0.0340 0.4430 sec/batch\n",
      "Epoch 886/2000  Iteration 1772/4000 Training loss: 0.0195 0.4431 sec/batch\n",
      "Epoch 887/2000  Iteration 1773/4000 Training loss: 0.0333 0.4428 sec/batch\n",
      "Epoch 887/2000  Iteration 1774/4000 Training loss: 0.0193 0.4434 sec/batch\n",
      "Epoch 888/2000  Iteration 1775/4000 Training loss: 0.0335 0.4433 sec/batch\n",
      "Epoch 888/2000  Iteration 1776/4000 Training loss: 0.0197 0.4435 sec/batch\n",
      "Epoch 889/2000  Iteration 1777/4000 Training loss: 0.0320 0.4440 sec/batch\n",
      "Epoch 889/2000  Iteration 1778/4000 Training loss: 0.0184 0.4436 sec/batch\n",
      "Epoch 890/2000  Iteration 1779/4000 Training loss: 0.0329 0.4432 sec/batch\n",
      "Epoch 890/2000  Iteration 1780/4000 Training loss: 0.0191 0.4440 sec/batch\n",
      "Epoch 891/2000  Iteration 1781/4000 Training loss: 0.0319 0.4433 sec/batch\n",
      "Epoch 891/2000  Iteration 1782/4000 Training loss: 0.0184 0.4434 sec/batch\n",
      "Epoch 892/2000  Iteration 1783/4000 Training loss: 0.0325 0.4431 sec/batch\n",
      "Epoch 892/2000  Iteration 1784/4000 Training loss: 0.0187 0.4435 sec/batch\n",
      "Epoch 893/2000  Iteration 1785/4000 Training loss: 0.0328 0.4438 sec/batch\n",
      "Epoch 893/2000  Iteration 1786/4000 Training loss: 0.0187 0.4439 sec/batch\n",
      "Epoch 894/2000  Iteration 1787/4000 Training loss: 0.0336 0.4437 sec/batch\n",
      "Epoch 894/2000  Iteration 1788/4000 Training loss: 0.0198 0.4439 sec/batch\n",
      "Epoch 895/2000  Iteration 1789/4000 Training loss: 0.0332 0.4436 sec/batch\n",
      "Epoch 895/2000  Iteration 1790/4000 Training loss: 0.0186 0.4436 sec/batch\n",
      "Epoch 896/2000  Iteration 1791/4000 Training loss: 0.0337 0.4434 sec/batch\n",
      "Epoch 896/2000  Iteration 1792/4000 Training loss: 0.0193 0.4435 sec/batch\n",
      "Epoch 897/2000  Iteration 1793/4000 Training loss: 0.0325 0.4433 sec/batch\n",
      "Epoch 897/2000  Iteration 1794/4000 Training loss: 0.0195 0.4436 sec/batch\n",
      "Epoch 898/2000  Iteration 1795/4000 Training loss: 0.0313 0.4433 sec/batch\n",
      "Epoch 898/2000  Iteration 1796/4000 Training loss: 0.0178 0.4435 sec/batch\n",
      "Epoch 899/2000  Iteration 1797/4000 Training loss: 0.0312 0.4431 sec/batch\n",
      "Epoch 899/2000  Iteration 1798/4000 Training loss: 0.0182 0.4440 sec/batch\n",
      "Epoch 900/2000  Iteration 1799/4000 Training loss: 0.0337 0.4435 sec/batch\n",
      "Epoch 900/2000  Iteration 1800/4000 Training loss: 0.0198 0.4440 sec/batch\n",
      "Validation loss: 6.1257 Saving checkpoint!\n",
      "Epoch 901/2000  Iteration 1801/4000 Training loss: 0.0312 0.4437 sec/batch\n",
      "Epoch 901/2000  Iteration 1802/4000 Training loss: 0.0183 0.4441 sec/batch\n",
      "Epoch 902/2000  Iteration 1803/4000 Training loss: 0.0330 0.4438 sec/batch\n",
      "Epoch 902/2000  Iteration 1804/4000 Training loss: 0.0190 0.4434 sec/batch\n",
      "Epoch 903/2000  Iteration 1805/4000 Training loss: 0.0336 0.4432 sec/batch\n",
      "Epoch 903/2000  Iteration 1806/4000 Training loss: 0.0192 0.4437 sec/batch\n",
      "Epoch 904/2000  Iteration 1807/4000 Training loss: 0.0330 0.4431 sec/batch\n",
      "Epoch 904/2000  Iteration 1808/4000 Training loss: 0.0193 0.4440 sec/batch\n",
      "Epoch 905/2000  Iteration 1809/4000 Training loss: 0.0328 0.4432 sec/batch\n",
      "Epoch 905/2000  Iteration 1810/4000 Training loss: 0.0194 0.4433 sec/batch\n",
      "Epoch 906/2000  Iteration 1811/4000 Training loss: 0.0336 0.4435 sec/batch\n",
      "Epoch 906/2000  Iteration 1812/4000 Training loss: 0.0194 0.4434 sec/batch\n",
      "Epoch 907/2000  Iteration 1813/4000 Training loss: 0.0328 0.4434 sec/batch\n",
      "Epoch 907/2000  Iteration 1814/4000 Training loss: 0.0189 0.4433 sec/batch\n",
      "Epoch 908/2000  Iteration 1815/4000 Training loss: 0.0326 0.4433 sec/batch\n",
      "Epoch 908/2000  Iteration 1816/4000 Training loss: 0.0185 0.4438 sec/batch\n",
      "Epoch 909/2000  Iteration 1817/4000 Training loss: 0.0322 0.4433 sec/batch\n",
      "Epoch 909/2000  Iteration 1818/4000 Training loss: 0.0185 0.4435 sec/batch\n",
      "Epoch 910/2000  Iteration 1819/4000 Training loss: 0.0320 0.4436 sec/batch\n",
      "Epoch 910/2000  Iteration 1820/4000 Training loss: 0.0186 0.4436 sec/batch\n",
      "Epoch 911/2000  Iteration 1821/4000 Training loss: 0.0326 0.4435 sec/batch\n",
      "Epoch 911/2000  Iteration 1822/4000 Training loss: 0.0188 0.4434 sec/batch\n",
      "Epoch 912/2000  Iteration 1823/4000 Training loss: 0.0331 0.4428 sec/batch\n",
      "Epoch 912/2000  Iteration 1824/4000 Training loss: 0.0189 0.4433 sec/batch\n",
      "Epoch 913/2000  Iteration 1825/4000 Training loss: 0.0327 0.4432 sec/batch\n",
      "Epoch 913/2000  Iteration 1826/4000 Training loss: 0.0188 0.4429 sec/batch\n",
      "Epoch 914/2000  Iteration 1827/4000 Training loss: 0.0322 0.4434 sec/batch\n",
      "Epoch 914/2000  Iteration 1828/4000 Training loss: 0.0186 0.4433 sec/batch\n",
      "Epoch 915/2000  Iteration 1829/4000 Training loss: 0.0326 0.4432 sec/batch\n",
      "Epoch 915/2000  Iteration 1830/4000 Training loss: 0.0189 0.4435 sec/batch\n",
      "Epoch 916/2000  Iteration 1831/4000 Training loss: 0.0333 0.4433 sec/batch\n",
      "Epoch 916/2000  Iteration 1832/4000 Training loss: 0.0196 0.4434 sec/batch\n",
      "Epoch 917/2000  Iteration 1833/4000 Training loss: 0.0324 0.4429 sec/batch\n",
      "Epoch 917/2000  Iteration 1834/4000 Training loss: 0.0184 0.4436 sec/batch\n",
      "Epoch 918/2000  Iteration 1835/4000 Training loss: 0.0327 0.4428 sec/batch\n",
      "Epoch 918/2000  Iteration 1836/4000 Training loss: 0.0193 0.4436 sec/batch\n",
      "Epoch 919/2000  Iteration 1837/4000 Training loss: 0.0315 0.4434 sec/batch\n",
      "Epoch 919/2000  Iteration 1838/4000 Training loss: 0.0184 0.4435 sec/batch\n",
      "Epoch 920/2000  Iteration 1839/4000 Training loss: 0.0315 0.4433 sec/batch\n",
      "Epoch 920/2000  Iteration 1840/4000 Training loss: 0.0181 0.4432 sec/batch\n",
      "Epoch 921/2000  Iteration 1841/4000 Training loss: 0.0327 0.4426 sec/batch\n",
      "Epoch 921/2000  Iteration 1842/4000 Training loss: 0.0187 0.4437 sec/batch\n",
      "Epoch 922/2000  Iteration 1843/4000 Training loss: 0.0328 0.4442 sec/batch\n",
      "Epoch 922/2000  Iteration 1844/4000 Training loss: 0.0189 0.4433 sec/batch\n",
      "Epoch 923/2000  Iteration 1845/4000 Training loss: 0.0320 0.4431 sec/batch\n",
      "Epoch 923/2000  Iteration 1846/4000 Training loss: 0.0183 0.4435 sec/batch\n",
      "Epoch 924/2000  Iteration 1847/4000 Training loss: 0.0328 0.4429 sec/batch\n",
      "Epoch 924/2000  Iteration 1848/4000 Training loss: 0.0187 0.4434 sec/batch\n",
      "Epoch 925/2000  Iteration 1849/4000 Training loss: 0.0314 0.4433 sec/batch\n",
      "Epoch 925/2000  Iteration 1850/4000 Training loss: 0.0178 0.4439 sec/batch\n",
      "Epoch 926/2000  Iteration 1851/4000 Training loss: 0.0318 0.4431 sec/batch\n",
      "Epoch 926/2000  Iteration 1852/4000 Training loss: 0.0178 0.4435 sec/batch\n",
      "Epoch 927/2000  Iteration 1853/4000 Training loss: 0.0309 0.4434 sec/batch\n",
      "Epoch 927/2000  Iteration 1854/4000 Training loss: 0.0175 0.4435 sec/batch\n",
      "Epoch 928/2000  Iteration 1855/4000 Training loss: 0.0312 0.4435 sec/batch\n",
      "Epoch 928/2000  Iteration 1856/4000 Training loss: 0.0178 0.4437 sec/batch\n",
      "Epoch 929/2000  Iteration 1857/4000 Training loss: 0.0303 0.4432 sec/batch\n",
      "Epoch 929/2000  Iteration 1858/4000 Training loss: 0.0175 0.4435 sec/batch\n",
      "Epoch 930/2000  Iteration 1859/4000 Training loss: 0.0308 0.4436 sec/batch\n",
      "Epoch 930/2000  Iteration 1860/4000 Training loss: 0.0179 0.4437 sec/batch\n",
      "Epoch 931/2000  Iteration 1861/4000 Training loss: 0.0322 0.4433 sec/batch\n",
      "Epoch 931/2000  Iteration 1862/4000 Training loss: 0.0185 0.4436 sec/batch\n",
      "Epoch 932/2000  Iteration 1863/4000 Training loss: 0.0305 0.4435 sec/batch\n",
      "Epoch 932/2000  Iteration 1864/4000 Training loss: 0.0173 0.4444 sec/batch\n",
      "Epoch 933/2000  Iteration 1865/4000 Training loss: 0.0321 0.4439 sec/batch\n",
      "Epoch 933/2000  Iteration 1866/4000 Training loss: 0.0179 0.4442 sec/batch\n",
      "Epoch 934/2000  Iteration 1867/4000 Training loss: 0.0311 0.4431 sec/batch\n",
      "Epoch 934/2000  Iteration 1868/4000 Training loss: 0.0177 0.4436 sec/batch\n",
      "Epoch 935/2000  Iteration 1869/4000 Training loss: 0.0314 0.4435 sec/batch\n",
      "Epoch 935/2000  Iteration 1870/4000 Training loss: 0.0178 0.4437 sec/batch\n",
      "Epoch 936/2000  Iteration 1871/4000 Training loss: 0.0319 0.4438 sec/batch\n",
      "Epoch 936/2000  Iteration 1872/4000 Training loss: 0.0186 0.4435 sec/batch\n",
      "Epoch 937/2000  Iteration 1873/4000 Training loss: 0.0318 0.4434 sec/batch\n",
      "Epoch 937/2000  Iteration 1874/4000 Training loss: 0.0181 0.4439 sec/batch\n",
      "Epoch 938/2000  Iteration 1875/4000 Training loss: 0.0312 0.4437 sec/batch\n",
      "Epoch 938/2000  Iteration 1876/4000 Training loss: 0.0177 0.4439 sec/batch\n",
      "Epoch 939/2000  Iteration 1877/4000 Training loss: 0.0326 0.4434 sec/batch\n",
      "Epoch 939/2000  Iteration 1878/4000 Training loss: 0.0185 0.4437 sec/batch\n",
      "Epoch 940/2000  Iteration 1879/4000 Training loss: 0.0318 0.4436 sec/batch\n",
      "Epoch 940/2000  Iteration 1880/4000 Training loss: 0.0178 0.4436 sec/batch\n",
      "Epoch 941/2000  Iteration 1881/4000 Training loss: 0.0310 0.4433 sec/batch\n",
      "Epoch 941/2000  Iteration 1882/4000 Training loss: 0.0176 0.4435 sec/batch\n",
      "Epoch 942/2000  Iteration 1883/4000 Training loss: 0.0316 0.4434 sec/batch\n",
      "Epoch 942/2000  Iteration 1884/4000 Training loss: 0.0178 0.4435 sec/batch\n",
      "Epoch 943/2000  Iteration 1885/4000 Training loss: 0.0326 0.4432 sec/batch\n",
      "Epoch 943/2000  Iteration 1886/4000 Training loss: 0.0185 0.4431 sec/batch\n",
      "Epoch 944/2000  Iteration 1887/4000 Training loss: 0.0314 0.4434 sec/batch\n",
      "Epoch 944/2000  Iteration 1888/4000 Training loss: 0.0180 0.4431 sec/batch\n",
      "Epoch 945/2000  Iteration 1889/4000 Training loss: 0.0311 0.4431 sec/batch\n",
      "Epoch 945/2000  Iteration 1890/4000 Training loss: 0.0175 0.4436 sec/batch\n",
      "Epoch 946/2000  Iteration 1891/4000 Training loss: 0.0314 0.4448 sec/batch\n",
      "Epoch 946/2000  Iteration 1892/4000 Training loss: 0.0179 0.4431 sec/batch\n",
      "Epoch 947/2000  Iteration 1893/4000 Training loss: 0.0320 0.4433 sec/batch\n",
      "Epoch 947/2000  Iteration 1894/4000 Training loss: 0.0182 0.4437 sec/batch\n",
      "Epoch 948/2000  Iteration 1895/4000 Training loss: 0.0314 0.4430 sec/batch\n",
      "Epoch 948/2000  Iteration 1896/4000 Training loss: 0.0175 0.4435 sec/batch\n",
      "Epoch 949/2000  Iteration 1897/4000 Training loss: 0.0317 0.4430 sec/batch\n",
      "Epoch 949/2000  Iteration 1898/4000 Training loss: 0.0180 0.4433 sec/batch\n",
      "Epoch 950/2000  Iteration 1899/4000 Training loss: 0.0308 0.4430 sec/batch\n",
      "Epoch 950/2000  Iteration 1900/4000 Training loss: 0.0174 0.4433 sec/batch\n",
      "Epoch 951/2000  Iteration 1901/4000 Training loss: 0.0305 0.4433 sec/batch\n",
      "Epoch 951/2000  Iteration 1902/4000 Training loss: 0.0171 0.4434 sec/batch\n",
      "Epoch 952/2000  Iteration 1903/4000 Training loss: 0.0312 0.4432 sec/batch\n",
      "Epoch 952/2000  Iteration 1904/4000 Training loss: 0.0177 0.4432 sec/batch\n",
      "Epoch 953/2000  Iteration 1905/4000 Training loss: 0.0310 0.4432 sec/batch\n",
      "Epoch 953/2000  Iteration 1906/4000 Training loss: 0.0177 0.4436 sec/batch\n",
      "Epoch 954/2000  Iteration 1907/4000 Training loss: 0.0292 0.4432 sec/batch\n",
      "Epoch 954/2000  Iteration 1908/4000 Training loss: 0.0165 0.4435 sec/batch\n",
      "Epoch 955/2000  Iteration 1909/4000 Training loss: 0.0325 0.4432 sec/batch\n",
      "Epoch 955/2000  Iteration 1910/4000 Training loss: 0.0183 0.4436 sec/batch\n",
      "Epoch 956/2000  Iteration 1911/4000 Training loss: 0.0306 0.4435 sec/batch\n",
      "Epoch 956/2000  Iteration 1912/4000 Training loss: 0.0172 0.4437 sec/batch\n",
      "Epoch 957/2000  Iteration 1913/4000 Training loss: 0.0307 0.4432 sec/batch\n",
      "Epoch 957/2000  Iteration 1914/4000 Training loss: 0.0175 0.4443 sec/batch\n",
      "Epoch 958/2000  Iteration 1915/4000 Training loss: 0.0305 0.4436 sec/batch\n",
      "Epoch 958/2000  Iteration 1916/4000 Training loss: 0.0171 0.4437 sec/batch\n",
      "Epoch 959/2000  Iteration 1917/4000 Training loss: 0.0297 0.4438 sec/batch\n",
      "Epoch 959/2000  Iteration 1918/4000 Training loss: 0.0169 0.4435 sec/batch\n",
      "Epoch 960/2000  Iteration 1919/4000 Training loss: 0.0303 0.4436 sec/batch\n",
      "Epoch 960/2000  Iteration 1920/4000 Training loss: 0.0171 0.4436 sec/batch\n",
      "Epoch 961/2000  Iteration 1921/4000 Training loss: 0.0310 0.4430 sec/batch\n",
      "Epoch 961/2000  Iteration 1922/4000 Training loss: 0.0173 0.4430 sec/batch\n",
      "Epoch 962/2000  Iteration 1923/4000 Training loss: 0.0306 0.4431 sec/batch\n",
      "Epoch 962/2000  Iteration 1924/4000 Training loss: 0.0173 0.4431 sec/batch\n",
      "Epoch 963/2000  Iteration 1925/4000 Training loss: 0.0310 0.4439 sec/batch\n",
      "Epoch 963/2000  Iteration 1926/4000 Training loss: 0.0175 0.4434 sec/batch\n",
      "Epoch 964/2000  Iteration 1927/4000 Training loss: 0.0312 0.4433 sec/batch\n",
      "Epoch 964/2000  Iteration 1928/4000 Training loss: 0.0174 0.4433 sec/batch\n",
      "Epoch 965/2000  Iteration 1929/4000 Training loss: 0.0311 0.4437 sec/batch\n",
      "Epoch 965/2000  Iteration 1930/4000 Training loss: 0.0176 0.4439 sec/batch\n",
      "Epoch 966/2000  Iteration 1931/4000 Training loss: 0.0310 0.4429 sec/batch\n",
      "Epoch 966/2000  Iteration 1932/4000 Training loss: 0.0176 0.4437 sec/batch\n",
      "Epoch 967/2000  Iteration 1933/4000 Training loss: 0.0306 0.4431 sec/batch\n",
      "Epoch 967/2000  Iteration 1934/4000 Training loss: 0.0173 0.4435 sec/batch\n",
      "Epoch 968/2000  Iteration 1935/4000 Training loss: 0.0307 0.4431 sec/batch\n",
      "Epoch 968/2000  Iteration 1936/4000 Training loss: 0.0174 0.4440 sec/batch\n",
      "Epoch 969/2000  Iteration 1937/4000 Training loss: 0.0316 0.4430 sec/batch\n",
      "Epoch 969/2000  Iteration 1938/4000 Training loss: 0.0181 0.4435 sec/batch\n",
      "Epoch 970/2000  Iteration 1939/4000 Training loss: 0.0307 0.4430 sec/batch\n",
      "Epoch 970/2000  Iteration 1940/4000 Training loss: 0.0177 0.4436 sec/batch\n",
      "Epoch 971/2000  Iteration 1941/4000 Training loss: 0.0305 0.4434 sec/batch\n",
      "Epoch 971/2000  Iteration 1942/4000 Training loss: 0.0178 0.4433 sec/batch\n",
      "Epoch 972/2000  Iteration 1943/4000 Training loss: 0.0310 0.4436 sec/batch\n",
      "Epoch 972/2000  Iteration 1944/4000 Training loss: 0.0176 0.4432 sec/batch\n",
      "Epoch 973/2000  Iteration 1945/4000 Training loss: 0.0316 0.4432 sec/batch\n",
      "Epoch 973/2000  Iteration 1946/4000 Training loss: 0.0183 0.4434 sec/batch\n",
      "Epoch 974/2000  Iteration 1947/4000 Training loss: 0.0311 0.4429 sec/batch\n",
      "Epoch 974/2000  Iteration 1948/4000 Training loss: 0.0179 0.4436 sec/batch\n",
      "Epoch 975/2000  Iteration 1949/4000 Training loss: 0.0305 0.4433 sec/batch\n",
      "Epoch 975/2000  Iteration 1950/4000 Training loss: 0.0172 0.4434 sec/batch\n",
      "Epoch 976/2000  Iteration 1951/4000 Training loss: 0.0307 0.4433 sec/batch\n",
      "Epoch 976/2000  Iteration 1952/4000 Training loss: 0.0179 0.4437 sec/batch\n",
      "Epoch 977/2000  Iteration 1953/4000 Training loss: 0.0308 0.4460 sec/batch\n",
      "Epoch 977/2000  Iteration 1954/4000 Training loss: 0.0177 0.4436 sec/batch\n",
      "Epoch 978/2000  Iteration 1955/4000 Training loss: 0.0300 0.4429 sec/batch\n",
      "Epoch 978/2000  Iteration 1956/4000 Training loss: 0.0172 0.4438 sec/batch\n",
      "Epoch 979/2000  Iteration 1957/4000 Training loss: 0.0305 0.4432 sec/batch\n",
      "Epoch 979/2000  Iteration 1958/4000 Training loss: 0.0174 0.4435 sec/batch\n",
      "Epoch 980/2000  Iteration 1959/4000 Training loss: 0.0287 0.4429 sec/batch\n",
      "Epoch 980/2000  Iteration 1960/4000 Training loss: 0.0163 0.4440 sec/batch\n",
      "Epoch 981/2000  Iteration 1961/4000 Training loss: 0.0300 0.4430 sec/batch\n",
      "Epoch 981/2000  Iteration 1962/4000 Training loss: 0.0169 0.4437 sec/batch\n",
      "Epoch 982/2000  Iteration 1963/4000 Training loss: 0.0301 0.4437 sec/batch\n",
      "Epoch 982/2000  Iteration 1964/4000 Training loss: 0.0176 0.4436 sec/batch\n",
      "Epoch 983/2000  Iteration 1965/4000 Training loss: 0.0296 0.4434 sec/batch\n",
      "Epoch 983/2000  Iteration 1966/4000 Training loss: 0.0168 0.4436 sec/batch\n",
      "Epoch 984/2000  Iteration 1967/4000 Training loss: 0.0304 0.4435 sec/batch\n",
      "Epoch 984/2000  Iteration 1968/4000 Training loss: 0.0174 0.4434 sec/batch\n",
      "Epoch 985/2000  Iteration 1969/4000 Training loss: 0.0300 0.4438 sec/batch\n",
      "Epoch 985/2000  Iteration 1970/4000 Training loss: 0.0171 0.4432 sec/batch\n",
      "Epoch 986/2000  Iteration 1971/4000 Training loss: 0.0305 0.4430 sec/batch\n",
      "Epoch 986/2000  Iteration 1972/4000 Training loss: 0.0178 0.4434 sec/batch\n",
      "Epoch 987/2000  Iteration 1973/4000 Training loss: 0.0299 0.4433 sec/batch\n",
      "Epoch 987/2000  Iteration 1974/4000 Training loss: 0.0172 0.4435 sec/batch\n",
      "Epoch 988/2000  Iteration 1975/4000 Training loss: 0.0301 0.4435 sec/batch\n",
      "Epoch 988/2000  Iteration 1976/4000 Training loss: 0.0174 0.4434 sec/batch\n",
      "Epoch 989/2000  Iteration 1977/4000 Training loss: 0.0299 0.4432 sec/batch\n",
      "Epoch 989/2000  Iteration 1978/4000 Training loss: 0.0171 0.4436 sec/batch\n",
      "Epoch 990/2000  Iteration 1979/4000 Training loss: 0.0308 0.4435 sec/batch\n",
      "Epoch 990/2000  Iteration 1980/4000 Training loss: 0.0176 0.4435 sec/batch\n",
      "Epoch 991/2000  Iteration 1981/4000 Training loss: 0.0297 0.4433 sec/batch\n",
      "Epoch 991/2000  Iteration 1982/4000 Training loss: 0.0170 0.4440 sec/batch\n",
      "Epoch 992/2000  Iteration 1983/4000 Training loss: 0.0295 0.4433 sec/batch\n",
      "Epoch 992/2000  Iteration 1984/4000 Training loss: 0.0167 0.4432 sec/batch\n",
      "Epoch 993/2000  Iteration 1985/4000 Training loss: 0.0311 0.4434 sec/batch\n",
      "Epoch 993/2000  Iteration 1986/4000 Training loss: 0.0180 0.4434 sec/batch\n",
      "Epoch 994/2000  Iteration 1987/4000 Training loss: 0.0290 0.4434 sec/batch\n",
      "Epoch 994/2000  Iteration 1988/4000 Training loss: 0.0171 0.4436 sec/batch\n",
      "Epoch 995/2000  Iteration 1989/4000 Training loss: 0.0303 0.4434 sec/batch\n",
      "Epoch 995/2000  Iteration 1990/4000 Training loss: 0.0171 0.4434 sec/batch\n",
      "Epoch 996/2000  Iteration 1991/4000 Training loss: 0.0305 0.4430 sec/batch\n",
      "Epoch 996/2000  Iteration 1992/4000 Training loss: 0.0174 0.4436 sec/batch\n",
      "Epoch 997/2000  Iteration 1993/4000 Training loss: 0.0299 0.4434 sec/batch\n",
      "Epoch 997/2000  Iteration 1994/4000 Training loss: 0.0169 0.4435 sec/batch\n",
      "Epoch 998/2000  Iteration 1995/4000 Training loss: 0.0299 0.4429 sec/batch\n",
      "Epoch 998/2000  Iteration 1996/4000 Training loss: 0.0175 0.4434 sec/batch\n",
      "Epoch 999/2000  Iteration 1997/4000 Training loss: 0.0299 0.4430 sec/batch\n",
      "Epoch 999/2000  Iteration 1998/4000 Training loss: 0.0174 0.4435 sec/batch\n",
      "Epoch 1000/2000  Iteration 1999/4000 Training loss: 0.0300 0.4433 sec/batch\n",
      "Epoch 1000/2000  Iteration 2000/4000 Training loss: 0.0177 0.4437 sec/batch\n",
      "Validation loss: 6.2147 Saving checkpoint!\n",
      "Epoch 1001/2000  Iteration 2001/4000 Training loss: 0.0287 0.4439 sec/batch\n",
      "Epoch 1001/2000  Iteration 2002/4000 Training loss: 0.0163 0.4436 sec/batch\n",
      "Epoch 1002/2000  Iteration 2003/4000 Training loss: 0.0296 0.4437 sec/batch\n",
      "Epoch 1002/2000  Iteration 2004/4000 Training loss: 0.0170 0.4439 sec/batch\n",
      "Epoch 1003/2000  Iteration 2005/4000 Training loss: 0.0309 0.4432 sec/batch\n",
      "Epoch 1003/2000  Iteration 2006/4000 Training loss: 0.0176 0.4440 sec/batch\n",
      "Epoch 1004/2000  Iteration 2007/4000 Training loss: 0.0291 0.4429 sec/batch\n",
      "Epoch 1004/2000  Iteration 2008/4000 Training loss: 0.0163 0.4437 sec/batch\n",
      "Epoch 1005/2000  Iteration 2009/4000 Training loss: 0.0310 0.4433 sec/batch\n",
      "Epoch 1005/2000  Iteration 2010/4000 Training loss: 0.0177 0.4438 sec/batch\n",
      "Epoch 1006/2000  Iteration 2011/4000 Training loss: 0.0287 0.4434 sec/batch\n",
      "Epoch 1006/2000  Iteration 2012/4000 Training loss: 0.0162 0.4436 sec/batch\n",
      "Epoch 1007/2000  Iteration 2013/4000 Training loss: 0.0300 0.4431 sec/batch\n",
      "Epoch 1007/2000  Iteration 2014/4000 Training loss: 0.0169 0.4437 sec/batch\n",
      "Epoch 1008/2000  Iteration 2015/4000 Training loss: 0.0302 0.4431 sec/batch\n",
      "Epoch 1008/2000  Iteration 2016/4000 Training loss: 0.0171 0.4438 sec/batch\n",
      "Epoch 1009/2000  Iteration 2017/4000 Training loss: 0.0301 0.4435 sec/batch\n",
      "Epoch 1009/2000  Iteration 2018/4000 Training loss: 0.0170 0.4439 sec/batch\n",
      "Epoch 1010/2000  Iteration 2019/4000 Training loss: 0.0291 0.4435 sec/batch\n",
      "Epoch 1010/2000  Iteration 2020/4000 Training loss: 0.0166 0.4439 sec/batch\n",
      "Epoch 1011/2000  Iteration 2021/4000 Training loss: 0.0285 0.4437 sec/batch\n",
      "Epoch 1011/2000  Iteration 2022/4000 Training loss: 0.0160 0.4437 sec/batch\n",
      "Epoch 1012/2000  Iteration 2023/4000 Training loss: 0.0297 0.4436 sec/batch\n",
      "Epoch 1012/2000  Iteration 2024/4000 Training loss: 0.0168 0.4439 sec/batch\n",
      "Epoch 1013/2000  Iteration 2025/4000 Training loss: 0.0296 0.4436 sec/batch\n",
      "Epoch 1013/2000  Iteration 2026/4000 Training loss: 0.0167 0.4439 sec/batch\n",
      "Epoch 1014/2000  Iteration 2027/4000 Training loss: 0.0292 0.4432 sec/batch\n",
      "Epoch 1014/2000  Iteration 2028/4000 Training loss: 0.0163 0.4433 sec/batch\n",
      "Epoch 1015/2000  Iteration 2029/4000 Training loss: 0.0300 0.4437 sec/batch\n",
      "Epoch 1015/2000  Iteration 2030/4000 Training loss: 0.0169 0.4433 sec/batch\n",
      "Epoch 1016/2000  Iteration 2031/4000 Training loss: 0.0289 0.4458 sec/batch\n",
      "Epoch 1016/2000  Iteration 2032/4000 Training loss: 0.0166 0.4434 sec/batch\n",
      "Epoch 1017/2000  Iteration 2033/4000 Training loss: 0.0291 0.4436 sec/batch\n",
      "Epoch 1017/2000  Iteration 2034/4000 Training loss: 0.0165 0.4434 sec/batch\n",
      "Epoch 1018/2000  Iteration 2035/4000 Training loss: 0.0287 0.4432 sec/batch\n",
      "Epoch 1018/2000  Iteration 2036/4000 Training loss: 0.0159 0.4434 sec/batch\n",
      "Epoch 1019/2000  Iteration 2037/4000 Training loss: 0.0294 0.4430 sec/batch\n",
      "Epoch 1019/2000  Iteration 2038/4000 Training loss: 0.0165 0.4438 sec/batch\n",
      "Epoch 1020/2000  Iteration 2039/4000 Training loss: 0.0295 0.4431 sec/batch\n",
      "Epoch 1020/2000  Iteration 2040/4000 Training loss: 0.0167 0.4441 sec/batch\n",
      "Epoch 1021/2000  Iteration 2041/4000 Training loss: 0.0305 0.4435 sec/batch\n",
      "Epoch 1021/2000  Iteration 2042/4000 Training loss: 0.0171 0.4436 sec/batch\n",
      "Epoch 1022/2000  Iteration 2043/4000 Training loss: 0.0302 0.4434 sec/batch\n",
      "Epoch 1022/2000  Iteration 2044/4000 Training loss: 0.0171 0.4435 sec/batch\n",
      "Epoch 1023/2000  Iteration 2045/4000 Training loss: 0.0296 0.4439 sec/batch\n",
      "Epoch 1023/2000  Iteration 2046/4000 Training loss: 0.0167 0.4442 sec/batch\n",
      "Epoch 1024/2000  Iteration 2047/4000 Training loss: 0.0296 0.4432 sec/batch\n",
      "Epoch 1024/2000  Iteration 2048/4000 Training loss: 0.0166 0.4438 sec/batch\n",
      "Epoch 1025/2000  Iteration 2049/4000 Training loss: 0.0300 0.4437 sec/batch\n",
      "Epoch 1025/2000  Iteration 2050/4000 Training loss: 0.0168 0.4434 sec/batch\n",
      "Epoch 1026/2000  Iteration 2051/4000 Training loss: 0.0287 0.4433 sec/batch\n",
      "Epoch 1026/2000  Iteration 2052/4000 Training loss: 0.0167 0.4443 sec/batch\n",
      "Epoch 1027/2000  Iteration 2053/4000 Training loss: 0.0291 0.4431 sec/batch\n",
      "Epoch 1027/2000  Iteration 2054/4000 Training loss: 0.0168 0.4447 sec/batch\n",
      "Epoch 1028/2000  Iteration 2055/4000 Training loss: 0.0301 0.4435 sec/batch\n",
      "Epoch 1028/2000  Iteration 2056/4000 Training loss: 0.0168 0.4436 sec/batch\n",
      "Epoch 1029/2000  Iteration 2057/4000 Training loss: 0.0295 0.4440 sec/batch\n",
      "Epoch 1029/2000  Iteration 2058/4000 Training loss: 0.0170 0.4438 sec/batch\n",
      "Epoch 1030/2000  Iteration 2059/4000 Training loss: 0.0294 0.4435 sec/batch\n",
      "Epoch 1030/2000  Iteration 2060/4000 Training loss: 0.0166 0.4438 sec/batch\n",
      "Epoch 1031/2000  Iteration 2061/4000 Training loss: 0.0295 0.4436 sec/batch\n",
      "Epoch 1031/2000  Iteration 2062/4000 Training loss: 0.0166 0.4439 sec/batch\n",
      "Epoch 1032/2000  Iteration 2063/4000 Training loss: 0.0293 0.4437 sec/batch\n",
      "Epoch 1032/2000  Iteration 2064/4000 Training loss: 0.0170 0.4440 sec/batch\n",
      "Epoch 1033/2000  Iteration 2065/4000 Training loss: 0.0294 0.4440 sec/batch\n",
      "Epoch 1033/2000  Iteration 2066/4000 Training loss: 0.0166 0.4439 sec/batch\n",
      "Epoch 1034/2000  Iteration 2067/4000 Training loss: 0.0277 0.4436 sec/batch\n",
      "Epoch 1034/2000  Iteration 2068/4000 Training loss: 0.0158 0.4439 sec/batch\n",
      "Epoch 1035/2000  Iteration 2069/4000 Training loss: 0.0293 0.4436 sec/batch\n",
      "Epoch 1035/2000  Iteration 2070/4000 Training loss: 0.0168 0.4439 sec/batch\n",
      "Epoch 1036/2000  Iteration 2071/4000 Training loss: 0.0283 0.4435 sec/batch\n",
      "Epoch 1036/2000  Iteration 2072/4000 Training loss: 0.0166 0.4441 sec/batch\n",
      "Epoch 1037/2000  Iteration 2073/4000 Training loss: 0.0289 0.4441 sec/batch\n",
      "Epoch 1037/2000  Iteration 2074/4000 Training loss: 0.0164 0.4436 sec/batch\n",
      "Epoch 1038/2000  Iteration 2075/4000 Training loss: 0.0279 0.4434 sec/batch\n",
      "Epoch 1038/2000  Iteration 2076/4000 Training loss: 0.0166 0.4442 sec/batch\n",
      "Epoch 1039/2000  Iteration 2077/4000 Training loss: 0.0300 0.4434 sec/batch\n",
      "Epoch 1039/2000  Iteration 2078/4000 Training loss: 0.0175 0.4439 sec/batch\n",
      "Epoch 1040/2000  Iteration 2079/4000 Training loss: 0.0295 0.4437 sec/batch\n",
      "Epoch 1040/2000  Iteration 2080/4000 Training loss: 0.0167 0.4438 sec/batch\n",
      "Epoch 1041/2000  Iteration 2081/4000 Training loss: 0.0295 0.4437 sec/batch\n",
      "Epoch 1041/2000  Iteration 2082/4000 Training loss: 0.0176 0.4446 sec/batch\n",
      "Epoch 1042/2000  Iteration 2083/4000 Training loss: 0.0283 0.4440 sec/batch\n",
      "Epoch 1042/2000  Iteration 2084/4000 Training loss: 0.0162 0.4439 sec/batch\n",
      "Epoch 1043/2000  Iteration 2085/4000 Training loss: 0.0285 0.4431 sec/batch\n",
      "Epoch 1043/2000  Iteration 2086/4000 Training loss: 0.0165 0.4439 sec/batch\n",
      "Epoch 1044/2000  Iteration 2087/4000 Training loss: 0.0298 0.4435 sec/batch\n",
      "Epoch 1044/2000  Iteration 2088/4000 Training loss: 0.0168 0.4438 sec/batch\n",
      "Epoch 1045/2000  Iteration 2089/4000 Training loss: 0.0284 0.4439 sec/batch\n",
      "Epoch 1045/2000  Iteration 2090/4000 Training loss: 0.0162 0.4437 sec/batch\n",
      "Epoch 1046/2000  Iteration 2091/4000 Training loss: 0.0276 0.4437 sec/batch\n",
      "Epoch 1046/2000  Iteration 2092/4000 Training loss: 0.0158 0.4437 sec/batch\n",
      "Epoch 1047/2000  Iteration 2093/4000 Training loss: 0.0284 0.4436 sec/batch\n",
      "Epoch 1047/2000  Iteration 2094/4000 Training loss: 0.0164 0.4437 sec/batch\n",
      "Epoch 1048/2000  Iteration 2095/4000 Training loss: 0.0281 0.4435 sec/batch\n",
      "Epoch 1048/2000  Iteration 2096/4000 Training loss: 0.0159 0.4440 sec/batch\n",
      "Epoch 1049/2000  Iteration 2097/4000 Training loss: 0.0296 0.4437 sec/batch\n",
      "Epoch 1049/2000  Iteration 2098/4000 Training loss: 0.0167 0.4445 sec/batch\n",
      "Epoch 1050/2000  Iteration 2099/4000 Training loss: 0.0284 0.4436 sec/batch\n",
      "Epoch 1050/2000  Iteration 2100/4000 Training loss: 0.0161 0.4445 sec/batch\n",
      "Epoch 1051/2000  Iteration 2101/4000 Training loss: 0.0295 0.4439 sec/batch\n",
      "Epoch 1051/2000  Iteration 2102/4000 Training loss: 0.0169 0.4440 sec/batch\n",
      "Epoch 1052/2000  Iteration 2103/4000 Training loss: 0.0277 0.4444 sec/batch\n",
      "Epoch 1052/2000  Iteration 2104/4000 Training loss: 0.0156 0.4445 sec/batch\n",
      "Epoch 1053/2000  Iteration 2105/4000 Training loss: 0.0293 0.4438 sec/batch\n",
      "Epoch 1053/2000  Iteration 2106/4000 Training loss: 0.0165 0.4436 sec/batch\n",
      "Epoch 1054/2000  Iteration 2107/4000 Training loss: 0.0285 0.4430 sec/batch\n",
      "Epoch 1054/2000  Iteration 2108/4000 Training loss: 0.0160 0.4436 sec/batch\n",
      "Epoch 1055/2000  Iteration 2109/4000 Training loss: 0.0290 0.4434 sec/batch\n",
      "Epoch 1055/2000  Iteration 2110/4000 Training loss: 0.0163 0.4433 sec/batch\n",
      "Epoch 1056/2000  Iteration 2111/4000 Training loss: 0.0289 0.4430 sec/batch\n",
      "Epoch 1056/2000  Iteration 2112/4000 Training loss: 0.0160 0.4437 sec/batch\n",
      "Epoch 1057/2000  Iteration 2113/4000 Training loss: 0.0282 0.4434 sec/batch\n",
      "Epoch 1057/2000  Iteration 2114/4000 Training loss: 0.0157 0.4435 sec/batch\n",
      "Epoch 1058/2000  Iteration 2115/4000 Training loss: 0.0281 0.4434 sec/batch\n",
      "Epoch 1058/2000  Iteration 2116/4000 Training loss: 0.0158 0.4430 sec/batch\n",
      "Epoch 1059/2000  Iteration 2117/4000 Training loss: 0.0283 0.4428 sec/batch\n",
      "Epoch 1059/2000  Iteration 2118/4000 Training loss: 0.0159 0.4432 sec/batch\n",
      "Epoch 1060/2000  Iteration 2119/4000 Training loss: 0.0284 0.4434 sec/batch\n",
      "Epoch 1060/2000  Iteration 2120/4000 Training loss: 0.0161 0.4436 sec/batch\n",
      "Epoch 1061/2000  Iteration 2121/4000 Training loss: 0.0285 0.4430 sec/batch\n",
      "Epoch 1061/2000  Iteration 2122/4000 Training loss: 0.0163 0.4432 sec/batch\n",
      "Epoch 1062/2000  Iteration 2123/4000 Training loss: 0.0291 0.4427 sec/batch\n",
      "Epoch 1062/2000  Iteration 2124/4000 Training loss: 0.0161 0.4435 sec/batch\n",
      "Epoch 1063/2000  Iteration 2125/4000 Training loss: 0.0278 0.4432 sec/batch\n",
      "Epoch 1063/2000  Iteration 2126/4000 Training loss: 0.0155 0.4436 sec/batch\n",
      "Epoch 1064/2000  Iteration 2127/4000 Training loss: 0.0276 0.4429 sec/batch\n",
      "Epoch 1064/2000  Iteration 2128/4000 Training loss: 0.0155 0.4432 sec/batch\n",
      "Epoch 1065/2000  Iteration 2129/4000 Training loss: 0.0284 0.4427 sec/batch\n",
      "Epoch 1065/2000  Iteration 2130/4000 Training loss: 0.0162 0.4434 sec/batch\n",
      "Epoch 1066/2000  Iteration 2131/4000 Training loss: 0.0294 0.4433 sec/batch\n",
      "Epoch 1066/2000  Iteration 2132/4000 Training loss: 0.0163 0.4436 sec/batch\n",
      "Epoch 1067/2000  Iteration 2133/4000 Training loss: 0.0284 0.4429 sec/batch\n",
      "Epoch 1067/2000  Iteration 2134/4000 Training loss: 0.0159 0.4434 sec/batch\n",
      "Epoch 1068/2000  Iteration 2135/4000 Training loss: 0.0283 0.4432 sec/batch\n",
      "Epoch 1068/2000  Iteration 2136/4000 Training loss: 0.0163 0.4439 sec/batch\n",
      "Epoch 1069/2000  Iteration 2137/4000 Training loss: 0.0292 0.4434 sec/batch\n",
      "Epoch 1069/2000  Iteration 2138/4000 Training loss: 0.0166 0.4432 sec/batch\n",
      "Epoch 1070/2000  Iteration 2139/4000 Training loss: 0.0279 0.4436 sec/batch\n",
      "Epoch 1070/2000  Iteration 2140/4000 Training loss: 0.0159 0.4433 sec/batch\n",
      "Epoch 1071/2000  Iteration 2141/4000 Training loss: 0.0281 0.4434 sec/batch\n",
      "Epoch 1071/2000  Iteration 2142/4000 Training loss: 0.0163 0.4438 sec/batch\n",
      "Epoch 1072/2000  Iteration 2143/4000 Training loss: 0.0272 0.4434 sec/batch\n",
      "Epoch 1072/2000  Iteration 2144/4000 Training loss: 0.0156 0.4435 sec/batch\n",
      "Epoch 1073/2000  Iteration 2145/4000 Training loss: 0.0275 0.4430 sec/batch\n",
      "Epoch 1073/2000  Iteration 2146/4000 Training loss: 0.0155 0.4433 sec/batch\n",
      "Epoch 1074/2000  Iteration 2147/4000 Training loss: 0.0290 0.4433 sec/batch\n",
      "Epoch 1074/2000  Iteration 2148/4000 Training loss: 0.0170 0.4436 sec/batch\n",
      "Epoch 1075/2000  Iteration 2149/4000 Training loss: 0.0272 0.4434 sec/batch\n",
      "Epoch 1075/2000  Iteration 2150/4000 Training loss: 0.0160 0.4432 sec/batch\n",
      "Epoch 1076/2000  Iteration 2151/4000 Training loss: 0.0274 0.4432 sec/batch\n",
      "Epoch 1076/2000  Iteration 2152/4000 Training loss: 0.0158 0.4438 sec/batch\n",
      "Epoch 1077/2000  Iteration 2153/4000 Training loss: 0.0279 0.4429 sec/batch\n",
      "Epoch 1077/2000  Iteration 2154/4000 Training loss: 0.0160 0.4435 sec/batch\n",
      "Epoch 1078/2000  Iteration 2155/4000 Training loss: 0.0288 0.4431 sec/batch\n",
      "Epoch 1078/2000  Iteration 2156/4000 Training loss: 0.0168 0.4432 sec/batch\n",
      "Epoch 1079/2000  Iteration 2157/4000 Training loss: 0.0280 0.4433 sec/batch\n",
      "Epoch 1079/2000  Iteration 2158/4000 Training loss: 0.0158 0.4436 sec/batch\n",
      "Epoch 1080/2000  Iteration 2159/4000 Training loss: 0.0283 0.4436 sec/batch\n",
      "Epoch 1080/2000  Iteration 2160/4000 Training loss: 0.0158 0.4435 sec/batch\n",
      "Epoch 1081/2000  Iteration 2161/4000 Training loss: 0.0276 0.4436 sec/batch\n",
      "Epoch 1081/2000  Iteration 2162/4000 Training loss: 0.0155 0.4440 sec/batch\n",
      "Epoch 1082/2000  Iteration 2163/4000 Training loss: 0.0292 0.4434 sec/batch\n",
      "Epoch 1082/2000  Iteration 2164/4000 Training loss: 0.0162 0.4437 sec/batch\n",
      "Epoch 1083/2000  Iteration 2165/4000 Training loss: 0.0280 0.4431 sec/batch\n",
      "Epoch 1083/2000  Iteration 2166/4000 Training loss: 0.0154 0.4432 sec/batch\n",
      "Epoch 1084/2000  Iteration 2167/4000 Training loss: 0.0274 0.4434 sec/batch\n",
      "Epoch 1084/2000  Iteration 2168/4000 Training loss: 0.0151 0.4431 sec/batch\n",
      "Epoch 1085/2000  Iteration 2169/4000 Training loss: 0.0296 0.4430 sec/batch\n",
      "Epoch 1085/2000  Iteration 2170/4000 Training loss: 0.0167 0.4431 sec/batch\n",
      "Epoch 1086/2000  Iteration 2171/4000 Training loss: 0.0262 0.4434 sec/batch\n",
      "Epoch 1086/2000  Iteration 2172/4000 Training loss: 0.0148 0.4434 sec/batch\n",
      "Epoch 1087/2000  Iteration 2173/4000 Training loss: 0.0276 0.4434 sec/batch\n",
      "Epoch 1087/2000  Iteration 2174/4000 Training loss: 0.0155 0.4436 sec/batch\n",
      "Epoch 1088/2000  Iteration 2175/4000 Training loss: 0.0288 0.4434 sec/batch\n",
      "Epoch 1088/2000  Iteration 2176/4000 Training loss: 0.0162 0.4433 sec/batch\n",
      "Epoch 1089/2000  Iteration 2177/4000 Training loss: 0.0265 0.4431 sec/batch\n",
      "Epoch 1089/2000  Iteration 2178/4000 Training loss: 0.0150 0.4437 sec/batch\n",
      "Epoch 1090/2000  Iteration 2179/4000 Training loss: 0.0276 0.4432 sec/batch\n",
      "Epoch 1090/2000  Iteration 2180/4000 Training loss: 0.0155 0.4436 sec/batch\n",
      "Epoch 1091/2000  Iteration 2181/4000 Training loss: 0.0292 0.4436 sec/batch\n",
      "Epoch 1091/2000  Iteration 2182/4000 Training loss: 0.0161 0.4433 sec/batch\n",
      "Epoch 1092/2000  Iteration 2183/4000 Training loss: 0.0288 0.4450 sec/batch\n",
      "Epoch 1092/2000  Iteration 2184/4000 Training loss: 0.0162 0.4441 sec/batch\n",
      "Epoch 1093/2000  Iteration 2185/4000 Training loss: 0.0272 0.4435 sec/batch\n",
      "Epoch 1093/2000  Iteration 2186/4000 Training loss: 0.0151 0.4434 sec/batch\n",
      "Epoch 1094/2000  Iteration 2187/4000 Training loss: 0.0275 0.4432 sec/batch\n",
      "Epoch 1094/2000  Iteration 2188/4000 Training loss: 0.0152 0.4433 sec/batch\n",
      "Epoch 1095/2000  Iteration 2189/4000 Training loss: 0.0283 0.4436 sec/batch\n",
      "Epoch 1095/2000  Iteration 2190/4000 Training loss: 0.0158 0.4433 sec/batch\n",
      "Epoch 1096/2000  Iteration 2191/4000 Training loss: 0.0271 0.4433 sec/batch\n",
      "Epoch 1096/2000  Iteration 2192/4000 Training loss: 0.0154 0.4431 sec/batch\n",
      "Epoch 1097/2000  Iteration 2193/4000 Training loss: 0.0271 0.4431 sec/batch\n",
      "Epoch 1097/2000  Iteration 2194/4000 Training loss: 0.0151 0.4429 sec/batch\n",
      "Epoch 1098/2000  Iteration 2195/4000 Training loss: 0.0279 0.4435 sec/batch\n",
      "Epoch 1098/2000  Iteration 2196/4000 Training loss: 0.0154 0.4430 sec/batch\n",
      "Epoch 1099/2000  Iteration 2197/4000 Training loss: 0.0277 0.4431 sec/batch\n",
      "Epoch 1099/2000  Iteration 2198/4000 Training loss: 0.0154 0.4433 sec/batch\n",
      "Epoch 1100/2000  Iteration 2199/4000 Training loss: 0.0264 0.4431 sec/batch\n",
      "Epoch 1100/2000  Iteration 2200/4000 Training loss: 0.0146 0.4433 sec/batch\n",
      "Validation loss: 6.36603 Saving checkpoint!\n",
      "Epoch 1101/2000  Iteration 2201/4000 Training loss: 0.0276 0.4433 sec/batch\n",
      "Epoch 1101/2000  Iteration 2202/4000 Training loss: 0.0155 0.4441 sec/batch\n",
      "Epoch 1102/2000  Iteration 2203/4000 Training loss: 0.0268 0.4436 sec/batch\n",
      "Epoch 1102/2000  Iteration 2204/4000 Training loss: 0.0148 0.4434 sec/batch\n",
      "Epoch 1103/2000  Iteration 2205/4000 Training loss: 0.0286 0.4461 sec/batch\n",
      "Epoch 1103/2000  Iteration 2206/4000 Training loss: 0.0160 0.4433 sec/batch\n",
      "Epoch 1104/2000  Iteration 2207/4000 Training loss: 0.0283 0.4431 sec/batch\n",
      "Epoch 1104/2000  Iteration 2208/4000 Training loss: 0.0156 0.4433 sec/batch\n",
      "Epoch 1105/2000  Iteration 2209/4000 Training loss: 0.0280 0.4433 sec/batch\n",
      "Epoch 1105/2000  Iteration 2210/4000 Training loss: 0.0155 0.4435 sec/batch\n",
      "Epoch 1106/2000  Iteration 2211/4000 Training loss: 0.0280 0.4430 sec/batch\n",
      "Epoch 1106/2000  Iteration 2212/4000 Training loss: 0.0157 0.4433 sec/batch\n",
      "Epoch 1107/2000  Iteration 2213/4000 Training loss: 0.0266 0.4432 sec/batch\n",
      "Epoch 1107/2000  Iteration 2214/4000 Training loss: 0.0148 0.4432 sec/batch\n",
      "Epoch 1108/2000  Iteration 2215/4000 Training loss: 0.0272 0.4432 sec/batch\n",
      "Epoch 1108/2000  Iteration 2216/4000 Training loss: 0.0151 0.4431 sec/batch\n",
      "Epoch 1109/2000  Iteration 2217/4000 Training loss: 0.0271 0.4432 sec/batch\n",
      "Epoch 1109/2000  Iteration 2218/4000 Training loss: 0.0149 0.4435 sec/batch\n",
      "Epoch 1110/2000  Iteration 2219/4000 Training loss: 0.0273 0.4433 sec/batch\n",
      "Epoch 1110/2000  Iteration 2220/4000 Training loss: 0.0154 0.4432 sec/batch\n",
      "Epoch 1111/2000  Iteration 2221/4000 Training loss: 0.0269 0.4431 sec/batch\n",
      "Epoch 1111/2000  Iteration 2222/4000 Training loss: 0.0149 0.4437 sec/batch\n",
      "Epoch 1112/2000  Iteration 2223/4000 Training loss: 0.0273 0.4434 sec/batch\n",
      "Epoch 1112/2000  Iteration 2224/4000 Training loss: 0.0154 0.4434 sec/batch\n",
      "Epoch 1113/2000  Iteration 2225/4000 Training loss: 0.0266 0.4434 sec/batch\n",
      "Epoch 1113/2000  Iteration 2226/4000 Training loss: 0.0145 0.4435 sec/batch\n",
      "Epoch 1114/2000  Iteration 2227/4000 Training loss: 0.0266 0.4430 sec/batch\n",
      "Epoch 1114/2000  Iteration 2228/4000 Training loss: 0.0145 0.4432 sec/batch\n",
      "Epoch 1115/2000  Iteration 2229/4000 Training loss: 0.0265 0.4428 sec/batch\n",
      "Epoch 1115/2000  Iteration 2230/4000 Training loss: 0.0145 0.4434 sec/batch\n",
      "Epoch 1116/2000  Iteration 2231/4000 Training loss: 0.0269 0.4432 sec/batch\n",
      "Epoch 1116/2000  Iteration 2232/4000 Training loss: 0.0148 0.4439 sec/batch\n",
      "Epoch 1117/2000  Iteration 2233/4000 Training loss: 0.0266 0.4430 sec/batch\n",
      "Epoch 1117/2000  Iteration 2234/4000 Training loss: 0.0146 0.4440 sec/batch\n",
      "Epoch 1118/2000  Iteration 2235/4000 Training loss: 0.0265 0.4432 sec/batch\n",
      "Epoch 1118/2000  Iteration 2236/4000 Training loss: 0.0144 0.4435 sec/batch\n",
      "Epoch 1119/2000  Iteration 2237/4000 Training loss: 0.0255 0.4436 sec/batch\n",
      "Epoch 1119/2000  Iteration 2238/4000 Training loss: 0.0145 0.4441 sec/batch\n",
      "Epoch 1120/2000  Iteration 2239/4000 Training loss: 0.0273 0.4431 sec/batch\n",
      "Epoch 1120/2000  Iteration 2240/4000 Training loss: 0.0149 0.4437 sec/batch\n",
      "Epoch 1121/2000  Iteration 2241/4000 Training loss: 0.0272 0.4431 sec/batch\n",
      "Epoch 1121/2000  Iteration 2242/4000 Training loss: 0.0147 0.4437 sec/batch\n",
      "Epoch 1122/2000  Iteration 2243/4000 Training loss: 0.0274 0.4435 sec/batch\n",
      "Epoch 1122/2000  Iteration 2244/4000 Training loss: 0.0151 0.4437 sec/batch\n",
      "Epoch 1123/2000  Iteration 2245/4000 Training loss: 0.0269 0.4435 sec/batch\n",
      "Epoch 1123/2000  Iteration 2246/4000 Training loss: 0.0146 0.4437 sec/batch\n",
      "Epoch 1124/2000  Iteration 2247/4000 Training loss: 0.0276 0.4435 sec/batch\n",
      "Epoch 1124/2000  Iteration 2248/4000 Training loss: 0.0154 0.4436 sec/batch\n",
      "Epoch 1125/2000  Iteration 2249/4000 Training loss: 0.0268 0.4436 sec/batch\n",
      "Epoch 1125/2000  Iteration 2250/4000 Training loss: 0.0150 0.4439 sec/batch\n",
      "Epoch 1126/2000  Iteration 2251/4000 Training loss: 0.0261 0.4432 sec/batch\n",
      "Epoch 1126/2000  Iteration 2252/4000 Training loss: 0.0148 0.4428 sec/batch\n",
      "Epoch 1127/2000  Iteration 2253/4000 Training loss: 0.0276 0.4435 sec/batch\n",
      "Epoch 1127/2000  Iteration 2254/4000 Training loss: 0.0154 0.4434 sec/batch\n",
      "Epoch 1128/2000  Iteration 2255/4000 Training loss: 0.0257 0.4432 sec/batch\n",
      "Epoch 1128/2000  Iteration 2256/4000 Training loss: 0.0144 0.4433 sec/batch\n",
      "Epoch 1129/2000  Iteration 2257/4000 Training loss: 0.0273 0.4432 sec/batch\n",
      "Epoch 1129/2000  Iteration 2258/4000 Training loss: 0.0151 0.4436 sec/batch\n",
      "Epoch 1130/2000  Iteration 2259/4000 Training loss: 0.0270 0.4433 sec/batch\n",
      "Epoch 1130/2000  Iteration 2260/4000 Training loss: 0.0149 0.4438 sec/batch\n",
      "Epoch 1131/2000  Iteration 2261/4000 Training loss: 0.0266 0.4432 sec/batch\n",
      "Epoch 1131/2000  Iteration 2262/4000 Training loss: 0.0153 0.4440 sec/batch\n",
      "Epoch 1132/2000  Iteration 2263/4000 Training loss: 0.0275 0.4433 sec/batch\n",
      "Epoch 1132/2000  Iteration 2264/4000 Training loss: 0.0153 0.4433 sec/batch\n",
      "Epoch 1133/2000  Iteration 2265/4000 Training loss: 0.0265 0.4431 sec/batch\n",
      "Epoch 1133/2000  Iteration 2266/4000 Training loss: 0.0149 0.4433 sec/batch\n",
      "Epoch 1134/2000  Iteration 2267/4000 Training loss: 0.0275 0.4431 sec/batch\n",
      "Epoch 1134/2000  Iteration 2268/4000 Training loss: 0.0155 0.4431 sec/batch\n",
      "Epoch 1135/2000  Iteration 2269/4000 Training loss: 0.0264 0.4430 sec/batch\n",
      "Epoch 1135/2000  Iteration 2270/4000 Training loss: 0.0149 0.4434 sec/batch\n",
      "Epoch 1136/2000  Iteration 2271/4000 Training loss: 0.0273 0.4430 sec/batch\n",
      "Epoch 1136/2000  Iteration 2272/4000 Training loss: 0.0154 0.4432 sec/batch\n",
      "Epoch 1137/2000  Iteration 2273/4000 Training loss: 0.0263 0.4430 sec/batch\n",
      "Epoch 1137/2000  Iteration 2274/4000 Training loss: 0.0147 0.4433 sec/batch\n",
      "Epoch 1138/2000  Iteration 2275/4000 Training loss: 0.0265 0.4435 sec/batch\n",
      "Epoch 1138/2000  Iteration 2276/4000 Training loss: 0.0149 0.4438 sec/batch\n",
      "Epoch 1139/2000  Iteration 2277/4000 Training loss: 0.0271 0.4434 sec/batch\n",
      "Epoch 1139/2000  Iteration 2278/4000 Training loss: 0.0156 0.4447 sec/batch\n",
      "Epoch 1140/2000  Iteration 2279/4000 Training loss: 0.0261 0.4436 sec/batch\n",
      "Epoch 1140/2000  Iteration 2280/4000 Training loss: 0.0150 0.4438 sec/batch\n",
      "Epoch 1141/2000  Iteration 2281/4000 Training loss: 0.0268 0.4433 sec/batch\n",
      "Epoch 1141/2000  Iteration 2282/4000 Training loss: 0.0149 0.4437 sec/batch\n",
      "Epoch 1142/2000  Iteration 2283/4000 Training loss: 0.0258 0.4430 sec/batch\n",
      "Epoch 1142/2000  Iteration 2284/4000 Training loss: 0.0150 0.4434 sec/batch\n",
      "Epoch 1143/2000  Iteration 2285/4000 Training loss: 0.0270 0.4436 sec/batch\n",
      "Epoch 1143/2000  Iteration 2286/4000 Training loss: 0.0150 0.4438 sec/batch\n",
      "Epoch 1144/2000  Iteration 2287/4000 Training loss: 0.0273 0.4436 sec/batch\n",
      "Epoch 1144/2000  Iteration 2288/4000 Training loss: 0.0149 0.4434 sec/batch\n",
      "Epoch 1145/2000  Iteration 2289/4000 Training loss: 0.0282 0.4429 sec/batch\n",
      "Epoch 1145/2000  Iteration 2290/4000 Training loss: 0.0157 0.4432 sec/batch\n",
      "Epoch 1146/2000  Iteration 2291/4000 Training loss: 0.0269 0.4435 sec/batch\n",
      "Epoch 1146/2000  Iteration 2292/4000 Training loss: 0.0150 0.4435 sec/batch\n",
      "Epoch 1147/2000  Iteration 2293/4000 Training loss: 0.0264 0.4428 sec/batch\n",
      "Epoch 1147/2000  Iteration 2294/4000 Training loss: 0.0151 0.4434 sec/batch\n",
      "Epoch 1148/2000  Iteration 2295/4000 Training loss: 0.0264 0.4431 sec/batch\n",
      "Epoch 1148/2000  Iteration 2296/4000 Training loss: 0.0150 0.4436 sec/batch\n",
      "Epoch 1149/2000  Iteration 2297/4000 Training loss: 0.0269 0.4430 sec/batch\n",
      "Epoch 1149/2000  Iteration 2298/4000 Training loss: 0.0149 0.4437 sec/batch\n",
      "Epoch 1150/2000  Iteration 2299/4000 Training loss: 0.0260 0.4429 sec/batch\n",
      "Epoch 1150/2000  Iteration 2300/4000 Training loss: 0.0152 0.4434 sec/batch\n",
      "Epoch 1151/2000  Iteration 2301/4000 Training loss: 0.0270 0.4433 sec/batch\n",
      "Epoch 1151/2000  Iteration 2302/4000 Training loss: 0.0152 0.4434 sec/batch\n",
      "Epoch 1152/2000  Iteration 2303/4000 Training loss: 0.0266 0.4434 sec/batch\n",
      "Epoch 1152/2000  Iteration 2304/4000 Training loss: 0.0153 0.4433 sec/batch\n",
      "Epoch 1153/2000  Iteration 2305/4000 Training loss: 0.0264 0.4433 sec/batch\n",
      "Epoch 1153/2000  Iteration 2306/4000 Training loss: 0.0149 0.4438 sec/batch\n",
      "Epoch 1154/2000  Iteration 2307/4000 Training loss: 0.0257 0.4435 sec/batch\n",
      "Epoch 1154/2000  Iteration 2308/4000 Training loss: 0.0146 0.4434 sec/batch\n",
      "Epoch 1155/2000  Iteration 2309/4000 Training loss: 0.0257 0.4433 sec/batch\n",
      "Epoch 1155/2000  Iteration 2310/4000 Training loss: 0.0145 0.4430 sec/batch\n",
      "Epoch 1156/2000  Iteration 2311/4000 Training loss: 0.0286 0.4429 sec/batch\n",
      "Epoch 1156/2000  Iteration 2312/4000 Training loss: 0.0161 0.4434 sec/batch\n",
      "Epoch 1157/2000  Iteration 2313/4000 Training loss: 0.0271 0.4434 sec/batch\n",
      "Epoch 1157/2000  Iteration 2314/4000 Training loss: 0.0154 0.4435 sec/batch\n",
      "Epoch 1158/2000  Iteration 2315/4000 Training loss: 0.0268 0.4430 sec/batch\n",
      "Epoch 1158/2000  Iteration 2316/4000 Training loss: 0.0152 0.4435 sec/batch\n",
      "Epoch 1159/2000  Iteration 2317/4000 Training loss: 0.0278 0.4433 sec/batch\n",
      "Epoch 1159/2000  Iteration 2318/4000 Training loss: 0.0156 0.4435 sec/batch\n",
      "Epoch 1160/2000  Iteration 2319/4000 Training loss: 0.0284 0.4430 sec/batch\n",
      "Epoch 1160/2000  Iteration 2320/4000 Training loss: 0.0158 0.4431 sec/batch\n",
      "Epoch 1161/2000  Iteration 2321/4000 Training loss: 0.0273 0.4435 sec/batch\n",
      "Epoch 1161/2000  Iteration 2322/4000 Training loss: 0.0153 0.4436 sec/batch\n",
      "Epoch 1162/2000  Iteration 2323/4000 Training loss: 0.0270 0.4433 sec/batch\n",
      "Epoch 1162/2000  Iteration 2324/4000 Training loss: 0.0153 0.4437 sec/batch\n",
      "Epoch 1163/2000  Iteration 2325/4000 Training loss: 0.0278 0.4434 sec/batch\n",
      "Epoch 1163/2000  Iteration 2326/4000 Training loss: 0.0156 0.4433 sec/batch\n",
      "Epoch 1164/2000  Iteration 2327/4000 Training loss: 0.0264 0.4435 sec/batch\n",
      "Epoch 1164/2000  Iteration 2328/4000 Training loss: 0.0153 0.4434 sec/batch\n",
      "Epoch 1165/2000  Iteration 2329/4000 Training loss: 0.0266 0.4437 sec/batch\n",
      "Epoch 1165/2000  Iteration 2330/4000 Training loss: 0.0152 0.4440 sec/batch\n",
      "Epoch 1166/2000  Iteration 2331/4000 Training loss: 0.0262 0.4437 sec/batch\n",
      "Epoch 1166/2000  Iteration 2332/4000 Training loss: 0.0146 0.4439 sec/batch\n",
      "Epoch 1167/2000  Iteration 2333/4000 Training loss: 0.0277 0.4429 sec/batch\n",
      "Epoch 1167/2000  Iteration 2334/4000 Training loss: 0.0162 0.4434 sec/batch\n",
      "Epoch 1168/2000  Iteration 2335/4000 Training loss: 0.0265 0.4431 sec/batch\n",
      "Epoch 1168/2000  Iteration 2336/4000 Training loss: 0.0152 0.4435 sec/batch\n",
      "Epoch 1169/2000  Iteration 2337/4000 Training loss: 0.0265 0.4428 sec/batch\n",
      "Epoch 1169/2000  Iteration 2338/4000 Training loss: 0.0148 0.4437 sec/batch\n",
      "Epoch 1170/2000  Iteration 2339/4000 Training loss: 0.0274 0.4437 sec/batch\n",
      "Epoch 1170/2000  Iteration 2340/4000 Training loss: 0.0155 0.4437 sec/batch\n",
      "Epoch 1171/2000  Iteration 2341/4000 Training loss: 0.0270 0.4436 sec/batch\n",
      "Epoch 1171/2000  Iteration 2342/4000 Training loss: 0.0149 0.4439 sec/batch\n",
      "Epoch 1172/2000  Iteration 2343/4000 Training loss: 0.0278 0.4439 sec/batch\n",
      "Epoch 1172/2000  Iteration 2344/4000 Training loss: 0.0153 0.4438 sec/batch\n",
      "Epoch 1173/2000  Iteration 2345/4000 Training loss: 0.0265 0.4435 sec/batch\n",
      "Epoch 1173/2000  Iteration 2346/4000 Training loss: 0.0149 0.4434 sec/batch\n",
      "Epoch 1174/2000  Iteration 2347/4000 Training loss: 0.0264 0.4435 sec/batch\n",
      "Epoch 1174/2000  Iteration 2348/4000 Training loss: 0.0147 0.4450 sec/batch\n",
      "Epoch 1175/2000  Iteration 2349/4000 Training loss: 0.0260 0.4441 sec/batch\n",
      "Epoch 1175/2000  Iteration 2350/4000 Training loss: 0.0146 0.4436 sec/batch\n",
      "Epoch 1176/2000  Iteration 2351/4000 Training loss: 0.0268 0.4434 sec/batch\n",
      "Epoch 1176/2000  Iteration 2352/4000 Training loss: 0.0148 0.4434 sec/batch\n",
      "Epoch 1177/2000  Iteration 2353/4000 Training loss: 0.0267 0.4432 sec/batch\n",
      "Epoch 1177/2000  Iteration 2354/4000 Training loss: 0.0148 0.4439 sec/batch\n",
      "Epoch 1178/2000  Iteration 2355/4000 Training loss: 0.0269 0.4431 sec/batch\n",
      "Epoch 1178/2000  Iteration 2356/4000 Training loss: 0.0149 0.4432 sec/batch\n",
      "Epoch 1179/2000  Iteration 2357/4000 Training loss: 0.0265 0.4436 sec/batch\n",
      "Epoch 1179/2000  Iteration 2358/4000 Training loss: 0.0150 0.4436 sec/batch\n",
      "Epoch 1180/2000  Iteration 2359/4000 Training loss: 0.0271 0.4436 sec/batch\n",
      "Epoch 1180/2000  Iteration 2360/4000 Training loss: 0.0152 0.4437 sec/batch\n",
      "Epoch 1181/2000  Iteration 2361/4000 Training loss: 0.0257 0.4432 sec/batch\n",
      "Epoch 1181/2000  Iteration 2362/4000 Training loss: 0.0144 0.4439 sec/batch\n",
      "Epoch 1182/2000  Iteration 2363/4000 Training loss: 0.0274 0.4435 sec/batch\n",
      "Epoch 1182/2000  Iteration 2364/4000 Training loss: 0.0151 0.4438 sec/batch\n",
      "Epoch 1183/2000  Iteration 2365/4000 Training loss: 0.0258 0.4437 sec/batch\n",
      "Epoch 1183/2000  Iteration 2366/4000 Training loss: 0.0140 0.4436 sec/batch\n",
      "Epoch 1184/2000  Iteration 2367/4000 Training loss: 0.0261 0.4429 sec/batch\n",
      "Epoch 1184/2000  Iteration 2368/4000 Training loss: 0.0146 0.4439 sec/batch\n",
      "Epoch 1185/2000  Iteration 2369/4000 Training loss: 0.0263 0.4432 sec/batch\n",
      "Epoch 1185/2000  Iteration 2370/4000 Training loss: 0.0146 0.4435 sec/batch\n",
      "Epoch 1186/2000  Iteration 2371/4000 Training loss: 0.0254 0.4433 sec/batch\n",
      "Epoch 1186/2000  Iteration 2372/4000 Training loss: 0.0141 0.4436 sec/batch\n",
      "Epoch 1187/2000  Iteration 2373/4000 Training loss: 0.0256 0.4434 sec/batch\n",
      "Epoch 1187/2000  Iteration 2374/4000 Training loss: 0.0143 0.4434 sec/batch\n",
      "Epoch 1188/2000  Iteration 2375/4000 Training loss: 0.0269 0.4436 sec/batch\n",
      "Epoch 1188/2000  Iteration 2376/4000 Training loss: 0.0147 0.4435 sec/batch\n",
      "Epoch 1189/2000  Iteration 2377/4000 Training loss: 0.0267 0.4430 sec/batch\n",
      "Epoch 1189/2000  Iteration 2378/4000 Training loss: 0.0148 0.4437 sec/batch\n",
      "Epoch 1190/2000  Iteration 2379/4000 Training loss: 0.0273 0.4434 sec/batch\n",
      "Epoch 1190/2000  Iteration 2380/4000 Training loss: 0.0151 0.4433 sec/batch\n",
      "Epoch 1191/2000  Iteration 2381/4000 Training loss: 0.0265 0.4434 sec/batch\n",
      "Epoch 1191/2000  Iteration 2382/4000 Training loss: 0.0152 0.4431 sec/batch\n",
      "Epoch 1192/2000  Iteration 2383/4000 Training loss: 0.0267 0.4433 sec/batch\n",
      "Epoch 1192/2000  Iteration 2384/4000 Training loss: 0.0150 0.4431 sec/batch\n",
      "Epoch 1193/2000  Iteration 2385/4000 Training loss: 0.0251 0.4433 sec/batch\n",
      "Epoch 1193/2000  Iteration 2386/4000 Training loss: 0.0145 0.4433 sec/batch\n",
      "Epoch 1194/2000  Iteration 2387/4000 Training loss: 0.0249 0.4430 sec/batch\n",
      "Epoch 1194/2000  Iteration 2388/4000 Training loss: 0.0143 0.4442 sec/batch\n",
      "Epoch 1195/2000  Iteration 2389/4000 Training loss: 0.0266 0.4441 sec/batch\n",
      "Epoch 1195/2000  Iteration 2390/4000 Training loss: 0.0150 0.4442 sec/batch\n",
      "Epoch 1196/2000  Iteration 2391/4000 Training loss: 0.0269 0.4443 sec/batch\n",
      "Epoch 1196/2000  Iteration 2392/4000 Training loss: 0.0148 0.4441 sec/batch\n",
      "Epoch 1197/2000  Iteration 2393/4000 Training loss: 0.0268 0.4434 sec/batch\n",
      "Epoch 1197/2000  Iteration 2394/4000 Training loss: 0.0155 0.4437 sec/batch\n",
      "Epoch 1198/2000  Iteration 2395/4000 Training loss: 0.0259 0.4435 sec/batch\n",
      "Epoch 1198/2000  Iteration 2396/4000 Training loss: 0.0142 0.4437 sec/batch\n",
      "Epoch 1199/2000  Iteration 2397/4000 Training loss: 0.0255 0.4432 sec/batch\n",
      "Epoch 1199/2000  Iteration 2398/4000 Training loss: 0.0145 0.4435 sec/batch\n",
      "Epoch 1200/2000  Iteration 2399/4000 Training loss: 0.0265 0.4434 sec/batch\n",
      "Epoch 1200/2000  Iteration 2400/4000 Training loss: 0.0149 0.4436 sec/batch\n",
      "Validation loss: 6.4563 Saving checkpoint!\n",
      "Epoch 1201/2000  Iteration 2401/4000 Training loss: 0.0257 0.4436 sec/batch\n",
      "Epoch 1201/2000  Iteration 2402/4000 Training loss: 0.0145 0.4435 sec/batch\n",
      "Epoch 1202/2000  Iteration 2403/4000 Training loss: 0.0265 0.4436 sec/batch\n",
      "Epoch 1202/2000  Iteration 2404/4000 Training loss: 0.0154 0.4437 sec/batch\n",
      "Epoch 1203/2000  Iteration 2405/4000 Training loss: 0.0266 0.4430 sec/batch\n",
      "Epoch 1203/2000  Iteration 2406/4000 Training loss: 0.0151 0.4435 sec/batch\n",
      "Epoch 1204/2000  Iteration 2407/4000 Training loss: 0.0257 0.4432 sec/batch\n",
      "Epoch 1204/2000  Iteration 2408/4000 Training loss: 0.0145 0.4429 sec/batch\n",
      "Epoch 1205/2000  Iteration 2409/4000 Training loss: 0.0269 0.4431 sec/batch\n",
      "Epoch 1205/2000  Iteration 2410/4000 Training loss: 0.0154 0.4432 sec/batch\n",
      "Epoch 1206/2000  Iteration 2411/4000 Training loss: 0.0262 0.4432 sec/batch\n",
      "Epoch 1206/2000  Iteration 2412/4000 Training loss: 0.0148 0.4433 sec/batch\n",
      "Epoch 1207/2000  Iteration 2413/4000 Training loss: 0.0260 0.4436 sec/batch\n",
      "Epoch 1207/2000  Iteration 2414/4000 Training loss: 0.0148 0.4437 sec/batch\n",
      "Epoch 1208/2000  Iteration 2415/4000 Training loss: 0.0254 0.4431 sec/batch\n",
      "Epoch 1208/2000  Iteration 2416/4000 Training loss: 0.0146 0.4433 sec/batch\n",
      "Epoch 1209/2000  Iteration 2417/4000 Training loss: 0.0270 0.4429 sec/batch\n",
      "Epoch 1209/2000  Iteration 2418/4000 Training loss: 0.0150 0.4430 sec/batch\n",
      "Epoch 1210/2000  Iteration 2419/4000 Training loss: 0.0265 0.4431 sec/batch\n",
      "Epoch 1210/2000  Iteration 2420/4000 Training loss: 0.0146 0.4433 sec/batch\n",
      "Epoch 1211/2000  Iteration 2421/4000 Training loss: 0.0260 0.4432 sec/batch\n",
      "Epoch 1211/2000  Iteration 2422/4000 Training loss: 0.0148 0.4436 sec/batch\n",
      "Epoch 1212/2000  Iteration 2423/4000 Training loss: 0.0267 0.4432 sec/batch\n",
      "Epoch 1212/2000  Iteration 2424/4000 Training loss: 0.0149 0.4435 sec/batch\n",
      "Epoch 1213/2000  Iteration 2425/4000 Training loss: 0.0253 0.4431 sec/batch\n",
      "Epoch 1213/2000  Iteration 2426/4000 Training loss: 0.0140 0.4434 sec/batch\n",
      "Epoch 1214/2000  Iteration 2427/4000 Training loss: 0.0267 0.4430 sec/batch\n",
      "Epoch 1214/2000  Iteration 2428/4000 Training loss: 0.0145 0.4436 sec/batch\n",
      "Epoch 1215/2000  Iteration 2429/4000 Training loss: 0.0249 0.4432 sec/batch\n",
      "Epoch 1215/2000  Iteration 2430/4000 Training loss: 0.0141 0.4435 sec/batch\n",
      "Epoch 1216/2000  Iteration 2431/4000 Training loss: 0.0263 0.4431 sec/batch\n",
      "Epoch 1216/2000  Iteration 2432/4000 Training loss: 0.0146 0.4434 sec/batch\n",
      "Epoch 1217/2000  Iteration 2433/4000 Training loss: 0.0251 0.4434 sec/batch\n",
      "Epoch 1217/2000  Iteration 2434/4000 Training loss: 0.0142 0.4435 sec/batch\n",
      "Epoch 1218/2000  Iteration 2435/4000 Training loss: 0.0251 0.4434 sec/batch\n",
      "Epoch 1218/2000  Iteration 2436/4000 Training loss: 0.0143 0.4435 sec/batch\n",
      "Epoch 1219/2000  Iteration 2437/4000 Training loss: 0.0260 0.4431 sec/batch\n",
      "Epoch 1219/2000  Iteration 2438/4000 Training loss: 0.0144 0.4438 sec/batch\n",
      "Epoch 1220/2000  Iteration 2439/4000 Training loss: 0.0244 0.4430 sec/batch\n",
      "Epoch 1220/2000  Iteration 2440/4000 Training loss: 0.0137 0.4432 sec/batch\n",
      "Epoch 1221/2000  Iteration 2441/4000 Training loss: 0.0250 0.4434 sec/batch\n",
      "Epoch 1221/2000  Iteration 2442/4000 Training loss: 0.0138 0.4433 sec/batch\n",
      "Epoch 1222/2000  Iteration 2443/4000 Training loss: 0.0262 0.4432 sec/batch\n",
      "Epoch 1222/2000  Iteration 2444/4000 Training loss: 0.0146 0.4443 sec/batch\n",
      "Epoch 1223/2000  Iteration 2445/4000 Training loss: 0.0269 0.4438 sec/batch\n",
      "Epoch 1223/2000  Iteration 2446/4000 Training loss: 0.0150 0.4434 sec/batch\n",
      "Epoch 1224/2000  Iteration 2447/4000 Training loss: 0.0263 0.4432 sec/batch\n",
      "Epoch 1224/2000  Iteration 2448/4000 Training loss: 0.0146 0.4431 sec/batch\n",
      "Epoch 1225/2000  Iteration 2449/4000 Training loss: 0.0261 0.4430 sec/batch\n",
      "Epoch 1225/2000  Iteration 2450/4000 Training loss: 0.0148 0.4439 sec/batch\n",
      "Epoch 1226/2000  Iteration 2451/4000 Training loss: 0.0257 0.4433 sec/batch\n",
      "Epoch 1226/2000  Iteration 2452/4000 Training loss: 0.0143 0.4433 sec/batch\n",
      "Epoch 1227/2000  Iteration 2453/4000 Training loss: 0.0266 0.4432 sec/batch\n",
      "Epoch 1227/2000  Iteration 2454/4000 Training loss: 0.0148 0.4435 sec/batch\n",
      "Epoch 1228/2000  Iteration 2455/4000 Training loss: 0.0257 0.4433 sec/batch\n",
      "Epoch 1228/2000  Iteration 2456/4000 Training loss: 0.0146 0.4436 sec/batch\n",
      "Epoch 1229/2000  Iteration 2457/4000 Training loss: 0.0259 0.4434 sec/batch\n",
      "Epoch 1229/2000  Iteration 2458/4000 Training loss: 0.0141 0.4433 sec/batch\n",
      "Epoch 1230/2000  Iteration 2459/4000 Training loss: 0.0261 0.4431 sec/batch\n",
      "Epoch 1230/2000  Iteration 2460/4000 Training loss: 0.0145 0.4434 sec/batch\n",
      "Epoch 1231/2000  Iteration 2461/4000 Training loss: 0.0283 0.4437 sec/batch\n",
      "Epoch 1231/2000  Iteration 2462/4000 Training loss: 0.0158 0.4436 sec/batch\n",
      "Epoch 1232/2000  Iteration 2463/4000 Training loss: 0.0251 0.4433 sec/batch\n",
      "Epoch 1232/2000  Iteration 2464/4000 Training loss: 0.0145 0.4433 sec/batch\n",
      "Epoch 1233/2000  Iteration 2465/4000 Training loss: 0.0261 0.4433 sec/batch\n",
      "Epoch 1233/2000  Iteration 2466/4000 Training loss: 0.0151 0.4430 sec/batch\n",
      "Epoch 1234/2000  Iteration 2467/4000 Training loss: 0.0258 0.4433 sec/batch\n",
      "Epoch 1234/2000  Iteration 2468/4000 Training loss: 0.0145 0.4446 sec/batch\n",
      "Epoch 1235/2000  Iteration 2469/4000 Training loss: 0.0260 0.4437 sec/batch\n",
      "Epoch 1235/2000  Iteration 2470/4000 Training loss: 0.0154 0.4436 sec/batch\n",
      "Epoch 1236/2000  Iteration 2471/4000 Training loss: 0.0248 0.4442 sec/batch\n",
      "Epoch 1236/2000  Iteration 2472/4000 Training loss: 0.0144 0.4436 sec/batch\n",
      "Epoch 1237/2000  Iteration 2473/4000 Training loss: 0.0254 0.4431 sec/batch\n",
      "Epoch 1237/2000  Iteration 2474/4000 Training loss: 0.0155 0.4434 sec/batch\n",
      "Epoch 1238/2000  Iteration 2475/4000 Training loss: 0.0258 0.4431 sec/batch\n",
      "Epoch 1238/2000  Iteration 2476/4000 Training loss: 0.0151 0.4432 sec/batch\n",
      "Epoch 1239/2000  Iteration 2477/4000 Training loss: 0.0264 0.4434 sec/batch\n",
      "Epoch 1239/2000  Iteration 2478/4000 Training loss: 0.0156 0.4433 sec/batch\n",
      "Epoch 1240/2000  Iteration 2479/4000 Training loss: 0.0271 0.4434 sec/batch\n",
      "Epoch 1240/2000  Iteration 2480/4000 Training loss: 0.0159 0.4433 sec/batch\n",
      "Epoch 1241/2000  Iteration 2481/4000 Training loss: 0.0273 0.4435 sec/batch\n",
      "Epoch 1241/2000  Iteration 2482/4000 Training loss: 0.0157 0.4438 sec/batch\n",
      "Epoch 1242/2000  Iteration 2483/4000 Training loss: 0.0278 0.4431 sec/batch\n",
      "Epoch 1242/2000  Iteration 2484/4000 Training loss: 0.0161 0.4435 sec/batch\n",
      "Epoch 1243/2000  Iteration 2485/4000 Training loss: 0.0273 0.4434 sec/batch\n",
      "Epoch 1243/2000  Iteration 2486/4000 Training loss: 0.0160 0.4441 sec/batch\n",
      "Epoch 1244/2000  Iteration 2487/4000 Training loss: 0.0258 0.4438 sec/batch\n",
      "Epoch 1244/2000  Iteration 2488/4000 Training loss: 0.0154 0.4440 sec/batch\n",
      "Epoch 1245/2000  Iteration 2489/4000 Training loss: 0.0260 0.4437 sec/batch\n",
      "Epoch 1245/2000  Iteration 2490/4000 Training loss: 0.0152 0.4437 sec/batch\n",
      "Epoch 1246/2000  Iteration 2491/4000 Training loss: 0.0274 0.4433 sec/batch\n",
      "Epoch 1246/2000  Iteration 2492/4000 Training loss: 0.0162 0.4447 sec/batch\n",
      "Epoch 1247/2000  Iteration 2493/4000 Training loss: 0.0265 0.4434 sec/batch\n",
      "Epoch 1247/2000  Iteration 2494/4000 Training loss: 0.0161 0.4436 sec/batch\n",
      "Epoch 1248/2000  Iteration 2495/4000 Training loss: 0.0283 0.4431 sec/batch\n",
      "Epoch 1248/2000  Iteration 2496/4000 Training loss: 0.0163 0.4439 sec/batch\n",
      "Epoch 1249/2000  Iteration 2497/4000 Training loss: 0.0267 0.4431 sec/batch\n",
      "Epoch 1249/2000  Iteration 2498/4000 Training loss: 0.0155 0.4433 sec/batch\n",
      "Epoch 1250/2000  Iteration 2499/4000 Training loss: 0.0261 0.4432 sec/batch\n",
      "Epoch 1250/2000  Iteration 2500/4000 Training loss: 0.0155 0.4436 sec/batch\n",
      "Epoch 1251/2000  Iteration 2501/4000 Training loss: 0.0280 0.4434 sec/batch\n",
      "Epoch 1251/2000  Iteration 2502/4000 Training loss: 0.0160 0.4432 sec/batch\n",
      "Epoch 1252/2000  Iteration 2503/4000 Training loss: 0.0255 0.4430 sec/batch\n",
      "Epoch 1252/2000  Iteration 2504/4000 Training loss: 0.0149 0.4434 sec/batch\n",
      "Epoch 1253/2000  Iteration 2505/4000 Training loss: 0.0268 0.4430 sec/batch\n",
      "Epoch 1253/2000  Iteration 2506/4000 Training loss: 0.0160 0.4432 sec/batch\n",
      "Epoch 1254/2000  Iteration 2507/4000 Training loss: 0.0259 0.4431 sec/batch\n",
      "Epoch 1254/2000  Iteration 2508/4000 Training loss: 0.0154 0.4434 sec/batch\n",
      "Epoch 1255/2000  Iteration 2509/4000 Training loss: 0.0267 0.4434 sec/batch\n",
      "Epoch 1255/2000  Iteration 2510/4000 Training loss: 0.0161 0.4439 sec/batch\n",
      "Epoch 1256/2000  Iteration 2511/4000 Training loss: 0.0279 0.4432 sec/batch\n",
      "Epoch 1256/2000  Iteration 2512/4000 Training loss: 0.0163 0.4438 sec/batch\n",
      "Epoch 1257/2000  Iteration 2513/4000 Training loss: 0.0263 0.4431 sec/batch\n",
      "Epoch 1257/2000  Iteration 2514/4000 Training loss: 0.0156 0.4434 sec/batch\n",
      "Epoch 1258/2000  Iteration 2515/4000 Training loss: 0.0273 0.4436 sec/batch\n",
      "Epoch 1258/2000  Iteration 2516/4000 Training loss: 0.0170 0.4436 sec/batch\n",
      "Epoch 1259/2000  Iteration 2517/4000 Training loss: 0.0278 0.4428 sec/batch\n",
      "Epoch 1259/2000  Iteration 2518/4000 Training loss: 0.0166 0.4437 sec/batch\n",
      "Epoch 1260/2000  Iteration 2519/4000 Training loss: 0.0292 0.4429 sec/batch\n",
      "Epoch 1260/2000  Iteration 2520/4000 Training loss: 0.0170 0.4435 sec/batch\n",
      "Epoch 1261/2000  Iteration 2521/4000 Training loss: 0.0266 0.4428 sec/batch\n",
      "Epoch 1261/2000  Iteration 2522/4000 Training loss: 0.0160 0.4442 sec/batch\n",
      "Epoch 1262/2000  Iteration 2523/4000 Training loss: 0.0268 0.4435 sec/batch\n",
      "Epoch 1262/2000  Iteration 2524/4000 Training loss: 0.0165 0.4436 sec/batch\n",
      "Epoch 1263/2000  Iteration 2525/4000 Training loss: 0.0259 0.4444 sec/batch\n",
      "Epoch 1263/2000  Iteration 2526/4000 Training loss: 0.0155 0.4437 sec/batch\n",
      "Epoch 1264/2000  Iteration 2527/4000 Training loss: 0.0275 0.4433 sec/batch\n",
      "Epoch 1264/2000  Iteration 2528/4000 Training loss: 0.0169 0.4437 sec/batch\n",
      "Epoch 1265/2000  Iteration 2529/4000 Training loss: 0.0273 0.4434 sec/batch\n",
      "Epoch 1265/2000  Iteration 2530/4000 Training loss: 0.0170 0.4435 sec/batch\n",
      "Epoch 1266/2000  Iteration 2531/4000 Training loss: 0.0256 0.4431 sec/batch\n",
      "Epoch 1266/2000  Iteration 2532/4000 Training loss: 0.0158 0.4436 sec/batch\n",
      "Epoch 1267/2000  Iteration 2533/4000 Training loss: 0.0264 0.4432 sec/batch\n",
      "Epoch 1267/2000  Iteration 2534/4000 Training loss: 0.0160 0.4438 sec/batch\n",
      "Epoch 1268/2000  Iteration 2535/4000 Training loss: 0.0257 0.4433 sec/batch\n",
      "Epoch 1268/2000  Iteration 2536/4000 Training loss: 0.0152 0.4438 sec/batch\n",
      "Epoch 1269/2000  Iteration 2537/4000 Training loss: 0.0280 0.4436 sec/batch\n",
      "Epoch 1269/2000  Iteration 2538/4000 Training loss: 0.0164 0.4439 sec/batch\n",
      "Epoch 1270/2000  Iteration 2539/4000 Training loss: 0.0262 0.4436 sec/batch\n",
      "Epoch 1270/2000  Iteration 2540/4000 Training loss: 0.0162 0.4434 sec/batch\n",
      "Epoch 1271/2000  Iteration 2541/4000 Training loss: 0.0270 0.4435 sec/batch\n",
      "Epoch 1271/2000  Iteration 2542/4000 Training loss: 0.0158 0.4436 sec/batch\n",
      "Epoch 1272/2000  Iteration 2543/4000 Training loss: 0.0261 0.4431 sec/batch\n",
      "Epoch 1272/2000  Iteration 2544/4000 Training loss: 0.0161 0.4438 sec/batch\n",
      "Epoch 1273/2000  Iteration 2545/4000 Training loss: 0.0268 0.4428 sec/batch\n",
      "Epoch 1273/2000  Iteration 2546/4000 Training loss: 0.0161 0.4434 sec/batch\n",
      "Epoch 1274/2000  Iteration 2547/4000 Training loss: 0.0272 0.4438 sec/batch\n",
      "Epoch 1274/2000  Iteration 2548/4000 Training loss: 0.0163 0.4440 sec/batch\n",
      "Epoch 1275/2000  Iteration 2549/4000 Training loss: 0.0285 0.4430 sec/batch\n",
      "Epoch 1275/2000  Iteration 2550/4000 Training loss: 0.0177 0.4437 sec/batch\n",
      "Epoch 1276/2000  Iteration 2551/4000 Training loss: 0.0272 0.4436 sec/batch\n",
      "Epoch 1276/2000  Iteration 2552/4000 Training loss: 0.0173 0.4434 sec/batch\n",
      "Epoch 1277/2000  Iteration 2553/4000 Training loss: 0.0274 0.4433 sec/batch\n",
      "Epoch 1277/2000  Iteration 2554/4000 Training loss: 0.0165 0.4434 sec/batch\n",
      "Epoch 1278/2000  Iteration 2555/4000 Training loss: 0.0266 0.4432 sec/batch\n",
      "Epoch 1278/2000  Iteration 2556/4000 Training loss: 0.0161 0.4440 sec/batch\n",
      "Epoch 1279/2000  Iteration 2557/4000 Training loss: 0.0270 0.4437 sec/batch\n",
      "Epoch 1279/2000  Iteration 2558/4000 Training loss: 0.0164 0.4433 sec/batch\n",
      "Epoch 1280/2000  Iteration 2559/4000 Training loss: 0.0279 0.4433 sec/batch\n",
      "Epoch 1280/2000  Iteration 2560/4000 Training loss: 0.0172 0.4433 sec/batch\n",
      "Epoch 1281/2000  Iteration 2561/4000 Training loss: 0.0274 0.4431 sec/batch\n",
      "Epoch 1281/2000  Iteration 2562/4000 Training loss: 0.0171 0.4435 sec/batch\n",
      "Epoch 1282/2000  Iteration 2563/4000 Training loss: 0.0272 0.4429 sec/batch\n",
      "Epoch 1282/2000  Iteration 2564/4000 Training loss: 0.0171 0.4430 sec/batch\n",
      "Epoch 1283/2000  Iteration 2565/4000 Training loss: 0.0282 0.4432 sec/batch\n",
      "Epoch 1283/2000  Iteration 2566/4000 Training loss: 0.0175 0.4434 sec/batch\n",
      "Epoch 1284/2000  Iteration 2567/4000 Training loss: 0.0267 0.4432 sec/batch\n",
      "Epoch 1284/2000  Iteration 2568/4000 Training loss: 0.0167 0.4429 sec/batch\n",
      "Epoch 1285/2000  Iteration 2569/4000 Training loss: 0.0274 0.4430 sec/batch\n",
      "Epoch 1285/2000  Iteration 2570/4000 Training loss: 0.0176 0.4435 sec/batch\n",
      "Epoch 1286/2000  Iteration 2571/4000 Training loss: 0.0284 0.4433 sec/batch\n",
      "Epoch 1286/2000  Iteration 2572/4000 Training loss: 0.0173 0.4435 sec/batch\n",
      "Epoch 1287/2000  Iteration 2573/4000 Training loss: 0.0274 0.4433 sec/batch\n",
      "Epoch 1287/2000  Iteration 2574/4000 Training loss: 0.0168 0.4434 sec/batch\n",
      "Epoch 1288/2000  Iteration 2575/4000 Training loss: 0.0275 0.4437 sec/batch\n",
      "Epoch 1288/2000  Iteration 2576/4000 Training loss: 0.0172 0.4438 sec/batch\n",
      "Epoch 1289/2000  Iteration 2577/4000 Training loss: 0.0264 0.4436 sec/batch\n",
      "Epoch 1289/2000  Iteration 2578/4000 Training loss: 0.0164 0.4440 sec/batch\n",
      "Epoch 1290/2000  Iteration 2579/4000 Training loss: 0.0279 0.4440 sec/batch\n",
      "Epoch 1290/2000  Iteration 2580/4000 Training loss: 0.0174 0.4435 sec/batch\n",
      "Epoch 1291/2000  Iteration 2581/4000 Training loss: 0.0275 0.4435 sec/batch\n",
      "Epoch 1291/2000  Iteration 2582/4000 Training loss: 0.0171 0.4437 sec/batch\n",
      "Epoch 1292/2000  Iteration 2583/4000 Training loss: 0.0272 0.4430 sec/batch\n",
      "Epoch 1292/2000  Iteration 2584/4000 Training loss: 0.0169 0.4438 sec/batch\n",
      "Epoch 1293/2000  Iteration 2585/4000 Training loss: 0.0277 0.4431 sec/batch\n",
      "Epoch 1293/2000  Iteration 2586/4000 Training loss: 0.0175 0.4434 sec/batch\n",
      "Epoch 1294/2000  Iteration 2587/4000 Training loss: 0.0277 0.4432 sec/batch\n",
      "Epoch 1294/2000  Iteration 2588/4000 Training loss: 0.0161 0.4432 sec/batch\n",
      "Epoch 1295/2000  Iteration 2589/4000 Training loss: 0.0260 0.4434 sec/batch\n",
      "Epoch 1295/2000  Iteration 2590/4000 Training loss: 0.0167 0.4436 sec/batch\n",
      "Epoch 1296/2000  Iteration 2591/4000 Training loss: 0.0286 0.4435 sec/batch\n",
      "Epoch 1296/2000  Iteration 2592/4000 Training loss: 0.0177 0.4435 sec/batch\n",
      "Epoch 1297/2000  Iteration 2593/4000 Training loss: 0.0274 0.4434 sec/batch\n",
      "Epoch 1297/2000  Iteration 2594/4000 Training loss: 0.0173 0.4437 sec/batch\n",
      "Epoch 1298/2000  Iteration 2595/4000 Training loss: 0.0262 0.4434 sec/batch\n",
      "Epoch 1298/2000  Iteration 2596/4000 Training loss: 0.0168 0.4435 sec/batch\n",
      "Epoch 1299/2000  Iteration 2597/4000 Training loss: 0.0283 0.4434 sec/batch\n",
      "Epoch 1299/2000  Iteration 2598/4000 Training loss: 0.0171 0.4432 sec/batch\n",
      "Epoch 1300/2000  Iteration 2599/4000 Training loss: 0.0278 0.4444 sec/batch\n",
      "Epoch 1300/2000  Iteration 2600/4000 Training loss: 0.0184 0.4440 sec/batch\n",
      "Validation loss: 6.39123 Saving checkpoint!\n",
      "Epoch 1301/2000  Iteration 2601/4000 Training loss: 0.0286 0.4434 sec/batch\n",
      "Epoch 1301/2000  Iteration 2602/4000 Training loss: 0.0172 0.4436 sec/batch\n",
      "Epoch 1302/2000  Iteration 2603/4000 Training loss: 0.0280 0.4436 sec/batch\n",
      "Epoch 1302/2000  Iteration 2604/4000 Training loss: 0.0176 0.4439 sec/batch\n",
      "Epoch 1303/2000  Iteration 2605/4000 Training loss: 0.0288 0.4433 sec/batch\n",
      "Epoch 1303/2000  Iteration 2606/4000 Training loss: 0.0179 0.4436 sec/batch\n",
      "Epoch 1304/2000  Iteration 2607/4000 Training loss: 0.0287 0.4433 sec/batch\n",
      "Epoch 1304/2000  Iteration 2608/4000 Training loss: 0.0179 0.4438 sec/batch\n",
      "Epoch 1305/2000  Iteration 2609/4000 Training loss: 0.0280 0.4438 sec/batch\n",
      "Epoch 1305/2000  Iteration 2610/4000 Training loss: 0.0174 0.4444 sec/batch\n",
      "Epoch 1306/2000  Iteration 2611/4000 Training loss: 0.0277 0.4433 sec/batch\n",
      "Epoch 1306/2000  Iteration 2612/4000 Training loss: 0.0175 0.4435 sec/batch\n",
      "Epoch 1307/2000  Iteration 2613/4000 Training loss: 0.0288 0.4437 sec/batch\n",
      "Epoch 1307/2000  Iteration 2614/4000 Training loss: 0.0179 0.4440 sec/batch\n",
      "Epoch 1308/2000  Iteration 2615/4000 Training loss: 0.0264 0.4435 sec/batch\n",
      "Epoch 1308/2000  Iteration 2616/4000 Training loss: 0.0170 0.4435 sec/batch\n",
      "Epoch 1309/2000  Iteration 2617/4000 Training loss: 0.0276 0.4432 sec/batch\n",
      "Epoch 1309/2000  Iteration 2618/4000 Training loss: 0.0165 0.4433 sec/batch\n",
      "Epoch 1310/2000  Iteration 2619/4000 Training loss: 0.0279 0.4431 sec/batch\n",
      "Epoch 1310/2000  Iteration 2620/4000 Training loss: 0.0175 0.4432 sec/batch\n",
      "Epoch 1311/2000  Iteration 2621/4000 Training loss: 0.0281 0.4430 sec/batch\n",
      "Epoch 1311/2000  Iteration 2622/4000 Training loss: 0.0173 0.4433 sec/batch\n",
      "Epoch 1312/2000  Iteration 2623/4000 Training loss: 0.0260 0.4432 sec/batch\n",
      "Epoch 1312/2000  Iteration 2624/4000 Training loss: 0.0161 0.4433 sec/batch\n",
      "Epoch 1313/2000  Iteration 2625/4000 Training loss: 0.0285 0.4428 sec/batch\n",
      "Epoch 1313/2000  Iteration 2626/4000 Training loss: 0.0173 0.4432 sec/batch\n",
      "Epoch 1314/2000  Iteration 2627/4000 Training loss: 0.0272 0.4434 sec/batch\n",
      "Epoch 1314/2000  Iteration 2628/4000 Training loss: 0.0166 0.4431 sec/batch\n",
      "Epoch 1315/2000  Iteration 2629/4000 Training loss: 0.0284 0.4430 sec/batch\n",
      "Epoch 1315/2000  Iteration 2630/4000 Training loss: 0.0174 0.4430 sec/batch\n",
      "Epoch 1316/2000  Iteration 2631/4000 Training loss: 0.0281 0.4429 sec/batch\n",
      "Epoch 1316/2000  Iteration 2632/4000 Training loss: 0.0169 0.4427 sec/batch\n",
      "Epoch 1317/2000  Iteration 2633/4000 Training loss: 0.0282 0.4433 sec/batch\n",
      "Epoch 1317/2000  Iteration 2634/4000 Training loss: 0.0163 0.4434 sec/batch\n",
      "Epoch 1318/2000  Iteration 2635/4000 Training loss: 0.0271 0.4433 sec/batch\n",
      "Epoch 1318/2000  Iteration 2636/4000 Training loss: 0.0165 0.4432 sec/batch\n",
      "Epoch 1319/2000  Iteration 2637/4000 Training loss: 0.0279 0.4434 sec/batch\n",
      "Epoch 1319/2000  Iteration 2638/4000 Training loss: 0.0166 0.4444 sec/batch\n",
      "Epoch 1320/2000  Iteration 2639/4000 Training loss: 0.0267 0.4433 sec/batch\n",
      "Epoch 1320/2000  Iteration 2640/4000 Training loss: 0.0161 0.4431 sec/batch\n",
      "Epoch 1321/2000  Iteration 2641/4000 Training loss: 0.0293 0.4432 sec/batch\n",
      "Epoch 1321/2000  Iteration 2642/4000 Training loss: 0.0182 0.4438 sec/batch\n",
      "Epoch 1322/2000  Iteration 2643/4000 Training loss: 0.0269 0.4433 sec/batch\n",
      "Epoch 1322/2000  Iteration 2644/4000 Training loss: 0.0168 0.4442 sec/batch\n",
      "Epoch 1323/2000  Iteration 2645/4000 Training loss: 0.0266 0.4431 sec/batch\n",
      "Epoch 1323/2000  Iteration 2646/4000 Training loss: 0.0160 0.4433 sec/batch\n",
      "Epoch 1324/2000  Iteration 2647/4000 Training loss: 0.0272 0.4439 sec/batch\n",
      "Epoch 1324/2000  Iteration 2648/4000 Training loss: 0.0167 0.4432 sec/batch\n",
      "Epoch 1325/2000  Iteration 2649/4000 Training loss: 0.0269 0.4432 sec/batch\n",
      "Epoch 1325/2000  Iteration 2650/4000 Training loss: 0.0161 0.4435 sec/batch\n",
      "Epoch 1326/2000  Iteration 2651/4000 Training loss: 0.0277 0.4431 sec/batch\n",
      "Epoch 1326/2000  Iteration 2652/4000 Training loss: 0.0164 0.4433 sec/batch\n",
      "Epoch 1327/2000  Iteration 2653/4000 Training loss: 0.0276 0.4428 sec/batch\n",
      "Epoch 1327/2000  Iteration 2654/4000 Training loss: 0.0163 0.4434 sec/batch\n",
      "Epoch 1328/2000  Iteration 2655/4000 Training loss: 0.0270 0.4433 sec/batch\n",
      "Epoch 1328/2000  Iteration 2656/4000 Training loss: 0.0164 0.4439 sec/batch\n",
      "Epoch 1329/2000  Iteration 2657/4000 Training loss: 0.0260 0.4428 sec/batch\n",
      "Epoch 1329/2000  Iteration 2658/4000 Training loss: 0.0160 0.4436 sec/batch\n",
      "Epoch 1330/2000  Iteration 2659/4000 Training loss: 0.0281 0.4434 sec/batch\n",
      "Epoch 1330/2000  Iteration 2660/4000 Training loss: 0.0171 0.4434 sec/batch\n",
      "Epoch 1331/2000  Iteration 2661/4000 Training loss: 0.0255 0.4432 sec/batch\n",
      "Epoch 1331/2000  Iteration 2662/4000 Training loss: 0.0156 0.4436 sec/batch\n",
      "Epoch 1332/2000  Iteration 2663/4000 Training loss: 0.0260 0.4433 sec/batch\n",
      "Epoch 1332/2000  Iteration 2664/4000 Training loss: 0.0164 0.4436 sec/batch\n",
      "Epoch 1333/2000  Iteration 2665/4000 Training loss: 0.0264 0.4435 sec/batch\n",
      "Epoch 1333/2000  Iteration 2666/4000 Training loss: 0.0158 0.4433 sec/batch\n",
      "Epoch 1334/2000  Iteration 2667/4000 Training loss: 0.0279 0.4437 sec/batch\n",
      "Epoch 1334/2000  Iteration 2668/4000 Training loss: 0.0167 0.4443 sec/batch\n",
      "Epoch 1335/2000  Iteration 2669/4000 Training loss: 0.0277 0.4436 sec/batch\n",
      "Epoch 1335/2000  Iteration 2670/4000 Training loss: 0.0166 0.4437 sec/batch\n",
      "Epoch 1336/2000  Iteration 2671/4000 Training loss: 0.0272 0.4431 sec/batch\n",
      "Epoch 1336/2000  Iteration 2672/4000 Training loss: 0.0167 0.4434 sec/batch\n",
      "Epoch 1337/2000  Iteration 2673/4000 Training loss: 0.0271 0.4444 sec/batch\n",
      "Epoch 1337/2000  Iteration 2674/4000 Training loss: 0.0166 0.4441 sec/batch\n",
      "Epoch 1338/2000  Iteration 2675/4000 Training loss: 0.0261 0.4434 sec/batch\n",
      "Epoch 1338/2000  Iteration 2676/4000 Training loss: 0.0156 0.4437 sec/batch\n",
      "Epoch 1339/2000  Iteration 2677/4000 Training loss: 0.0258 0.4434 sec/batch\n",
      "Epoch 1339/2000  Iteration 2678/4000 Training loss: 0.0155 0.4440 sec/batch\n",
      "Epoch 1340/2000  Iteration 2679/4000 Training loss: 0.0280 0.4430 sec/batch\n",
      "Epoch 1340/2000  Iteration 2680/4000 Training loss: 0.0164 0.4436 sec/batch\n",
      "Epoch 1341/2000  Iteration 2681/4000 Training loss: 0.0284 0.4435 sec/batch\n",
      "Epoch 1341/2000  Iteration 2682/4000 Training loss: 0.0169 0.4437 sec/batch\n",
      "Epoch 1342/2000  Iteration 2683/4000 Training loss: 0.0268 0.4436 sec/batch\n",
      "Epoch 1342/2000  Iteration 2684/4000 Training loss: 0.0162 0.4435 sec/batch\n",
      "Epoch 1343/2000  Iteration 2685/4000 Training loss: 0.0248 0.4435 sec/batch\n",
      "Epoch 1343/2000  Iteration 2686/4000 Training loss: 0.0148 0.4441 sec/batch\n",
      "Epoch 1344/2000  Iteration 2687/4000 Training loss: 0.0265 0.4433 sec/batch\n",
      "Epoch 1344/2000  Iteration 2688/4000 Training loss: 0.0154 0.4435 sec/batch\n",
      "Epoch 1345/2000  Iteration 2689/4000 Training loss: 0.0266 0.4435 sec/batch\n",
      "Epoch 1345/2000  Iteration 2690/4000 Training loss: 0.0156 0.4435 sec/batch\n",
      "Epoch 1346/2000  Iteration 2691/4000 Training loss: 0.0240 0.4431 sec/batch\n",
      "Epoch 1346/2000  Iteration 2692/4000 Training loss: 0.0145 0.4432 sec/batch\n",
      "Epoch 1347/2000  Iteration 2693/4000 Training loss: 0.0275 0.4429 sec/batch\n",
      "Epoch 1347/2000  Iteration 2694/4000 Training loss: 0.0161 0.4437 sec/batch\n",
      "Epoch 1348/2000  Iteration 2695/4000 Training loss: 0.0263 0.4431 sec/batch\n",
      "Epoch 1348/2000  Iteration 2696/4000 Training loss: 0.0160 0.4437 sec/batch\n",
      "Epoch 1349/2000  Iteration 2697/4000 Training loss: 0.0262 0.4434 sec/batch\n",
      "Epoch 1349/2000  Iteration 2698/4000 Training loss: 0.0159 0.4435 sec/batch\n",
      "Epoch 1350/2000  Iteration 2699/4000 Training loss: 0.0260 0.4433 sec/batch\n",
      "Epoch 1350/2000  Iteration 2700/4000 Training loss: 0.0156 0.4436 sec/batch\n",
      "Epoch 1351/2000  Iteration 2701/4000 Training loss: 0.0265 0.4433 sec/batch\n",
      "Epoch 1351/2000  Iteration 2702/4000 Training loss: 0.0157 0.4435 sec/batch\n",
      "Epoch 1352/2000  Iteration 2703/4000 Training loss: 0.0270 0.4431 sec/batch\n",
      "Epoch 1352/2000  Iteration 2704/4000 Training loss: 0.0159 0.4431 sec/batch\n",
      "Epoch 1353/2000  Iteration 2705/4000 Training loss: 0.0252 0.4432 sec/batch\n",
      "Epoch 1353/2000  Iteration 2706/4000 Training loss: 0.0148 0.4441 sec/batch\n",
      "Epoch 1354/2000  Iteration 2707/4000 Training loss: 0.0256 0.4432 sec/batch\n",
      "Epoch 1354/2000  Iteration 2708/4000 Training loss: 0.0154 0.4437 sec/batch\n",
      "Epoch 1355/2000  Iteration 2709/4000 Training loss: 0.0255 0.4435 sec/batch\n",
      "Epoch 1355/2000  Iteration 2710/4000 Training loss: 0.0147 0.4432 sec/batch\n",
      "Epoch 1356/2000  Iteration 2711/4000 Training loss: 0.0262 0.4432 sec/batch\n",
      "Epoch 1356/2000  Iteration 2712/4000 Training loss: 0.0154 0.4436 sec/batch\n",
      "Epoch 1357/2000  Iteration 2713/4000 Training loss: 0.0250 0.4429 sec/batch\n",
      "Epoch 1357/2000  Iteration 2714/4000 Training loss: 0.0143 0.4436 sec/batch\n",
      "Epoch 1358/2000  Iteration 2715/4000 Training loss: 0.0257 0.4434 sec/batch\n",
      "Epoch 1358/2000  Iteration 2716/4000 Training loss: 0.0151 0.4435 sec/batch\n",
      "Epoch 1359/2000  Iteration 2717/4000 Training loss: 0.0255 0.4431 sec/batch\n",
      "Epoch 1359/2000  Iteration 2718/4000 Training loss: 0.0143 0.4434 sec/batch\n",
      "Epoch 1360/2000  Iteration 2719/4000 Training loss: 0.0252 0.4434 sec/batch\n",
      "Epoch 1360/2000  Iteration 2720/4000 Training loss: 0.0144 0.4434 sec/batch\n",
      "Epoch 1361/2000  Iteration 2721/4000 Training loss: 0.0240 0.4433 sec/batch\n",
      "Epoch 1361/2000  Iteration 2722/4000 Training loss: 0.0135 0.4433 sec/batch\n",
      "Epoch 1362/2000  Iteration 2723/4000 Training loss: 0.0253 0.4434 sec/batch\n",
      "Epoch 1362/2000  Iteration 2724/4000 Training loss: 0.0146 0.4439 sec/batch\n",
      "Epoch 1363/2000  Iteration 2725/4000 Training loss: 0.0272 0.4433 sec/batch\n",
      "Epoch 1363/2000  Iteration 2726/4000 Training loss: 0.0154 0.4436 sec/batch\n",
      "Epoch 1364/2000  Iteration 2727/4000 Training loss: 0.0240 0.4439 sec/batch\n",
      "Epoch 1364/2000  Iteration 2728/4000 Training loss: 0.0137 0.4437 sec/batch\n",
      "Epoch 1365/2000  Iteration 2729/4000 Training loss: 0.0252 0.4434 sec/batch\n",
      "Epoch 1365/2000  Iteration 2730/4000 Training loss: 0.0150 0.4435 sec/batch\n",
      "Epoch 1366/2000  Iteration 2731/4000 Training loss: 0.0251 0.4429 sec/batch\n",
      "Epoch 1366/2000  Iteration 2732/4000 Training loss: 0.0144 0.4437 sec/batch\n",
      "Epoch 1367/2000  Iteration 2733/4000 Training loss: 0.0245 0.4435 sec/batch\n",
      "Epoch 1367/2000  Iteration 2734/4000 Training loss: 0.0139 0.4433 sec/batch\n",
      "Epoch 1368/2000  Iteration 2735/4000 Training loss: 0.0249 0.4432 sec/batch\n",
      "Epoch 1368/2000  Iteration 2736/4000 Training loss: 0.0145 0.4433 sec/batch\n",
      "Epoch 1369/2000  Iteration 2737/4000 Training loss: 0.0255 0.4431 sec/batch\n",
      "Epoch 1369/2000  Iteration 2738/4000 Training loss: 0.0143 0.4435 sec/batch\n",
      "Epoch 1370/2000  Iteration 2739/4000 Training loss: 0.0234 0.4433 sec/batch\n",
      "Epoch 1370/2000  Iteration 2740/4000 Training loss: 0.0132 0.4437 sec/batch\n",
      "Epoch 1371/2000  Iteration 2741/4000 Training loss: 0.0239 0.4430 sec/batch\n",
      "Epoch 1371/2000  Iteration 2742/4000 Training loss: 0.0134 0.4439 sec/batch\n",
      "Epoch 1372/2000  Iteration 2743/4000 Training loss: 0.0247 0.4433 sec/batch\n",
      "Epoch 1372/2000  Iteration 2744/4000 Training loss: 0.0139 0.4433 sec/batch\n",
      "Epoch 1373/2000  Iteration 2745/4000 Training loss: 0.0251 0.4434 sec/batch\n",
      "Epoch 1373/2000  Iteration 2746/4000 Training loss: 0.0143 0.4436 sec/batch\n",
      "Epoch 1374/2000  Iteration 2747/4000 Training loss: 0.0237 0.4430 sec/batch\n",
      "Epoch 1374/2000  Iteration 2748/4000 Training loss: 0.0130 0.4432 sec/batch\n",
      "Epoch 1375/2000  Iteration 2749/4000 Training loss: 0.0241 0.4429 sec/batch\n",
      "Epoch 1375/2000  Iteration 2750/4000 Training loss: 0.0138 0.4433 sec/batch\n",
      "Epoch 1376/2000  Iteration 2751/4000 Training loss: 0.0249 0.4432 sec/batch\n",
      "Epoch 1376/2000  Iteration 2752/4000 Training loss: 0.0140 0.4434 sec/batch\n",
      "Epoch 1377/2000  Iteration 2753/4000 Training loss: 0.0240 0.4431 sec/batch\n",
      "Epoch 1377/2000  Iteration 2754/4000 Training loss: 0.0133 0.4429 sec/batch\n",
      "Epoch 1378/2000  Iteration 2755/4000 Training loss: 0.0236 0.4436 sec/batch\n",
      "Epoch 1378/2000  Iteration 2756/4000 Training loss: 0.0130 0.4437 sec/batch\n",
      "Epoch 1379/2000  Iteration 2757/4000 Training loss: 0.0249 0.4437 sec/batch\n",
      "Epoch 1379/2000  Iteration 2758/4000 Training loss: 0.0138 0.4438 sec/batch\n",
      "Epoch 1380/2000  Iteration 2759/4000 Training loss: 0.0248 0.4430 sec/batch\n",
      "Epoch 1380/2000  Iteration 2760/4000 Training loss: 0.0137 0.4433 sec/batch\n",
      "Epoch 1381/2000  Iteration 2761/4000 Training loss: 0.0248 0.4430 sec/batch\n",
      "Epoch 1381/2000  Iteration 2762/4000 Training loss: 0.0140 0.4438 sec/batch\n",
      "Epoch 1382/2000  Iteration 2763/4000 Training loss: 0.0234 0.4433 sec/batch\n",
      "Epoch 1382/2000  Iteration 2764/4000 Training loss: 0.0127 0.4432 sec/batch\n",
      "Epoch 1383/2000  Iteration 2765/4000 Training loss: 0.0248 0.4433 sec/batch\n",
      "Epoch 1383/2000  Iteration 2766/4000 Training loss: 0.0135 0.4431 sec/batch\n",
      "Epoch 1384/2000  Iteration 2767/4000 Training loss: 0.0241 0.4433 sec/batch\n",
      "Epoch 1384/2000  Iteration 2768/4000 Training loss: 0.0139 0.4436 sec/batch\n",
      "Epoch 1385/2000  Iteration 2769/4000 Training loss: 0.0245 0.4439 sec/batch\n",
      "Epoch 1385/2000  Iteration 2770/4000 Training loss: 0.0138 0.4449 sec/batch\n",
      "Epoch 1386/2000  Iteration 2771/4000 Training loss: 0.0252 0.4443 sec/batch\n",
      "Epoch 1386/2000  Iteration 2772/4000 Training loss: 0.0141 0.4448 sec/batch\n",
      "Epoch 1387/2000  Iteration 2773/4000 Training loss: 0.0234 0.4439 sec/batch\n",
      "Epoch 1387/2000  Iteration 2774/4000 Training loss: 0.0130 0.4439 sec/batch\n",
      "Epoch 1388/2000  Iteration 2775/4000 Training loss: 0.0240 0.4435 sec/batch\n",
      "Epoch 1388/2000  Iteration 2776/4000 Training loss: 0.0133 0.4438 sec/batch\n",
      "Epoch 1389/2000  Iteration 2777/4000 Training loss: 0.0236 0.4438 sec/batch\n",
      "Epoch 1389/2000  Iteration 2778/4000 Training loss: 0.0131 0.4441 sec/batch\n",
      "Epoch 1390/2000  Iteration 2779/4000 Training loss: 0.0242 0.4438 sec/batch\n",
      "Epoch 1390/2000  Iteration 2780/4000 Training loss: 0.0137 0.4437 sec/batch\n",
      "Epoch 1391/2000  Iteration 2781/4000 Training loss: 0.0241 0.4437 sec/batch\n",
      "Epoch 1391/2000  Iteration 2782/4000 Training loss: 0.0136 0.4440 sec/batch\n",
      "Epoch 1392/2000  Iteration 2783/4000 Training loss: 0.0246 0.4436 sec/batch\n",
      "Epoch 1392/2000  Iteration 2784/4000 Training loss: 0.0138 0.4434 sec/batch\n",
      "Epoch 1393/2000  Iteration 2785/4000 Training loss: 0.0247 0.4438 sec/batch\n",
      "Epoch 1393/2000  Iteration 2786/4000 Training loss: 0.0135 0.4441 sec/batch\n",
      "Epoch 1394/2000  Iteration 2787/4000 Training loss: 0.0237 0.4435 sec/batch\n",
      "Epoch 1394/2000  Iteration 2788/4000 Training loss: 0.0129 0.4439 sec/batch\n",
      "Epoch 1395/2000  Iteration 2789/4000 Training loss: 0.0249 0.4432 sec/batch\n",
      "Epoch 1395/2000  Iteration 2790/4000 Training loss: 0.0139 0.4438 sec/batch\n",
      "Epoch 1396/2000  Iteration 2791/4000 Training loss: 0.0232 0.4437 sec/batch\n",
      "Epoch 1396/2000  Iteration 2792/4000 Training loss: 0.0130 0.4434 sec/batch\n",
      "Epoch 1397/2000  Iteration 2793/4000 Training loss: 0.0234 0.4434 sec/batch\n",
      "Epoch 1397/2000  Iteration 2794/4000 Training loss: 0.0127 0.4441 sec/batch\n",
      "Epoch 1398/2000  Iteration 2795/4000 Training loss: 0.0246 0.4445 sec/batch\n",
      "Epoch 1398/2000  Iteration 2796/4000 Training loss: 0.0133 0.4438 sec/batch\n",
      "Epoch 1399/2000  Iteration 2797/4000 Training loss: 0.0239 0.4440 sec/batch\n",
      "Epoch 1399/2000  Iteration 2798/4000 Training loss: 0.0132 0.4439 sec/batch\n",
      "Epoch 1400/2000  Iteration 2799/4000 Training loss: 0.0232 0.4441 sec/batch\n",
      "Epoch 1400/2000  Iteration 2800/4000 Training loss: 0.0131 0.4433 sec/batch\n",
      "Validation loss: 6.46269 Saving checkpoint!\n",
      "Epoch 1401/2000  Iteration 2801/4000 Training loss: 0.0251 0.4444 sec/batch\n",
      "Epoch 1401/2000  Iteration 2802/4000 Training loss: 0.0139 0.4441 sec/batch\n",
      "Epoch 1402/2000  Iteration 2803/4000 Training loss: 0.0245 0.4445 sec/batch\n",
      "Epoch 1402/2000  Iteration 2804/4000 Training loss: 0.0135 0.4447 sec/batch\n",
      "Epoch 1403/2000  Iteration 2805/4000 Training loss: 0.0241 0.4459 sec/batch\n",
      "Epoch 1403/2000  Iteration 2806/4000 Training loss: 0.0132 0.4436 sec/batch\n",
      "Epoch 1404/2000  Iteration 2807/4000 Training loss: 0.0242 0.4434 sec/batch\n",
      "Epoch 1404/2000  Iteration 2808/4000 Training loss: 0.0133 0.4438 sec/batch\n",
      "Epoch 1405/2000  Iteration 2809/4000 Training loss: 0.0242 0.4434 sec/batch\n",
      "Epoch 1405/2000  Iteration 2810/4000 Training loss: 0.0133 0.4437 sec/batch\n",
      "Epoch 1406/2000  Iteration 2811/4000 Training loss: 0.0229 0.4436 sec/batch\n",
      "Epoch 1406/2000  Iteration 2812/4000 Training loss: 0.0125 0.4439 sec/batch\n",
      "Epoch 1407/2000  Iteration 2813/4000 Training loss: 0.0246 0.4435 sec/batch\n",
      "Epoch 1407/2000  Iteration 2814/4000 Training loss: 0.0135 0.4438 sec/batch\n",
      "Epoch 1408/2000  Iteration 2815/4000 Training loss: 0.0233 0.4435 sec/batch\n",
      "Epoch 1408/2000  Iteration 2816/4000 Training loss: 0.0130 0.4434 sec/batch\n",
      "Epoch 1409/2000  Iteration 2817/4000 Training loss: 0.0246 0.4435 sec/batch\n",
      "Epoch 1409/2000  Iteration 2818/4000 Training loss: 0.0135 0.4440 sec/batch\n",
      "Epoch 1410/2000  Iteration 2819/4000 Training loss: 0.0239 0.4434 sec/batch\n",
      "Epoch 1410/2000  Iteration 2820/4000 Training loss: 0.0131 0.4432 sec/batch\n",
      "Epoch 1411/2000  Iteration 2821/4000 Training loss: 0.0261 0.4431 sec/batch\n",
      "Epoch 1411/2000  Iteration 2822/4000 Training loss: 0.0142 0.4436 sec/batch\n",
      "Epoch 1412/2000  Iteration 2823/4000 Training loss: 0.0242 0.4430 sec/batch\n",
      "Epoch 1412/2000  Iteration 2824/4000 Training loss: 0.0132 0.4435 sec/batch\n",
      "Epoch 1413/2000  Iteration 2825/4000 Training loss: 0.0231 0.4429 sec/batch\n",
      "Epoch 1413/2000  Iteration 2826/4000 Training loss: 0.0126 0.4431 sec/batch\n",
      "Epoch 1414/2000  Iteration 2827/4000 Training loss: 0.0238 0.4436 sec/batch\n",
      "Epoch 1414/2000  Iteration 2828/4000 Training loss: 0.0128 0.4434 sec/batch\n",
      "Epoch 1415/2000  Iteration 2829/4000 Training loss: 0.0243 0.4435 sec/batch\n",
      "Epoch 1415/2000  Iteration 2830/4000 Training loss: 0.0131 0.4439 sec/batch\n",
      "Epoch 1416/2000  Iteration 2831/4000 Training loss: 0.0241 0.4434 sec/batch\n",
      "Epoch 1416/2000  Iteration 2832/4000 Training loss: 0.0129 0.4435 sec/batch\n",
      "Epoch 1417/2000  Iteration 2833/4000 Training loss: 0.0234 0.4428 sec/batch\n",
      "Epoch 1417/2000  Iteration 2834/4000 Training loss: 0.0128 0.4435 sec/batch\n",
      "Epoch 1418/2000  Iteration 2835/4000 Training loss: 0.0244 0.4434 sec/batch\n",
      "Epoch 1418/2000  Iteration 2836/4000 Training loss: 0.0135 0.4436 sec/batch\n",
      "Epoch 1419/2000  Iteration 2837/4000 Training loss: 0.0224 0.4432 sec/batch\n",
      "Epoch 1419/2000  Iteration 2838/4000 Training loss: 0.0122 0.4438 sec/batch\n",
      "Epoch 1420/2000  Iteration 2839/4000 Training loss: 0.0233 0.4432 sec/batch\n",
      "Epoch 1420/2000  Iteration 2840/4000 Training loss: 0.0126 0.4442 sec/batch\n",
      "Epoch 1421/2000  Iteration 2841/4000 Training loss: 0.0244 0.4435 sec/batch\n",
      "Epoch 1421/2000  Iteration 2842/4000 Training loss: 0.0131 0.4445 sec/batch\n",
      "Epoch 1422/2000  Iteration 2843/4000 Training loss: 0.0238 0.4430 sec/batch\n",
      "Epoch 1422/2000  Iteration 2844/4000 Training loss: 0.0128 0.4441 sec/batch\n",
      "Epoch 1423/2000  Iteration 2845/4000 Training loss: 0.0231 0.4432 sec/batch\n",
      "Epoch 1423/2000  Iteration 2846/4000 Training loss: 0.0124 0.4437 sec/batch\n",
      "Epoch 1424/2000  Iteration 2847/4000 Training loss: 0.0244 0.4433 sec/batch\n",
      "Epoch 1424/2000  Iteration 2848/4000 Training loss: 0.0132 0.4439 sec/batch\n",
      "Epoch 1425/2000  Iteration 2849/4000 Training loss: 0.0239 0.4435 sec/batch\n",
      "Epoch 1425/2000  Iteration 2850/4000 Training loss: 0.0130 0.4432 sec/batch\n",
      "Epoch 1426/2000  Iteration 2851/4000 Training loss: 0.0235 0.4431 sec/batch\n",
      "Epoch 1426/2000  Iteration 2852/4000 Training loss: 0.0127 0.4431 sec/batch\n",
      "Epoch 1427/2000  Iteration 2853/4000 Training loss: 0.0239 0.4432 sec/batch\n",
      "Epoch 1427/2000  Iteration 2854/4000 Training loss: 0.0128 0.4431 sec/batch\n",
      "Epoch 1428/2000  Iteration 2855/4000 Training loss: 0.0219 0.4431 sec/batch\n",
      "Epoch 1428/2000  Iteration 2856/4000 Training loss: 0.0121 0.4435 sec/batch\n",
      "Epoch 1429/2000  Iteration 2857/4000 Training loss: 0.0228 0.4432 sec/batch\n",
      "Epoch 1429/2000  Iteration 2858/4000 Training loss: 0.0120 0.4435 sec/batch\n",
      "Epoch 1430/2000  Iteration 2859/4000 Training loss: 0.0230 0.4442 sec/batch\n",
      "Epoch 1430/2000  Iteration 2860/4000 Training loss: 0.0125 0.4432 sec/batch\n",
      "Epoch 1431/2000  Iteration 2861/4000 Training loss: 0.0232 0.4430 sec/batch\n",
      "Epoch 1431/2000  Iteration 2862/4000 Training loss: 0.0125 0.4437 sec/batch\n",
      "Epoch 1432/2000  Iteration 2863/4000 Training loss: 0.0226 0.4432 sec/batch\n",
      "Epoch 1432/2000  Iteration 2864/4000 Training loss: 0.0122 0.4436 sec/batch\n",
      "Epoch 1433/2000  Iteration 2865/4000 Training loss: 0.0244 0.4437 sec/batch\n",
      "Epoch 1433/2000  Iteration 2866/4000 Training loss: 0.0132 0.4436 sec/batch\n",
      "Epoch 1434/2000  Iteration 2867/4000 Training loss: 0.0233 0.4435 sec/batch\n",
      "Epoch 1434/2000  Iteration 2868/4000 Training loss: 0.0125 0.4433 sec/batch\n",
      "Epoch 1435/2000  Iteration 2869/4000 Training loss: 0.0226 0.4433 sec/batch\n",
      "Epoch 1435/2000  Iteration 2870/4000 Training loss: 0.0124 0.4434 sec/batch\n",
      "Epoch 1436/2000  Iteration 2871/4000 Training loss: 0.0238 0.4434 sec/batch\n",
      "Epoch 1436/2000  Iteration 2872/4000 Training loss: 0.0126 0.4433 sec/batch\n",
      "Epoch 1437/2000  Iteration 2873/4000 Training loss: 0.0238 0.4432 sec/batch\n",
      "Epoch 1437/2000  Iteration 2874/4000 Training loss: 0.0129 0.4434 sec/batch\n",
      "Epoch 1438/2000  Iteration 2875/4000 Training loss: 0.0222 0.4431 sec/batch\n",
      "Epoch 1438/2000  Iteration 2876/4000 Training loss: 0.0119 0.4433 sec/batch\n",
      "Epoch 1439/2000  Iteration 2877/4000 Training loss: 0.0230 0.4436 sec/batch\n",
      "Epoch 1439/2000  Iteration 2878/4000 Training loss: 0.0126 0.4439 sec/batch\n",
      "Epoch 1440/2000  Iteration 2879/4000 Training loss: 0.0232 0.4436 sec/batch\n",
      "Epoch 1440/2000  Iteration 2880/4000 Training loss: 0.0125 0.4440 sec/batch\n",
      "Epoch 1441/2000  Iteration 2881/4000 Training loss: 0.0237 0.4433 sec/batch\n",
      "Epoch 1441/2000  Iteration 2882/4000 Training loss: 0.0127 0.4439 sec/batch\n",
      "Epoch 1442/2000  Iteration 2883/4000 Training loss: 0.0239 0.4437 sec/batch\n",
      "Epoch 1442/2000  Iteration 2884/4000 Training loss: 0.0127 0.4434 sec/batch\n",
      "Epoch 1443/2000  Iteration 2885/4000 Training loss: 0.0227 0.4433 sec/batch\n",
      "Epoch 1443/2000  Iteration 2886/4000 Training loss: 0.0120 0.4429 sec/batch\n",
      "Epoch 1444/2000  Iteration 2887/4000 Training loss: 0.0232 0.4439 sec/batch\n",
      "Epoch 1444/2000  Iteration 2888/4000 Training loss: 0.0124 0.4438 sec/batch\n",
      "Epoch 1445/2000  Iteration 2889/4000 Training loss: 0.0233 0.4432 sec/batch\n",
      "Epoch 1445/2000  Iteration 2890/4000 Training loss: 0.0123 0.4437 sec/batch\n",
      "Epoch 1446/2000  Iteration 2891/4000 Training loss: 0.0238 0.4432 sec/batch\n",
      "Epoch 1446/2000  Iteration 2892/4000 Training loss: 0.0125 0.4432 sec/batch\n",
      "Epoch 1447/2000  Iteration 2893/4000 Training loss: 0.0237 0.4433 sec/batch\n",
      "Epoch 1447/2000  Iteration 2894/4000 Training loss: 0.0126 0.4432 sec/batch\n",
      "Epoch 1448/2000  Iteration 2895/4000 Training loss: 0.0224 0.4431 sec/batch\n",
      "Epoch 1448/2000  Iteration 2896/4000 Training loss: 0.0118 0.4436 sec/batch\n",
      "Epoch 1449/2000  Iteration 2897/4000 Training loss: 0.0223 0.4432 sec/batch\n",
      "Epoch 1449/2000  Iteration 2898/4000 Training loss: 0.0119 0.4437 sec/batch\n",
      "Epoch 1450/2000  Iteration 2899/4000 Training loss: 0.0232 0.4433 sec/batch\n",
      "Epoch 1450/2000  Iteration 2900/4000 Training loss: 0.0121 0.4433 sec/batch\n",
      "Epoch 1451/2000  Iteration 2901/4000 Training loss: 0.0232 0.4436 sec/batch\n",
      "Epoch 1451/2000  Iteration 2902/4000 Training loss: 0.0124 0.4439 sec/batch\n",
      "Epoch 1452/2000  Iteration 2903/4000 Training loss: 0.0232 0.4435 sec/batch\n",
      "Epoch 1452/2000  Iteration 2904/4000 Training loss: 0.0123 0.4428 sec/batch\n",
      "Epoch 1453/2000  Iteration 2905/4000 Training loss: 0.0217 0.4431 sec/batch\n",
      "Epoch 1453/2000  Iteration 2906/4000 Training loss: 0.0116 0.4436 sec/batch\n",
      "Epoch 1454/2000  Iteration 2907/4000 Training loss: 0.0237 0.4430 sec/batch\n",
      "Epoch 1454/2000  Iteration 2908/4000 Training loss: 0.0126 0.4436 sec/batch\n",
      "Epoch 1455/2000  Iteration 2909/4000 Training loss: 0.0222 0.4436 sec/batch\n",
      "Epoch 1455/2000  Iteration 2910/4000 Training loss: 0.0117 0.4437 sec/batch\n",
      "Epoch 1456/2000  Iteration 2911/4000 Training loss: 0.0226 0.4433 sec/batch\n",
      "Epoch 1456/2000  Iteration 2912/4000 Training loss: 0.0119 0.4437 sec/batch\n",
      "Epoch 1457/2000  Iteration 2913/4000 Training loss: 0.0233 0.4435 sec/batch\n",
      "Epoch 1457/2000  Iteration 2914/4000 Training loss: 0.0123 0.4436 sec/batch\n",
      "Epoch 1458/2000  Iteration 2915/4000 Training loss: 0.0226 0.4426 sec/batch\n",
      "Epoch 1458/2000  Iteration 2916/4000 Training loss: 0.0119 0.4435 sec/batch\n",
      "Epoch 1459/2000  Iteration 2917/4000 Training loss: 0.0233 0.4432 sec/batch\n",
      "Epoch 1459/2000  Iteration 2918/4000 Training loss: 0.0123 0.4436 sec/batch\n",
      "Epoch 1460/2000  Iteration 2919/4000 Training loss: 0.0221 0.4438 sec/batch\n",
      "Epoch 1460/2000  Iteration 2920/4000 Training loss: 0.0118 0.4436 sec/batch\n",
      "Epoch 1461/2000  Iteration 2921/4000 Training loss: 0.0221 0.4433 sec/batch\n",
      "Epoch 1461/2000  Iteration 2922/4000 Training loss: 0.0118 0.4433 sec/batch\n",
      "Epoch 1462/2000  Iteration 2923/4000 Training loss: 0.0227 0.4434 sec/batch\n",
      "Epoch 1462/2000  Iteration 2924/4000 Training loss: 0.0119 0.4436 sec/batch\n",
      "Epoch 1463/2000  Iteration 2925/4000 Training loss: 0.0222 0.4430 sec/batch\n",
      "Epoch 1463/2000  Iteration 2926/4000 Training loss: 0.0118 0.4434 sec/batch\n",
      "Epoch 1464/2000  Iteration 2927/4000 Training loss: 0.0225 0.4431 sec/batch\n",
      "Epoch 1464/2000  Iteration 2928/4000 Training loss: 0.0118 0.4435 sec/batch\n",
      "Epoch 1465/2000  Iteration 2929/4000 Training loss: 0.0220 0.4433 sec/batch\n",
      "Epoch 1465/2000  Iteration 2930/4000 Training loss: 0.0116 0.4435 sec/batch\n",
      "Epoch 1466/2000  Iteration 2931/4000 Training loss: 0.0240 0.4429 sec/batch\n",
      "Epoch 1466/2000  Iteration 2932/4000 Training loss: 0.0127 0.4435 sec/batch\n",
      "Epoch 1467/2000  Iteration 2933/4000 Training loss: 0.0224 0.4436 sec/batch\n",
      "Epoch 1467/2000  Iteration 2934/4000 Training loss: 0.0118 0.4434 sec/batch\n",
      "Epoch 1468/2000  Iteration 2935/4000 Training loss: 0.0217 0.4434 sec/batch\n",
      "Epoch 1468/2000  Iteration 2936/4000 Training loss: 0.0113 0.4435 sec/batch\n",
      "Epoch 1469/2000  Iteration 2937/4000 Training loss: 0.0216 0.4431 sec/batch\n",
      "Epoch 1469/2000  Iteration 2938/4000 Training loss: 0.0117 0.4435 sec/batch\n",
      "Epoch 1470/2000  Iteration 2939/4000 Training loss: 0.0223 0.4435 sec/batch\n",
      "Epoch 1470/2000  Iteration 2940/4000 Training loss: 0.0116 0.4436 sec/batch\n",
      "Epoch 1471/2000  Iteration 2941/4000 Training loss: 0.0229 0.4437 sec/batch\n",
      "Epoch 1471/2000  Iteration 2942/4000 Training loss: 0.0121 0.4436 sec/batch\n",
      "Epoch 1472/2000  Iteration 2943/4000 Training loss: 0.0229 0.4435 sec/batch\n",
      "Epoch 1472/2000  Iteration 2944/4000 Training loss: 0.0119 0.4437 sec/batch\n",
      "Epoch 1473/2000  Iteration 2945/4000 Training loss: 0.0227 0.4433 sec/batch\n",
      "Epoch 1473/2000  Iteration 2946/4000 Training loss: 0.0123 0.4438 sec/batch\n",
      "Epoch 1474/2000  Iteration 2947/4000 Training loss: 0.0223 0.4431 sec/batch\n",
      "Epoch 1474/2000  Iteration 2948/4000 Training loss: 0.0117 0.4434 sec/batch\n",
      "Epoch 1475/2000  Iteration 2949/4000 Training loss: 0.0232 0.4430 sec/batch\n",
      "Epoch 1475/2000  Iteration 2950/4000 Training loss: 0.0122 0.4435 sec/batch\n",
      "Epoch 1476/2000  Iteration 2951/4000 Training loss: 0.0224 0.4428 sec/batch\n",
      "Epoch 1476/2000  Iteration 2952/4000 Training loss: 0.0117 0.4441 sec/batch\n",
      "Epoch 1477/2000  Iteration 2953/4000 Training loss: 0.0236 0.4437 sec/batch\n",
      "Epoch 1477/2000  Iteration 2954/4000 Training loss: 0.0125 0.4435 sec/batch\n",
      "Epoch 1478/2000  Iteration 2955/4000 Training loss: 0.0217 0.4447 sec/batch\n",
      "Epoch 1478/2000  Iteration 2956/4000 Training loss: 0.0115 0.4439 sec/batch\n",
      "Epoch 1479/2000  Iteration 2957/4000 Training loss: 0.0231 0.4437 sec/batch\n",
      "Epoch 1479/2000  Iteration 2958/4000 Training loss: 0.0120 0.4430 sec/batch\n",
      "Epoch 1480/2000  Iteration 2959/4000 Training loss: 0.0233 0.4435 sec/batch\n",
      "Epoch 1480/2000  Iteration 2960/4000 Training loss: 0.0122 0.4439 sec/batch\n",
      "Epoch 1481/2000  Iteration 2961/4000 Training loss: 0.0225 0.4435 sec/batch\n",
      "Epoch 1481/2000  Iteration 2962/4000 Training loss: 0.0120 0.4436 sec/batch\n",
      "Epoch 1482/2000  Iteration 2963/4000 Training loss: 0.0231 0.4431 sec/batch\n",
      "Epoch 1482/2000  Iteration 2964/4000 Training loss: 0.0120 0.4432 sec/batch\n",
      "Epoch 1483/2000  Iteration 2965/4000 Training loss: 0.0225 0.4433 sec/batch\n",
      "Epoch 1483/2000  Iteration 2966/4000 Training loss: 0.0118 0.4435 sec/batch\n",
      "Epoch 1484/2000  Iteration 2967/4000 Training loss: 0.0211 0.4432 sec/batch\n",
      "Epoch 1484/2000  Iteration 2968/4000 Training loss: 0.0112 0.4439 sec/batch\n",
      "Epoch 1485/2000  Iteration 2969/4000 Training loss: 0.0225 0.4434 sec/batch\n",
      "Epoch 1485/2000  Iteration 2970/4000 Training loss: 0.0117 0.4435 sec/batch\n",
      "Epoch 1486/2000  Iteration 2971/4000 Training loss: 0.0230 0.4436 sec/batch\n",
      "Epoch 1486/2000  Iteration 2972/4000 Training loss: 0.0120 0.4433 sec/batch\n",
      "Epoch 1487/2000  Iteration 2973/4000 Training loss: 0.0214 0.4433 sec/batch\n",
      "Epoch 1487/2000  Iteration 2974/4000 Training loss: 0.0111 0.4434 sec/batch\n",
      "Epoch 1488/2000  Iteration 2975/4000 Training loss: 0.0220 0.4435 sec/batch\n",
      "Epoch 1488/2000  Iteration 2976/4000 Training loss: 0.0114 0.4436 sec/batch\n",
      "Epoch 1489/2000  Iteration 2977/4000 Training loss: 0.0216 0.4430 sec/batch\n",
      "Epoch 1489/2000  Iteration 2978/4000 Training loss: 0.0114 0.4435 sec/batch\n",
      "Epoch 1490/2000  Iteration 2979/4000 Training loss: 0.0231 0.4439 sec/batch\n",
      "Epoch 1490/2000  Iteration 2980/4000 Training loss: 0.0121 0.4434 sec/batch\n",
      "Epoch 1491/2000  Iteration 2981/4000 Training loss: 0.0220 0.4436 sec/batch\n",
      "Epoch 1491/2000  Iteration 2982/4000 Training loss: 0.0114 0.4432 sec/batch\n",
      "Epoch 1492/2000  Iteration 2983/4000 Training loss: 0.0224 0.4434 sec/batch\n",
      "Epoch 1492/2000  Iteration 2984/4000 Training loss: 0.0117 0.4432 sec/batch\n",
      "Epoch 1493/2000  Iteration 2985/4000 Training loss: 0.0220 0.4429 sec/batch\n",
      "Epoch 1493/2000  Iteration 2986/4000 Training loss: 0.0116 0.4434 sec/batch\n",
      "Epoch 1494/2000  Iteration 2987/4000 Training loss: 0.0233 0.4431 sec/batch\n",
      "Epoch 1494/2000  Iteration 2988/4000 Training loss: 0.0120 0.4435 sec/batch\n",
      "Epoch 1495/2000  Iteration 2989/4000 Training loss: 0.0216 0.4434 sec/batch\n",
      "Epoch 1495/2000  Iteration 2990/4000 Training loss: 0.0112 0.4433 sec/batch\n",
      "Epoch 1496/2000  Iteration 2991/4000 Training loss: 0.0223 0.4430 sec/batch\n",
      "Epoch 1496/2000  Iteration 2992/4000 Training loss: 0.0115 0.4436 sec/batch\n",
      "Epoch 1497/2000  Iteration 2993/4000 Training loss: 0.0226 0.4436 sec/batch\n",
      "Epoch 1497/2000  Iteration 2994/4000 Training loss: 0.0116 0.4436 sec/batch\n",
      "Epoch 1498/2000  Iteration 2995/4000 Training loss: 0.0223 0.4433 sec/batch\n",
      "Epoch 1498/2000  Iteration 2996/4000 Training loss: 0.0115 0.4433 sec/batch\n",
      "Epoch 1499/2000  Iteration 2997/4000 Training loss: 0.0230 0.4437 sec/batch\n",
      "Epoch 1499/2000  Iteration 2998/4000 Training loss: 0.0119 0.4440 sec/batch\n",
      "Epoch 1500/2000  Iteration 2999/4000 Training loss: 0.0231 0.4436 sec/batch\n",
      "Epoch 1500/2000  Iteration 3000/4000 Training loss: 0.0119 0.4435 sec/batch\n",
      "Validation loss: 6.66298 Saving checkpoint!\n",
      "Epoch 1501/2000  Iteration 3001/4000 Training loss: 0.0236 0.4439 sec/batch\n",
      "Epoch 1501/2000  Iteration 3002/4000 Training loss: 0.0122 0.4433 sec/batch\n",
      "Epoch 1502/2000  Iteration 3003/4000 Training loss: 0.0222 0.4435 sec/batch\n",
      "Epoch 1502/2000  Iteration 3004/4000 Training loss: 0.0115 0.4440 sec/batch\n",
      "Epoch 1503/2000  Iteration 3005/4000 Training loss: 0.0218 0.4436 sec/batch\n",
      "Epoch 1503/2000  Iteration 3006/4000 Training loss: 0.0112 0.4437 sec/batch\n",
      "Epoch 1504/2000  Iteration 3007/4000 Training loss: 0.0224 0.4434 sec/batch\n",
      "Epoch 1504/2000  Iteration 3008/4000 Training loss: 0.0117 0.4434 sec/batch\n",
      "Epoch 1505/2000  Iteration 3009/4000 Training loss: 0.0224 0.4430 sec/batch\n",
      "Epoch 1505/2000  Iteration 3010/4000 Training loss: 0.0116 0.4435 sec/batch\n",
      "Epoch 1506/2000  Iteration 3011/4000 Training loss: 0.0221 0.4431 sec/batch\n",
      "Epoch 1506/2000  Iteration 3012/4000 Training loss: 0.0113 0.4438 sec/batch\n",
      "Epoch 1507/2000  Iteration 3013/4000 Training loss: 0.0222 0.4465 sec/batch\n",
      "Epoch 1507/2000  Iteration 3014/4000 Training loss: 0.0115 0.4434 sec/batch\n",
      "Epoch 1508/2000  Iteration 3015/4000 Training loss: 0.0217 0.4430 sec/batch\n",
      "Epoch 1508/2000  Iteration 3016/4000 Training loss: 0.0112 0.4440 sec/batch\n",
      "Epoch 1509/2000  Iteration 3017/4000 Training loss: 0.0214 0.4435 sec/batch\n",
      "Epoch 1509/2000  Iteration 3018/4000 Training loss: 0.0110 0.4468 sec/batch\n",
      "Epoch 1510/2000  Iteration 3019/4000 Training loss: 0.0218 0.4435 sec/batch\n",
      "Epoch 1510/2000  Iteration 3020/4000 Training loss: 0.0115 0.4446 sec/batch\n",
      "Epoch 1511/2000  Iteration 3021/4000 Training loss: 0.0219 0.4433 sec/batch\n",
      "Epoch 1511/2000  Iteration 3022/4000 Training loss: 0.0113 0.4439 sec/batch\n",
      "Epoch 1512/2000  Iteration 3023/4000 Training loss: 0.0210 0.4437 sec/batch\n",
      "Epoch 1512/2000  Iteration 3024/4000 Training loss: 0.0109 0.4439 sec/batch\n",
      "Epoch 1513/2000  Iteration 3025/4000 Training loss: 0.0220 0.4436 sec/batch\n",
      "Epoch 1513/2000  Iteration 3026/4000 Training loss: 0.0116 0.4438 sec/batch\n",
      "Epoch 1514/2000  Iteration 3027/4000 Training loss: 0.0224 0.4436 sec/batch\n",
      "Epoch 1514/2000  Iteration 3028/4000 Training loss: 0.0115 0.4442 sec/batch\n",
      "Epoch 1515/2000  Iteration 3029/4000 Training loss: 0.0223 0.4439 sec/batch\n",
      "Epoch 1515/2000  Iteration 3030/4000 Training loss: 0.0114 0.4437 sec/batch\n",
      "Epoch 1516/2000  Iteration 3031/4000 Training loss: 0.0217 0.4431 sec/batch\n",
      "Epoch 1516/2000  Iteration 3032/4000 Training loss: 0.0113 0.4435 sec/batch\n",
      "Epoch 1517/2000  Iteration 3033/4000 Training loss: 0.0217 0.4433 sec/batch\n",
      "Epoch 1517/2000  Iteration 3034/4000 Training loss: 0.0113 0.4429 sec/batch\n",
      "Epoch 1518/2000  Iteration 3035/4000 Training loss: 0.0222 0.4435 sec/batch\n",
      "Epoch 1518/2000  Iteration 3036/4000 Training loss: 0.0115 0.4433 sec/batch\n",
      "Epoch 1519/2000  Iteration 3037/4000 Training loss: 0.0225 0.4431 sec/batch\n",
      "Epoch 1519/2000  Iteration 3038/4000 Training loss: 0.0116 0.4437 sec/batch\n",
      "Epoch 1520/2000  Iteration 3039/4000 Training loss: 0.0219 0.4431 sec/batch\n",
      "Epoch 1520/2000  Iteration 3040/4000 Training loss: 0.0113 0.4437 sec/batch\n",
      "Epoch 1521/2000  Iteration 3041/4000 Training loss: 0.0215 0.4431 sec/batch\n",
      "Epoch 1521/2000  Iteration 3042/4000 Training loss: 0.0111 0.4431 sec/batch\n",
      "Epoch 1522/2000  Iteration 3043/4000 Training loss: 0.0226 0.4433 sec/batch\n",
      "Epoch 1522/2000  Iteration 3044/4000 Training loss: 0.0117 0.4432 sec/batch\n",
      "Epoch 1523/2000  Iteration 3045/4000 Training loss: 0.0220 0.4433 sec/batch\n",
      "Epoch 1523/2000  Iteration 3046/4000 Training loss: 0.0115 0.4433 sec/batch\n",
      "Epoch 1524/2000  Iteration 3047/4000 Training loss: 0.0213 0.4425 sec/batch\n",
      "Epoch 1524/2000  Iteration 3048/4000 Training loss: 0.0111 0.4434 sec/batch\n",
      "Epoch 1525/2000  Iteration 3049/4000 Training loss: 0.0210 0.4433 sec/batch\n",
      "Epoch 1525/2000  Iteration 3050/4000 Training loss: 0.0110 0.4433 sec/batch\n",
      "Epoch 1526/2000  Iteration 3051/4000 Training loss: 0.0227 0.4432 sec/batch\n",
      "Epoch 1526/2000  Iteration 3052/4000 Training loss: 0.0117 0.4432 sec/batch\n",
      "Epoch 1527/2000  Iteration 3053/4000 Training loss: 0.0221 0.4433 sec/batch\n",
      "Epoch 1527/2000  Iteration 3054/4000 Training loss: 0.0116 0.4435 sec/batch\n",
      "Epoch 1528/2000  Iteration 3055/4000 Training loss: 0.0227 0.4431 sec/batch\n",
      "Epoch 1528/2000  Iteration 3056/4000 Training loss: 0.0118 0.4435 sec/batch\n",
      "Epoch 1529/2000  Iteration 3057/4000 Training loss: 0.0221 0.4429 sec/batch\n",
      "Epoch 1529/2000  Iteration 3058/4000 Training loss: 0.0113 0.4437 sec/batch\n",
      "Epoch 1530/2000  Iteration 3059/4000 Training loss: 0.0228 0.4433 sec/batch\n",
      "Epoch 1530/2000  Iteration 3060/4000 Training loss: 0.0119 0.4436 sec/batch\n",
      "Epoch 1531/2000  Iteration 3061/4000 Training loss: 0.0215 0.4430 sec/batch\n",
      "Epoch 1531/2000  Iteration 3062/4000 Training loss: 0.0113 0.4437 sec/batch\n",
      "Epoch 1532/2000  Iteration 3063/4000 Training loss: 0.0225 0.4430 sec/batch\n",
      "Epoch 1532/2000  Iteration 3064/4000 Training loss: 0.0118 0.4433 sec/batch\n",
      "Epoch 1533/2000  Iteration 3065/4000 Training loss: 0.0211 0.4437 sec/batch\n",
      "Epoch 1533/2000  Iteration 3066/4000 Training loss: 0.0112 0.4435 sec/batch\n",
      "Epoch 1534/2000  Iteration 3067/4000 Training loss: 0.0224 0.4433 sec/batch\n",
      "Epoch 1534/2000  Iteration 3068/4000 Training loss: 0.0118 0.4434 sec/batch\n",
      "Epoch 1535/2000  Iteration 3069/4000 Training loss: 0.0222 0.4430 sec/batch\n",
      "Epoch 1535/2000  Iteration 3070/4000 Training loss: 0.0116 0.4431 sec/batch\n",
      "Epoch 1536/2000  Iteration 3071/4000 Training loss: 0.0210 0.4433 sec/batch\n",
      "Epoch 1536/2000  Iteration 3072/4000 Training loss: 0.0110 0.4432 sec/batch\n",
      "Epoch 1537/2000  Iteration 3073/4000 Training loss: 0.0241 0.4432 sec/batch\n",
      "Epoch 1537/2000  Iteration 3074/4000 Training loss: 0.0125 0.4437 sec/batch\n",
      "Epoch 1538/2000  Iteration 3075/4000 Training loss: 0.0202 0.4431 sec/batch\n",
      "Epoch 1538/2000  Iteration 3076/4000 Training loss: 0.0109 0.4438 sec/batch\n",
      "Epoch 1539/2000  Iteration 3077/4000 Training loss: 0.0218 0.4431 sec/batch\n",
      "Epoch 1539/2000  Iteration 3078/4000 Training loss: 0.0113 0.4437 sec/batch\n",
      "Epoch 1540/2000  Iteration 3079/4000 Training loss: 0.0219 0.4436 sec/batch\n",
      "Epoch 1540/2000  Iteration 3080/4000 Training loss: 0.0115 0.4437 sec/batch\n",
      "Epoch 1541/2000  Iteration 3081/4000 Training loss: 0.0232 0.4434 sec/batch\n",
      "Epoch 1541/2000  Iteration 3082/4000 Training loss: 0.0125 0.4436 sec/batch\n",
      "Epoch 1542/2000  Iteration 3083/4000 Training loss: 0.0213 0.4438 sec/batch\n",
      "Epoch 1542/2000  Iteration 3084/4000 Training loss: 0.0113 0.4440 sec/batch\n",
      "Epoch 1543/2000  Iteration 3085/4000 Training loss: 0.0216 0.4432 sec/batch\n",
      "Epoch 1543/2000  Iteration 3086/4000 Training loss: 0.0113 0.4435 sec/batch\n",
      "Epoch 1544/2000  Iteration 3087/4000 Training loss: 0.0223 0.4435 sec/batch\n",
      "Epoch 1544/2000  Iteration 3088/4000 Training loss: 0.0116 0.4431 sec/batch\n",
      "Epoch 1545/2000  Iteration 3089/4000 Training loss: 0.0210 0.4429 sec/batch\n",
      "Epoch 1545/2000  Iteration 3090/4000 Training loss: 0.0112 0.4432 sec/batch\n",
      "Epoch 1546/2000  Iteration 3091/4000 Training loss: 0.0227 0.4429 sec/batch\n",
      "Epoch 1546/2000  Iteration 3092/4000 Training loss: 0.0117 0.4436 sec/batch\n",
      "Epoch 1547/2000  Iteration 3093/4000 Training loss: 0.0221 0.4429 sec/batch\n",
      "Epoch 1547/2000  Iteration 3094/4000 Training loss: 0.0115 0.4438 sec/batch\n",
      "Epoch 1548/2000  Iteration 3095/4000 Training loss: 0.0219 0.4431 sec/batch\n",
      "Epoch 1548/2000  Iteration 3096/4000 Training loss: 0.0115 0.4433 sec/batch\n",
      "Epoch 1549/2000  Iteration 3097/4000 Training loss: 0.0218 0.4433 sec/batch\n",
      "Epoch 1549/2000  Iteration 3098/4000 Training loss: 0.0118 0.4433 sec/batch\n",
      "Epoch 1550/2000  Iteration 3099/4000 Training loss: 0.0232 0.4434 sec/batch\n",
      "Epoch 1550/2000  Iteration 3100/4000 Training loss: 0.0124 0.4433 sec/batch\n",
      "Epoch 1551/2000  Iteration 3101/4000 Training loss: 0.0210 0.4430 sec/batch\n",
      "Epoch 1551/2000  Iteration 3102/4000 Training loss: 0.0115 0.4437 sec/batch\n",
      "Epoch 1552/2000  Iteration 3103/4000 Training loss: 0.0222 0.4436 sec/batch\n",
      "Epoch 1552/2000  Iteration 3104/4000 Training loss: 0.0116 0.4438 sec/batch\n",
      "Epoch 1553/2000  Iteration 3105/4000 Training loss: 0.0205 0.4437 sec/batch\n",
      "Epoch 1553/2000  Iteration 3106/4000 Training loss: 0.0107 0.4436 sec/batch\n",
      "Epoch 1554/2000  Iteration 3107/4000 Training loss: 0.0226 0.4440 sec/batch\n",
      "Epoch 1554/2000  Iteration 3108/4000 Training loss: 0.0118 0.4433 sec/batch\n",
      "Epoch 1555/2000  Iteration 3109/4000 Training loss: 0.0222 0.4439 sec/batch\n",
      "Epoch 1555/2000  Iteration 3110/4000 Training loss: 0.0117 0.4434 sec/batch\n",
      "Epoch 1556/2000  Iteration 3111/4000 Training loss: 0.0223 0.4434 sec/batch\n",
      "Epoch 1556/2000  Iteration 3112/4000 Training loss: 0.0117 0.4436 sec/batch\n",
      "Epoch 1557/2000  Iteration 3113/4000 Training loss: 0.0212 0.4431 sec/batch\n",
      "Epoch 1557/2000  Iteration 3114/4000 Training loss: 0.0112 0.4433 sec/batch\n",
      "Epoch 1558/2000  Iteration 3115/4000 Training loss: 0.0211 0.4437 sec/batch\n",
      "Epoch 1558/2000  Iteration 3116/4000 Training loss: 0.0113 0.4433 sec/batch\n",
      "Epoch 1559/2000  Iteration 3117/4000 Training loss: 0.0217 0.4435 sec/batch\n",
      "Epoch 1559/2000  Iteration 3118/4000 Training loss: 0.0114 0.4436 sec/batch\n",
      "Epoch 1560/2000  Iteration 3119/4000 Training loss: 0.0227 0.4433 sec/batch\n",
      "Epoch 1560/2000  Iteration 3120/4000 Training loss: 0.0119 0.4434 sec/batch\n",
      "Epoch 1561/2000  Iteration 3121/4000 Training loss: 0.0216 0.4435 sec/batch\n",
      "Epoch 1561/2000  Iteration 3122/4000 Training loss: 0.0115 0.4437 sec/batch\n",
      "Epoch 1562/2000  Iteration 3123/4000 Training loss: 0.0215 0.4452 sec/batch\n",
      "Epoch 1562/2000  Iteration 3124/4000 Training loss: 0.0114 0.4436 sec/batch\n",
      "Epoch 1563/2000  Iteration 3125/4000 Training loss: 0.0222 0.4433 sec/batch\n",
      "Epoch 1563/2000  Iteration 3126/4000 Training loss: 0.0116 0.4435 sec/batch\n",
      "Epoch 1564/2000  Iteration 3127/4000 Training loss: 0.0215 0.4436 sec/batch\n",
      "Epoch 1564/2000  Iteration 3128/4000 Training loss: 0.0112 0.4436 sec/batch\n",
      "Epoch 1565/2000  Iteration 3129/4000 Training loss: 0.0216 0.4433 sec/batch\n",
      "Epoch 1565/2000  Iteration 3130/4000 Training loss: 0.0113 0.4439 sec/batch\n",
      "Epoch 1566/2000  Iteration 3131/4000 Training loss: 0.0207 0.4436 sec/batch\n",
      "Epoch 1566/2000  Iteration 3132/4000 Training loss: 0.0108 0.4434 sec/batch\n",
      "Epoch 1567/2000  Iteration 3133/4000 Training loss: 0.0219 0.4439 sec/batch\n",
      "Epoch 1567/2000  Iteration 3134/4000 Training loss: 0.0114 0.4437 sec/batch\n",
      "Epoch 1568/2000  Iteration 3135/4000 Training loss: 0.0220 0.4432 sec/batch\n",
      "Epoch 1568/2000  Iteration 3136/4000 Training loss: 0.0117 0.4429 sec/batch\n",
      "Epoch 1569/2000  Iteration 3137/4000 Training loss: 0.0208 0.4432 sec/batch\n",
      "Epoch 1569/2000  Iteration 3138/4000 Training loss: 0.0108 0.4434 sec/batch\n",
      "Epoch 1570/2000  Iteration 3139/4000 Training loss: 0.0221 0.4432 sec/batch\n",
      "Epoch 1570/2000  Iteration 3140/4000 Training loss: 0.0115 0.4435 sec/batch\n",
      "Epoch 1571/2000  Iteration 3141/4000 Training loss: 0.0214 0.4438 sec/batch\n",
      "Epoch 1571/2000  Iteration 3142/4000 Training loss: 0.0112 0.4435 sec/batch\n",
      "Epoch 1572/2000  Iteration 3143/4000 Training loss: 0.0210 0.4432 sec/batch\n",
      "Epoch 1572/2000  Iteration 3144/4000 Training loss: 0.0109 0.4434 sec/batch\n",
      "Epoch 1573/2000  Iteration 3145/4000 Training loss: 0.0222 0.4432 sec/batch\n",
      "Epoch 1573/2000  Iteration 3146/4000 Training loss: 0.0116 0.4438 sec/batch\n",
      "Epoch 1574/2000  Iteration 3147/4000 Training loss: 0.0217 0.4432 sec/batch\n",
      "Epoch 1574/2000  Iteration 3148/4000 Training loss: 0.0113 0.4433 sec/batch\n",
      "Epoch 1575/2000  Iteration 3149/4000 Training loss: 0.0217 0.4434 sec/batch\n",
      "Epoch 1575/2000  Iteration 3150/4000 Training loss: 0.0113 0.4460 sec/batch\n",
      "Epoch 1576/2000  Iteration 3151/4000 Training loss: 0.0211 0.4455 sec/batch\n",
      "Epoch 1576/2000  Iteration 3152/4000 Training loss: 0.0110 0.4438 sec/batch\n",
      "Epoch 1577/2000  Iteration 3153/4000 Training loss: 0.0216 0.4431 sec/batch\n",
      "Epoch 1577/2000  Iteration 3154/4000 Training loss: 0.0113 0.4436 sec/batch\n",
      "Epoch 1578/2000  Iteration 3155/4000 Training loss: 0.0223 0.4434 sec/batch\n",
      "Epoch 1578/2000  Iteration 3156/4000 Training loss: 0.0116 0.4436 sec/batch\n",
      "Epoch 1579/2000  Iteration 3157/4000 Training loss: 0.0223 0.4429 sec/batch\n",
      "Epoch 1579/2000  Iteration 3158/4000 Training loss: 0.0115 0.4435 sec/batch\n",
      "Epoch 1580/2000  Iteration 3159/4000 Training loss: 0.0217 0.4434 sec/batch\n",
      "Epoch 1580/2000  Iteration 3160/4000 Training loss: 0.0113 0.4437 sec/batch\n",
      "Epoch 1581/2000  Iteration 3161/4000 Training loss: 0.0224 0.4437 sec/batch\n",
      "Epoch 1581/2000  Iteration 3162/4000 Training loss: 0.0117 0.4438 sec/batch\n",
      "Epoch 1582/2000  Iteration 3163/4000 Training loss: 0.0219 0.4431 sec/batch\n",
      "Epoch 1582/2000  Iteration 3164/4000 Training loss: 0.0114 0.4432 sec/batch\n",
      "Epoch 1583/2000  Iteration 3165/4000 Training loss: 0.0220 0.4432 sec/batch\n",
      "Epoch 1583/2000  Iteration 3166/4000 Training loss: 0.0116 0.4433 sec/batch\n",
      "Epoch 1584/2000  Iteration 3167/4000 Training loss: 0.0220 0.4432 sec/batch\n",
      "Epoch 1584/2000  Iteration 3168/4000 Training loss: 0.0114 0.4435 sec/batch\n",
      "Epoch 1585/2000  Iteration 3169/4000 Training loss: 0.0214 0.4433 sec/batch\n",
      "Epoch 1585/2000  Iteration 3170/4000 Training loss: 0.0110 0.4436 sec/batch\n",
      "Epoch 1586/2000  Iteration 3171/4000 Training loss: 0.0226 0.4431 sec/batch\n",
      "Epoch 1586/2000  Iteration 3172/4000 Training loss: 0.0117 0.4437 sec/batch\n",
      "Epoch 1587/2000  Iteration 3173/4000 Training loss: 0.0203 0.4436 sec/batch\n",
      "Epoch 1587/2000  Iteration 3174/4000 Training loss: 0.0108 0.4435 sec/batch\n",
      "Epoch 1588/2000  Iteration 3175/4000 Training loss: 0.0215 0.4431 sec/batch\n",
      "Epoch 1588/2000  Iteration 3176/4000 Training loss: 0.0114 0.4440 sec/batch\n",
      "Epoch 1589/2000  Iteration 3177/4000 Training loss: 0.0224 0.4431 sec/batch\n",
      "Epoch 1589/2000  Iteration 3178/4000 Training loss: 0.0117 0.4437 sec/batch\n",
      "Epoch 1590/2000  Iteration 3179/4000 Training loss: 0.0220 0.4430 sec/batch\n",
      "Epoch 1590/2000  Iteration 3180/4000 Training loss: 0.0115 0.4436 sec/batch\n",
      "Epoch 1591/2000  Iteration 3181/4000 Training loss: 0.0217 0.4428 sec/batch\n",
      "Epoch 1591/2000  Iteration 3182/4000 Training loss: 0.0125 0.4436 sec/batch\n",
      "Epoch 1592/2000  Iteration 3183/4000 Training loss: 0.0224 0.4434 sec/batch\n",
      "Epoch 1592/2000  Iteration 3184/4000 Training loss: 0.0118 0.4432 sec/batch\n",
      "Epoch 1593/2000  Iteration 3185/4000 Training loss: 0.0214 0.4430 sec/batch\n",
      "Epoch 1593/2000  Iteration 3186/4000 Training loss: 0.0115 0.4434 sec/batch\n",
      "Epoch 1594/2000  Iteration 3187/4000 Training loss: 0.0212 0.4434 sec/batch\n",
      "Epoch 1594/2000  Iteration 3188/4000 Training loss: 0.0113 0.4434 sec/batch\n",
      "Epoch 1595/2000  Iteration 3189/4000 Training loss: 0.0212 0.4433 sec/batch\n",
      "Epoch 1595/2000  Iteration 3190/4000 Training loss: 0.0115 0.4437 sec/batch\n",
      "Epoch 1596/2000  Iteration 3191/4000 Training loss: 0.0219 0.4431 sec/batch\n",
      "Epoch 1596/2000  Iteration 3192/4000 Training loss: 0.0117 0.4436 sec/batch\n",
      "Epoch 1597/2000  Iteration 3193/4000 Training loss: 0.0219 0.4430 sec/batch\n",
      "Epoch 1597/2000  Iteration 3194/4000 Training loss: 0.0118 0.4434 sec/batch\n",
      "Epoch 1598/2000  Iteration 3195/4000 Training loss: 0.0208 0.4433 sec/batch\n",
      "Epoch 1598/2000  Iteration 3196/4000 Training loss: 0.0109 0.4436 sec/batch\n",
      "Epoch 1599/2000  Iteration 3197/4000 Training loss: 0.0218 0.4431 sec/batch\n",
      "Epoch 1599/2000  Iteration 3198/4000 Training loss: 0.0116 0.4433 sec/batch\n",
      "Epoch 1600/2000  Iteration 3199/4000 Training loss: 0.0221 0.4433 sec/batch\n",
      "Epoch 1600/2000  Iteration 3200/4000 Training loss: 0.0116 0.4433 sec/batch\n",
      "Validation loss: 6.70353 Saving checkpoint!\n",
      "Epoch 1601/2000  Iteration 3201/4000 Training loss: 0.0230 0.4435 sec/batch\n",
      "Epoch 1601/2000  Iteration 3202/4000 Training loss: 0.0121 0.4436 sec/batch\n",
      "Epoch 1602/2000  Iteration 3203/4000 Training loss: 0.0224 0.4433 sec/batch\n",
      "Epoch 1602/2000  Iteration 3204/4000 Training loss: 0.0121 0.4436 sec/batch\n",
      "Epoch 1603/2000  Iteration 3205/4000 Training loss: 0.0222 0.4436 sec/batch\n",
      "Epoch 1603/2000  Iteration 3206/4000 Training loss: 0.0117 0.4436 sec/batch\n",
      "Epoch 1604/2000  Iteration 3207/4000 Training loss: 0.0213 0.4432 sec/batch\n",
      "Epoch 1604/2000  Iteration 3208/4000 Training loss: 0.0113 0.4472 sec/batch\n",
      "Epoch 1605/2000  Iteration 3209/4000 Training loss: 0.0226 0.4432 sec/batch\n",
      "Epoch 1605/2000  Iteration 3210/4000 Training loss: 0.0117 0.4435 sec/batch\n",
      "Epoch 1606/2000  Iteration 3211/4000 Training loss: 0.0217 0.4438 sec/batch\n",
      "Epoch 1606/2000  Iteration 3212/4000 Training loss: 0.0113 0.4435 sec/batch\n",
      "Epoch 1607/2000  Iteration 3213/4000 Training loss: 0.0212 0.4435 sec/batch\n",
      "Epoch 1607/2000  Iteration 3214/4000 Training loss: 0.0113 0.4439 sec/batch\n",
      "Epoch 1608/2000  Iteration 3215/4000 Training loss: 0.0224 0.4435 sec/batch\n",
      "Epoch 1608/2000  Iteration 3216/4000 Training loss: 0.0117 0.4436 sec/batch\n",
      "Epoch 1609/2000  Iteration 3217/4000 Training loss: 0.0224 0.4429 sec/batch\n",
      "Epoch 1609/2000  Iteration 3218/4000 Training loss: 0.0119 0.4434 sec/batch\n",
      "Epoch 1610/2000  Iteration 3219/4000 Training loss: 0.0229 0.4435 sec/batch\n",
      "Epoch 1610/2000  Iteration 3220/4000 Training loss: 0.0120 0.4437 sec/batch\n",
      "Epoch 1611/2000  Iteration 3221/4000 Training loss: 0.0224 0.4433 sec/batch\n",
      "Epoch 1611/2000  Iteration 3222/4000 Training loss: 0.0118 0.4431 sec/batch\n",
      "Epoch 1612/2000  Iteration 3223/4000 Training loss: 0.0212 0.4433 sec/batch\n",
      "Epoch 1612/2000  Iteration 3224/4000 Training loss: 0.0111 0.4435 sec/batch\n",
      "Epoch 1613/2000  Iteration 3225/4000 Training loss: 0.0222 0.4432 sec/batch\n",
      "Epoch 1613/2000  Iteration 3226/4000 Training loss: 0.0117 0.4437 sec/batch\n",
      "Epoch 1614/2000  Iteration 3227/4000 Training loss: 0.0223 0.4434 sec/batch\n",
      "Epoch 1614/2000  Iteration 3228/4000 Training loss: 0.0118 0.4432 sec/batch\n",
      "Epoch 1615/2000  Iteration 3229/4000 Training loss: 0.0218 0.4436 sec/batch\n",
      "Epoch 1615/2000  Iteration 3230/4000 Training loss: 0.0116 0.4436 sec/batch\n",
      "Epoch 1616/2000  Iteration 3231/4000 Training loss: 0.0214 0.4433 sec/batch\n",
      "Epoch 1616/2000  Iteration 3232/4000 Training loss: 0.0113 0.4436 sec/batch\n",
      "Epoch 1617/2000  Iteration 3233/4000 Training loss: 0.0204 0.4435 sec/batch\n",
      "Epoch 1617/2000  Iteration 3234/4000 Training loss: 0.0108 0.4438 sec/batch\n",
      "Epoch 1618/2000  Iteration 3235/4000 Training loss: 0.0210 0.4433 sec/batch\n",
      "Epoch 1618/2000  Iteration 3236/4000 Training loss: 0.0114 0.4437 sec/batch\n",
      "Epoch 1619/2000  Iteration 3237/4000 Training loss: 0.0221 0.4440 sec/batch\n",
      "Epoch 1619/2000  Iteration 3238/4000 Training loss: 0.0116 0.4435 sec/batch\n",
      "Epoch 1620/2000  Iteration 3239/4000 Training loss: 0.0224 0.4431 sec/batch\n",
      "Epoch 1620/2000  Iteration 3240/4000 Training loss: 0.0119 0.4431 sec/batch\n",
      "Epoch 1621/2000  Iteration 3241/4000 Training loss: 0.0215 0.4432 sec/batch\n",
      "Epoch 1621/2000  Iteration 3242/4000 Training loss: 0.0115 0.4433 sec/batch\n",
      "Epoch 1622/2000  Iteration 3243/4000 Training loss: 0.0220 0.4431 sec/batch\n",
      "Epoch 1622/2000  Iteration 3244/4000 Training loss: 0.0118 0.4434 sec/batch\n",
      "Epoch 1623/2000  Iteration 3245/4000 Training loss: 0.0231 0.4435 sec/batch\n",
      "Epoch 1623/2000  Iteration 3246/4000 Training loss: 0.0122 0.4465 sec/batch\n",
      "Epoch 1624/2000  Iteration 3247/4000 Training loss: 0.0210 0.4436 sec/batch\n",
      "Epoch 1624/2000  Iteration 3248/4000 Training loss: 0.0114 0.4434 sec/batch\n",
      "Epoch 1625/2000  Iteration 3249/4000 Training loss: 0.0231 0.4434 sec/batch\n",
      "Epoch 1625/2000  Iteration 3250/4000 Training loss: 0.0125 0.4437 sec/batch\n",
      "Epoch 1626/2000  Iteration 3251/4000 Training loss: 0.0218 0.4432 sec/batch\n",
      "Epoch 1626/2000  Iteration 3252/4000 Training loss: 0.0117 0.4434 sec/batch\n",
      "Epoch 1627/2000  Iteration 3253/4000 Training loss: 0.0220 0.4431 sec/batch\n",
      "Epoch 1627/2000  Iteration 3254/4000 Training loss: 0.0119 0.4440 sec/batch\n",
      "Epoch 1628/2000  Iteration 3255/4000 Training loss: 0.0234 0.4435 sec/batch\n",
      "Epoch 1628/2000  Iteration 3256/4000 Training loss: 0.0128 0.4436 sec/batch\n",
      "Epoch 1629/2000  Iteration 3257/4000 Training loss: 0.0220 0.4430 sec/batch\n",
      "Epoch 1629/2000  Iteration 3258/4000 Training loss: 0.0118 0.4434 sec/batch\n",
      "Epoch 1630/2000  Iteration 3259/4000 Training loss: 0.0221 0.4433 sec/batch\n",
      "Epoch 1630/2000  Iteration 3260/4000 Training loss: 0.0121 0.4439 sec/batch\n",
      "Epoch 1631/2000  Iteration 3261/4000 Training loss: 0.0217 0.4431 sec/batch\n",
      "Epoch 1631/2000  Iteration 3262/4000 Training loss: 0.0119 0.4431 sec/batch\n",
      "Epoch 1632/2000  Iteration 3263/4000 Training loss: 0.0214 0.4430 sec/batch\n",
      "Epoch 1632/2000  Iteration 3264/4000 Training loss: 0.0120 0.4433 sec/batch\n",
      "Epoch 1633/2000  Iteration 3265/4000 Training loss: 0.0218 0.4458 sec/batch\n",
      "Epoch 1633/2000  Iteration 3266/4000 Training loss: 0.0117 0.4441 sec/batch\n",
      "Epoch 1634/2000  Iteration 3267/4000 Training loss: 0.0212 0.4432 sec/batch\n",
      "Epoch 1634/2000  Iteration 3268/4000 Training loss: 0.0115 0.4435 sec/batch\n",
      "Epoch 1635/2000  Iteration 3269/4000 Training loss: 0.0220 0.4436 sec/batch\n",
      "Epoch 1635/2000  Iteration 3270/4000 Training loss: 0.0118 0.4440 sec/batch\n",
      "Epoch 1636/2000  Iteration 3271/4000 Training loss: 0.0225 0.4437 sec/batch\n",
      "Epoch 1636/2000  Iteration 3272/4000 Training loss: 0.0120 0.4445 sec/batch\n",
      "Epoch 1637/2000  Iteration 3273/4000 Training loss: 0.0226 0.4434 sec/batch\n",
      "Epoch 1637/2000  Iteration 3274/4000 Training loss: 0.0124 0.4445 sec/batch\n",
      "Epoch 1638/2000  Iteration 3275/4000 Training loss: 0.0213 0.4434 sec/batch\n",
      "Epoch 1638/2000  Iteration 3276/4000 Training loss: 0.0115 0.4438 sec/batch\n",
      "Epoch 1639/2000  Iteration 3277/4000 Training loss: 0.0237 0.4433 sec/batch\n",
      "Epoch 1639/2000  Iteration 3278/4000 Training loss: 0.0127 0.4438 sec/batch\n",
      "Epoch 1640/2000  Iteration 3279/4000 Training loss: 0.0227 0.4439 sec/batch\n",
      "Epoch 1640/2000  Iteration 3280/4000 Training loss: 0.0124 0.4441 sec/batch\n",
      "Epoch 1641/2000  Iteration 3281/4000 Training loss: 0.0228 0.4438 sec/batch\n",
      "Epoch 1641/2000  Iteration 3282/4000 Training loss: 0.0122 0.4442 sec/batch\n",
      "Epoch 1642/2000  Iteration 3283/4000 Training loss: 0.0226 0.4437 sec/batch\n",
      "Epoch 1642/2000  Iteration 3284/4000 Training loss: 0.0122 0.4435 sec/batch\n",
      "Epoch 1643/2000  Iteration 3285/4000 Training loss: 0.0221 0.4434 sec/batch\n",
      "Epoch 1643/2000  Iteration 3286/4000 Training loss: 0.0124 0.4434 sec/batch\n",
      "Epoch 1644/2000  Iteration 3287/4000 Training loss: 0.0225 0.4433 sec/batch\n",
      "Epoch 1644/2000  Iteration 3288/4000 Training loss: 0.0120 0.4437 sec/batch\n",
      "Epoch 1645/2000  Iteration 3289/4000 Training loss: 0.0223 0.4435 sec/batch\n",
      "Epoch 1645/2000  Iteration 3290/4000 Training loss: 0.0121 0.4436 sec/batch\n",
      "Epoch 1646/2000  Iteration 3291/4000 Training loss: 0.0216 0.4431 sec/batch\n",
      "Epoch 1646/2000  Iteration 3292/4000 Training loss: 0.0118 0.4437 sec/batch\n",
      "Epoch 1647/2000  Iteration 3293/4000 Training loss: 0.0227 0.4430 sec/batch\n",
      "Epoch 1647/2000  Iteration 3294/4000 Training loss: 0.0121 0.4433 sec/batch\n",
      "Epoch 1648/2000  Iteration 3295/4000 Training loss: 0.0217 0.4433 sec/batch\n",
      "Epoch 1648/2000  Iteration 3296/4000 Training loss: 0.0118 0.4434 sec/batch\n",
      "Epoch 1649/2000  Iteration 3297/4000 Training loss: 0.0220 0.4432 sec/batch\n",
      "Epoch 1649/2000  Iteration 3298/4000 Training loss: 0.0122 0.4435 sec/batch\n",
      "Epoch 1650/2000  Iteration 3299/4000 Training loss: 0.0227 0.4435 sec/batch\n",
      "Epoch 1650/2000  Iteration 3300/4000 Training loss: 0.0123 0.4436 sec/batch\n",
      "Epoch 1651/2000  Iteration 3301/4000 Training loss: 0.0227 0.4431 sec/batch\n",
      "Epoch 1651/2000  Iteration 3302/4000 Training loss: 0.0126 0.4437 sec/batch\n",
      "Epoch 1652/2000  Iteration 3303/4000 Training loss: 0.0221 0.4433 sec/batch\n",
      "Epoch 1652/2000  Iteration 3304/4000 Training loss: 0.0122 0.4434 sec/batch\n",
      "Epoch 1653/2000  Iteration 3305/4000 Training loss: 0.0237 0.4432 sec/batch\n",
      "Epoch 1653/2000  Iteration 3306/4000 Training loss: 0.0130 0.4437 sec/batch\n",
      "Epoch 1654/2000  Iteration 3307/4000 Training loss: 0.0230 0.4436 sec/batch\n",
      "Epoch 1654/2000  Iteration 3308/4000 Training loss: 0.0124 0.4433 sec/batch\n",
      "Epoch 1655/2000  Iteration 3309/4000 Training loss: 0.0214 0.4432 sec/batch\n",
      "Epoch 1655/2000  Iteration 3310/4000 Training loss: 0.0116 0.4436 sec/batch\n",
      "Epoch 1656/2000  Iteration 3311/4000 Training loss: 0.0235 0.4430 sec/batch\n",
      "Epoch 1656/2000  Iteration 3312/4000 Training loss: 0.0129 0.4436 sec/batch\n",
      "Epoch 1657/2000  Iteration 3313/4000 Training loss: 0.0218 0.4431 sec/batch\n",
      "Epoch 1657/2000  Iteration 3314/4000 Training loss: 0.0121 0.4434 sec/batch\n",
      "Epoch 1658/2000  Iteration 3315/4000 Training loss: 0.0222 0.4432 sec/batch\n",
      "Epoch 1658/2000  Iteration 3316/4000 Training loss: 0.0121 0.4430 sec/batch\n",
      "Epoch 1659/2000  Iteration 3317/4000 Training loss: 0.0220 0.4433 sec/batch\n",
      "Epoch 1659/2000  Iteration 3318/4000 Training loss: 0.0118 0.4434 sec/batch\n",
      "Epoch 1660/2000  Iteration 3319/4000 Training loss: 0.0226 0.4434 sec/batch\n",
      "Epoch 1660/2000  Iteration 3320/4000 Training loss: 0.0122 0.4437 sec/batch\n",
      "Epoch 1661/2000  Iteration 3321/4000 Training loss: 0.0230 0.4434 sec/batch\n",
      "Epoch 1661/2000  Iteration 3322/4000 Training loss: 0.0126 0.4435 sec/batch\n",
      "Epoch 1662/2000  Iteration 3323/4000 Training loss: 0.0232 0.4432 sec/batch\n",
      "Epoch 1662/2000  Iteration 3324/4000 Training loss: 0.0124 0.4437 sec/batch\n",
      "Epoch 1663/2000  Iteration 3325/4000 Training loss: 0.0221 0.4432 sec/batch\n",
      "Epoch 1663/2000  Iteration 3326/4000 Training loss: 0.0122 0.4433 sec/batch\n",
      "Epoch 1664/2000  Iteration 3327/4000 Training loss: 0.0223 0.4438 sec/batch\n",
      "Epoch 1664/2000  Iteration 3328/4000 Training loss: 0.0122 0.4443 sec/batch\n",
      "Epoch 1665/2000  Iteration 3329/4000 Training loss: 0.0206 0.4434 sec/batch\n",
      "Epoch 1665/2000  Iteration 3330/4000 Training loss: 0.0113 0.4449 sec/batch\n",
      "Epoch 1666/2000  Iteration 3331/4000 Training loss: 0.0223 0.4439 sec/batch\n",
      "Epoch 1666/2000  Iteration 3332/4000 Training loss: 0.0124 0.4437 sec/batch\n",
      "Epoch 1667/2000  Iteration 3333/4000 Training loss: 0.0219 0.4433 sec/batch\n",
      "Epoch 1667/2000  Iteration 3334/4000 Training loss: 0.0120 0.4436 sec/batch\n",
      "Epoch 1668/2000  Iteration 3335/4000 Training loss: 0.0227 0.4435 sec/batch\n",
      "Epoch 1668/2000  Iteration 3336/4000 Training loss: 0.0124 0.4436 sec/batch\n",
      "Epoch 1669/2000  Iteration 3337/4000 Training loss: 0.0247 0.4435 sec/batch\n",
      "Epoch 1669/2000  Iteration 3338/4000 Training loss: 0.0136 0.4438 sec/batch\n",
      "Epoch 1670/2000  Iteration 3339/4000 Training loss: 0.0228 0.4437 sec/batch\n",
      "Epoch 1670/2000  Iteration 3340/4000 Training loss: 0.0126 0.4441 sec/batch\n",
      "Epoch 1671/2000  Iteration 3341/4000 Training loss: 0.0221 0.4432 sec/batch\n",
      "Epoch 1671/2000  Iteration 3342/4000 Training loss: 0.0123 0.4440 sec/batch\n",
      "Epoch 1672/2000  Iteration 3343/4000 Training loss: 0.0234 0.4431 sec/batch\n",
      "Epoch 1672/2000  Iteration 3344/4000 Training loss: 0.0126 0.4439 sec/batch\n",
      "Epoch 1673/2000  Iteration 3345/4000 Training loss: 0.0231 0.4433 sec/batch\n",
      "Epoch 1673/2000  Iteration 3346/4000 Training loss: 0.0132 0.4431 sec/batch\n",
      "Epoch 1674/2000  Iteration 3347/4000 Training loss: 0.0228 0.4433 sec/batch\n",
      "Epoch 1674/2000  Iteration 3348/4000 Training loss: 0.0126 0.4435 sec/batch\n",
      "Epoch 1675/2000  Iteration 3349/4000 Training loss: 0.0247 0.4436 sec/batch\n",
      "Epoch 1675/2000  Iteration 3350/4000 Training loss: 0.0135 0.4430 sec/batch\n",
      "Epoch 1676/2000  Iteration 3351/4000 Training loss: 0.0226 0.4432 sec/batch\n",
      "Epoch 1676/2000  Iteration 3352/4000 Training loss: 0.0125 0.4432 sec/batch\n",
      "Epoch 1677/2000  Iteration 3353/4000 Training loss: 0.0232 0.4432 sec/batch\n",
      "Epoch 1677/2000  Iteration 3354/4000 Training loss: 0.0128 0.4437 sec/batch\n",
      "Epoch 1678/2000  Iteration 3355/4000 Training loss: 0.0233 0.4433 sec/batch\n",
      "Epoch 1678/2000  Iteration 3356/4000 Training loss: 0.0133 0.4437 sec/batch\n",
      "Epoch 1679/2000  Iteration 3357/4000 Training loss: 0.0228 0.4435 sec/batch\n",
      "Epoch 1679/2000  Iteration 3358/4000 Training loss: 0.0130 0.4435 sec/batch\n",
      "Epoch 1680/2000  Iteration 3359/4000 Training loss: 0.0245 0.4435 sec/batch\n",
      "Epoch 1680/2000  Iteration 3360/4000 Training loss: 0.0142 0.4436 sec/batch\n",
      "Epoch 1681/2000  Iteration 3361/4000 Training loss: 0.0232 0.4433 sec/batch\n",
      "Epoch 1681/2000  Iteration 3362/4000 Training loss: 0.0129 0.4436 sec/batch\n",
      "Epoch 1682/2000  Iteration 3363/4000 Training loss: 0.0236 0.4432 sec/batch\n",
      "Epoch 1682/2000  Iteration 3364/4000 Training loss: 0.0136 0.4434 sec/batch\n",
      "Epoch 1683/2000  Iteration 3365/4000 Training loss: 0.0240 0.4429 sec/batch\n",
      "Epoch 1683/2000  Iteration 3366/4000 Training loss: 0.0139 0.4434 sec/batch\n",
      "Epoch 1684/2000  Iteration 3367/4000 Training loss: 0.0243 0.4438 sec/batch\n",
      "Epoch 1684/2000  Iteration 3368/4000 Training loss: 0.0146 0.4438 sec/batch\n",
      "Epoch 1685/2000  Iteration 3369/4000 Training loss: 0.0233 0.4446 sec/batch\n",
      "Epoch 1685/2000  Iteration 3370/4000 Training loss: 0.0135 0.4451 sec/batch\n",
      "Epoch 1686/2000  Iteration 3371/4000 Training loss: 0.0234 0.4436 sec/batch\n",
      "Epoch 1686/2000  Iteration 3372/4000 Training loss: 0.0141 0.4438 sec/batch\n",
      "Epoch 1687/2000  Iteration 3373/4000 Training loss: 0.0245 0.4432 sec/batch\n",
      "Epoch 1687/2000  Iteration 3374/4000 Training loss: 0.0148 0.4430 sec/batch\n",
      "Epoch 1688/2000  Iteration 3375/4000 Training loss: 0.0247 0.4437 sec/batch\n",
      "Epoch 1688/2000  Iteration 3376/4000 Training loss: 0.0142 0.4439 sec/batch\n",
      "Epoch 1689/2000  Iteration 3377/4000 Training loss: 0.0246 0.4432 sec/batch\n",
      "Epoch 1689/2000  Iteration 3378/4000 Training loss: 0.0144 0.4432 sec/batch\n",
      "Epoch 1690/2000  Iteration 3379/4000 Training loss: 0.0243 0.4434 sec/batch\n",
      "Epoch 1690/2000  Iteration 3380/4000 Training loss: 0.0146 0.4436 sec/batch\n",
      "Epoch 1691/2000  Iteration 3381/4000 Training loss: 0.0243 0.4433 sec/batch\n",
      "Epoch 1691/2000  Iteration 3382/4000 Training loss: 0.0143 0.4435 sec/batch\n",
      "Epoch 1692/2000  Iteration 3383/4000 Training loss: 0.0246 0.4435 sec/batch\n",
      "Epoch 1692/2000  Iteration 3384/4000 Training loss: 0.0157 0.4440 sec/batch\n",
      "Epoch 1693/2000  Iteration 3385/4000 Training loss: 0.0245 0.4436 sec/batch\n",
      "Epoch 1693/2000  Iteration 3386/4000 Training loss: 0.0151 0.4434 sec/batch\n",
      "Epoch 1694/2000  Iteration 3387/4000 Training loss: 0.0264 0.4432 sec/batch\n",
      "Epoch 1694/2000  Iteration 3388/4000 Training loss: 0.0173 0.4435 sec/batch\n",
      "Epoch 1695/2000  Iteration 3389/4000 Training loss: 0.0245 0.4435 sec/batch\n",
      "Epoch 1695/2000  Iteration 3390/4000 Training loss: 0.0161 0.4437 sec/batch\n",
      "Epoch 1696/2000  Iteration 3391/4000 Training loss: 0.0252 0.4436 sec/batch\n",
      "Epoch 1696/2000  Iteration 3392/4000 Training loss: 0.0163 0.4434 sec/batch\n",
      "Epoch 1697/2000  Iteration 3393/4000 Training loss: 0.0242 0.4436 sec/batch\n",
      "Epoch 1697/2000  Iteration 3394/4000 Training loss: 0.0154 0.4438 sec/batch\n",
      "Epoch 1698/2000  Iteration 3395/4000 Training loss: 0.0269 0.4431 sec/batch\n",
      "Epoch 1698/2000  Iteration 3396/4000 Training loss: 0.0172 0.4435 sec/batch\n",
      "Epoch 1699/2000  Iteration 3397/4000 Training loss: 0.0264 0.4432 sec/batch\n",
      "Epoch 1699/2000  Iteration 3398/4000 Training loss: 0.0163 0.4438 sec/batch\n",
      "Epoch 1700/2000  Iteration 3399/4000 Training loss: 0.0269 0.4435 sec/batch\n",
      "Epoch 1700/2000  Iteration 3400/4000 Training loss: 0.0171 0.4438 sec/batch\n",
      "Validation loss: 6.49296 Saving checkpoint!\n",
      "Epoch 1701/2000  Iteration 3401/4000 Training loss: 0.0269 0.4437 sec/batch\n",
      "Epoch 1701/2000  Iteration 3402/4000 Training loss: 0.0168 0.4437 sec/batch\n",
      "Epoch 1702/2000  Iteration 3403/4000 Training loss: 0.0261 0.4431 sec/batch\n",
      "Epoch 1702/2000  Iteration 3404/4000 Training loss: 0.0177 0.4435 sec/batch\n",
      "Epoch 1703/2000  Iteration 3405/4000 Training loss: 0.0269 0.4430 sec/batch\n",
      "Epoch 1703/2000  Iteration 3406/4000 Training loss: 0.0176 0.4436 sec/batch\n",
      "Epoch 1704/2000  Iteration 3407/4000 Training loss: 0.0295 0.4432 sec/batch\n",
      "Epoch 1704/2000  Iteration 3408/4000 Training loss: 0.0201 0.4433 sec/batch\n",
      "Epoch 1705/2000  Iteration 3409/4000 Training loss: 0.0256 0.4433 sec/batch\n",
      "Epoch 1705/2000  Iteration 3410/4000 Training loss: 0.0182 0.4439 sec/batch\n",
      "Epoch 1706/2000  Iteration 3411/4000 Training loss: 0.0283 0.4436 sec/batch\n",
      "Epoch 1706/2000  Iteration 3412/4000 Training loss: 0.0201 0.4434 sec/batch\n",
      "Epoch 1707/2000  Iteration 3413/4000 Training loss: 0.0278 0.4432 sec/batch\n",
      "Epoch 1707/2000  Iteration 3414/4000 Training loss: 0.0195 0.4436 sec/batch\n",
      "Epoch 1708/2000  Iteration 3415/4000 Training loss: 0.0279 0.4429 sec/batch\n",
      "Epoch 1708/2000  Iteration 3416/4000 Training loss: 0.0194 0.4436 sec/batch\n",
      "Epoch 1709/2000  Iteration 3417/4000 Training loss: 0.0292 0.4433 sec/batch\n",
      "Epoch 1709/2000  Iteration 3418/4000 Training loss: 0.0209 0.4437 sec/batch\n",
      "Epoch 1710/2000  Iteration 3419/4000 Training loss: 0.0312 0.4431 sec/batch\n",
      "Epoch 1710/2000  Iteration 3420/4000 Training loss: 0.0218 0.4435 sec/batch\n",
      "Epoch 1711/2000  Iteration 3421/4000 Training loss: 0.0306 0.4429 sec/batch\n",
      "Epoch 1711/2000  Iteration 3422/4000 Training loss: 0.0219 0.4435 sec/batch\n",
      "Epoch 1712/2000  Iteration 3423/4000 Training loss: 0.0335 0.4433 sec/batch\n",
      "Epoch 1712/2000  Iteration 3424/4000 Training loss: 0.0237 0.4434 sec/batch\n",
      "Epoch 1713/2000  Iteration 3425/4000 Training loss: 0.0303 0.4433 sec/batch\n",
      "Epoch 1713/2000  Iteration 3426/4000 Training loss: 0.0237 0.4430 sec/batch\n",
      "Epoch 1714/2000  Iteration 3427/4000 Training loss: 0.0331 0.4431 sec/batch\n",
      "Epoch 1714/2000  Iteration 3428/4000 Training loss: 0.0246 0.4438 sec/batch\n",
      "Epoch 1715/2000  Iteration 3429/4000 Training loss: 0.0347 0.4430 sec/batch\n",
      "Epoch 1715/2000  Iteration 3430/4000 Training loss: 0.0244 0.4432 sec/batch\n",
      "Epoch 1716/2000  Iteration 3431/4000 Training loss: 0.0331 0.4433 sec/batch\n",
      "Epoch 1716/2000  Iteration 3432/4000 Training loss: 0.0243 0.4434 sec/batch\n",
      "Epoch 1717/2000  Iteration 3433/4000 Training loss: 0.0351 0.4434 sec/batch\n",
      "Epoch 1717/2000  Iteration 3434/4000 Training loss: 0.0257 0.4435 sec/batch\n",
      "Epoch 1718/2000  Iteration 3435/4000 Training loss: 0.0337 0.4428 sec/batch\n",
      "Epoch 1718/2000  Iteration 3436/4000 Training loss: 0.0253 0.4432 sec/batch\n",
      "Epoch 1719/2000  Iteration 3437/4000 Training loss: 0.0373 0.4431 sec/batch\n",
      "Epoch 1719/2000  Iteration 3438/4000 Training loss: 0.0276 0.4434 sec/batch\n",
      "Epoch 1720/2000  Iteration 3439/4000 Training loss: 0.0361 0.4429 sec/batch\n",
      "Epoch 1720/2000  Iteration 3440/4000 Training loss: 0.0287 0.4434 sec/batch\n",
      "Epoch 1721/2000  Iteration 3441/4000 Training loss: 0.0368 0.4433 sec/batch\n",
      "Epoch 1721/2000  Iteration 3442/4000 Training loss: 0.0320 0.4437 sec/batch\n",
      "Epoch 1722/2000  Iteration 3443/4000 Training loss: 0.0383 0.4431 sec/batch\n",
      "Epoch 1722/2000  Iteration 3444/4000 Training loss: 0.0311 0.4435 sec/batch\n",
      "Epoch 1723/2000  Iteration 3445/4000 Training loss: 0.0354 0.4433 sec/batch\n",
      "Epoch 1723/2000  Iteration 3446/4000 Training loss: 0.0305 0.4431 sec/batch\n",
      "Epoch 1724/2000  Iteration 3447/4000 Training loss: 0.0418 0.4435 sec/batch\n",
      "Epoch 1724/2000  Iteration 3448/4000 Training loss: 0.0333 0.4433 sec/batch\n",
      "Epoch 1725/2000  Iteration 3449/4000 Training loss: 0.0427 0.4434 sec/batch\n",
      "Epoch 1725/2000  Iteration 3450/4000 Training loss: 0.0341 0.4433 sec/batch\n",
      "Epoch 1726/2000  Iteration 3451/4000 Training loss: 0.0360 0.4436 sec/batch\n",
      "Epoch 1726/2000  Iteration 3452/4000 Training loss: 0.0291 0.4437 sec/batch\n",
      "Epoch 1727/2000  Iteration 3453/4000 Training loss: 0.0383 0.4431 sec/batch\n",
      "Epoch 1727/2000  Iteration 3454/4000 Training loss: 0.0333 0.4438 sec/batch\n",
      "Epoch 1728/2000  Iteration 3455/4000 Training loss: 0.0401 0.4436 sec/batch\n",
      "Epoch 1728/2000  Iteration 3456/4000 Training loss: 0.0322 0.4436 sec/batch\n",
      "Epoch 1729/2000  Iteration 3457/4000 Training loss: 0.0441 0.4435 sec/batch\n",
      "Epoch 1729/2000  Iteration 3458/4000 Training loss: 0.0369 0.4434 sec/batch\n",
      "Epoch 1730/2000  Iteration 3459/4000 Training loss: 0.0408 0.4430 sec/batch\n",
      "Epoch 1730/2000  Iteration 3460/4000 Training loss: 0.0351 0.4438 sec/batch\n",
      "Epoch 1731/2000  Iteration 3461/4000 Training loss: 0.0392 0.4432 sec/batch\n",
      "Epoch 1731/2000  Iteration 3462/4000 Training loss: 0.0323 0.4436 sec/batch\n",
      "Epoch 1732/2000  Iteration 3463/4000 Training loss: 0.0473 0.4433 sec/batch\n",
      "Epoch 1732/2000  Iteration 3464/4000 Training loss: 0.0343 0.4435 sec/batch\n",
      "Epoch 1733/2000  Iteration 3465/4000 Training loss: 0.0417 0.4435 sec/batch\n",
      "Epoch 1733/2000  Iteration 3466/4000 Training loss: 0.0340 0.4462 sec/batch\n",
      "Epoch 1734/2000  Iteration 3467/4000 Training loss: 0.0428 0.4444 sec/batch\n",
      "Epoch 1734/2000  Iteration 3468/4000 Training loss: 0.0327 0.4438 sec/batch\n",
      "Epoch 1735/2000  Iteration 3469/4000 Training loss: 0.0407 0.4441 sec/batch\n",
      "Epoch 1735/2000  Iteration 3470/4000 Training loss: 0.0328 0.4440 sec/batch\n",
      "Epoch 1736/2000  Iteration 3471/4000 Training loss: 0.0381 0.4436 sec/batch\n",
      "Epoch 1736/2000  Iteration 3472/4000 Training loss: 0.0318 0.4434 sec/batch\n",
      "Epoch 1737/2000  Iteration 3473/4000 Training loss: 0.0402 0.4442 sec/batch\n",
      "Epoch 1737/2000  Iteration 3474/4000 Training loss: 0.0311 0.4444 sec/batch\n",
      "Epoch 1738/2000  Iteration 3475/4000 Training loss: 0.0386 0.4434 sec/batch\n",
      "Epoch 1738/2000  Iteration 3476/4000 Training loss: 0.0297 0.4445 sec/batch\n",
      "Epoch 1739/2000  Iteration 3477/4000 Training loss: 0.0386 0.4441 sec/batch\n",
      "Epoch 1739/2000  Iteration 3478/4000 Training loss: 0.0309 0.4439 sec/batch\n",
      "Epoch 1740/2000  Iteration 3479/4000 Training loss: 0.0376 0.4442 sec/batch\n",
      "Epoch 1740/2000  Iteration 3480/4000 Training loss: 0.0306 0.4438 sec/batch\n",
      "Epoch 1741/2000  Iteration 3481/4000 Training loss: 0.0375 0.4439 sec/batch\n",
      "Epoch 1741/2000  Iteration 3482/4000 Training loss: 0.0293 0.4440 sec/batch\n",
      "Epoch 1742/2000  Iteration 3483/4000 Training loss: 0.0348 0.4438 sec/batch\n",
      "Epoch 1742/2000  Iteration 3484/4000 Training loss: 0.0275 0.4444 sec/batch\n",
      "Epoch 1743/2000  Iteration 3485/4000 Training loss: 0.0363 0.4451 sec/batch\n",
      "Epoch 1743/2000  Iteration 3486/4000 Training loss: 0.0275 0.4439 sec/batch\n",
      "Epoch 1744/2000  Iteration 3487/4000 Training loss: 0.0361 0.4438 sec/batch\n",
      "Epoch 1744/2000  Iteration 3488/4000 Training loss: 0.0273 0.4434 sec/batch\n",
      "Epoch 1745/2000  Iteration 3489/4000 Training loss: 0.0352 0.4440 sec/batch\n",
      "Epoch 1745/2000  Iteration 3490/4000 Training loss: 0.0265 0.4442 sec/batch\n",
      "Epoch 1746/2000  Iteration 3491/4000 Training loss: 0.0357 0.4436 sec/batch\n",
      "Epoch 1746/2000  Iteration 3492/4000 Training loss: 0.0257 0.4442 sec/batch\n",
      "Epoch 1747/2000  Iteration 3493/4000 Training loss: 0.0344 0.4437 sec/batch\n",
      "Epoch 1747/2000  Iteration 3494/4000 Training loss: 0.0248 0.4439 sec/batch\n",
      "Epoch 1748/2000  Iteration 3495/4000 Training loss: 0.0330 0.4440 sec/batch\n",
      "Epoch 1748/2000  Iteration 3496/4000 Training loss: 0.0249 0.4440 sec/batch\n",
      "Epoch 1749/2000  Iteration 3497/4000 Training loss: 0.0333 0.4436 sec/batch\n",
      "Epoch 1749/2000  Iteration 3498/4000 Training loss: 0.0239 0.4442 sec/batch\n",
      "Epoch 1750/2000  Iteration 3499/4000 Training loss: 0.0321 0.4434 sec/batch\n",
      "Epoch 1750/2000  Iteration 3500/4000 Training loss: 0.0233 0.4438 sec/batch\n",
      "Epoch 1751/2000  Iteration 3501/4000 Training loss: 0.0307 0.4434 sec/batch\n",
      "Epoch 1751/2000  Iteration 3502/4000 Training loss: 0.0209 0.4435 sec/batch\n",
      "Epoch 1752/2000  Iteration 3503/4000 Training loss: 0.0307 0.4430 sec/batch\n",
      "Epoch 1752/2000  Iteration 3504/4000 Training loss: 0.0215 0.4443 sec/batch\n",
      "Epoch 1753/2000  Iteration 3505/4000 Training loss: 0.0306 0.4439 sec/batch\n",
      "Epoch 1753/2000  Iteration 3506/4000 Training loss: 0.0211 0.4438 sec/batch\n",
      "Epoch 1754/2000  Iteration 3507/4000 Training loss: 0.0289 0.4439 sec/batch\n",
      "Epoch 1754/2000  Iteration 3508/4000 Training loss: 0.0196 0.4432 sec/batch\n",
      "Epoch 1755/2000  Iteration 3509/4000 Training loss: 0.0282 0.4429 sec/batch\n",
      "Epoch 1755/2000  Iteration 3510/4000 Training loss: 0.0189 0.4432 sec/batch\n",
      "Epoch 1756/2000  Iteration 3511/4000 Training loss: 0.0284 0.4430 sec/batch\n",
      "Epoch 1756/2000  Iteration 3512/4000 Training loss: 0.0193 0.4435 sec/batch\n",
      "Epoch 1757/2000  Iteration 3513/4000 Training loss: 0.0287 0.4430 sec/batch\n",
      "Epoch 1757/2000  Iteration 3514/4000 Training loss: 0.0190 0.4432 sec/batch\n",
      "Epoch 1758/2000  Iteration 3515/4000 Training loss: 0.0264 0.4433 sec/batch\n",
      "Epoch 1758/2000  Iteration 3516/4000 Training loss: 0.0172 0.4435 sec/batch\n",
      "Epoch 1759/2000  Iteration 3517/4000 Training loss: 0.0297 0.4440 sec/batch\n",
      "Epoch 1759/2000  Iteration 3518/4000 Training loss: 0.0183 0.4438 sec/batch\n",
      "Epoch 1760/2000  Iteration 3519/4000 Training loss: 0.0276 0.4440 sec/batch\n",
      "Epoch 1760/2000  Iteration 3520/4000 Training loss: 0.0179 0.4446 sec/batch\n",
      "Epoch 1761/2000  Iteration 3521/4000 Training loss: 0.0271 0.4431 sec/batch\n",
      "Epoch 1761/2000  Iteration 3522/4000 Training loss: 0.0172 0.4436 sec/batch\n",
      "Epoch 1762/2000  Iteration 3523/4000 Training loss: 0.0261 0.4432 sec/batch\n",
      "Epoch 1762/2000  Iteration 3524/4000 Training loss: 0.0167 0.4438 sec/batch\n",
      "Epoch 1763/2000  Iteration 3525/4000 Training loss: 0.0255 0.4434 sec/batch\n",
      "Epoch 1763/2000  Iteration 3526/4000 Training loss: 0.0158 0.4435 sec/batch\n",
      "Epoch 1764/2000  Iteration 3527/4000 Training loss: 0.0259 0.4442 sec/batch\n",
      "Epoch 1764/2000  Iteration 3528/4000 Training loss: 0.0159 0.4436 sec/batch\n",
      "Epoch 1765/2000  Iteration 3529/4000 Training loss: 0.0256 0.4433 sec/batch\n",
      "Epoch 1765/2000  Iteration 3530/4000 Training loss: 0.0156 0.4434 sec/batch\n",
      "Epoch 1766/2000  Iteration 3531/4000 Training loss: 0.0246 0.4434 sec/batch\n",
      "Epoch 1766/2000  Iteration 3532/4000 Training loss: 0.0153 0.4434 sec/batch\n",
      "Epoch 1767/2000  Iteration 3533/4000 Training loss: 0.0249 0.4437 sec/batch\n",
      "Epoch 1767/2000  Iteration 3534/4000 Training loss: 0.0151 0.4431 sec/batch\n",
      "Epoch 1768/2000  Iteration 3535/4000 Training loss: 0.0254 0.4434 sec/batch\n",
      "Epoch 1768/2000  Iteration 3536/4000 Training loss: 0.0152 0.4435 sec/batch\n",
      "Epoch 1769/2000  Iteration 3537/4000 Training loss: 0.0254 0.4432 sec/batch\n",
      "Epoch 1769/2000  Iteration 3538/4000 Training loss: 0.0146 0.4429 sec/batch\n",
      "Epoch 1770/2000  Iteration 3539/4000 Training loss: 0.0248 0.4433 sec/batch\n",
      "Epoch 1770/2000  Iteration 3540/4000 Training loss: 0.0145 0.4446 sec/batch\n",
      "Epoch 1771/2000  Iteration 3541/4000 Training loss: 0.0242 0.4430 sec/batch\n",
      "Epoch 1771/2000  Iteration 3542/4000 Training loss: 0.0146 0.4437 sec/batch\n",
      "Epoch 1772/2000  Iteration 3543/4000 Training loss: 0.0245 0.4432 sec/batch\n",
      "Epoch 1772/2000  Iteration 3544/4000 Training loss: 0.0144 0.4436 sec/batch\n",
      "Epoch 1773/2000  Iteration 3545/4000 Training loss: 0.0239 0.4431 sec/batch\n",
      "Epoch 1773/2000  Iteration 3546/4000 Training loss: 0.0144 0.4441 sec/batch\n",
      "Epoch 1774/2000  Iteration 3547/4000 Training loss: 0.0247 0.4450 sec/batch\n",
      "Epoch 1774/2000  Iteration 3548/4000 Training loss: 0.0142 0.4438 sec/batch\n",
      "Epoch 1775/2000  Iteration 3549/4000 Training loss: 0.0226 0.4434 sec/batch\n",
      "Epoch 1775/2000  Iteration 3550/4000 Training loss: 0.0132 0.4437 sec/batch\n",
      "Epoch 1776/2000  Iteration 3551/4000 Training loss: 0.0252 0.4433 sec/batch\n",
      "Epoch 1776/2000  Iteration 3552/4000 Training loss: 0.0144 0.4437 sec/batch\n",
      "Epoch 1777/2000  Iteration 3553/4000 Training loss: 0.0233 0.4430 sec/batch\n",
      "Epoch 1777/2000  Iteration 3554/4000 Training loss: 0.0133 0.4440 sec/batch\n",
      "Epoch 1778/2000  Iteration 3555/4000 Training loss: 0.0241 0.4435 sec/batch\n",
      "Epoch 1778/2000  Iteration 3556/4000 Training loss: 0.0135 0.4436 sec/batch\n",
      "Epoch 1779/2000  Iteration 3557/4000 Training loss: 0.0226 0.4435 sec/batch\n",
      "Epoch 1779/2000  Iteration 3558/4000 Training loss: 0.0134 0.4434 sec/batch\n",
      "Epoch 1780/2000  Iteration 3559/4000 Training loss: 0.0224 0.4432 sec/batch\n",
      "Epoch 1780/2000  Iteration 3560/4000 Training loss: 0.0127 0.4434 sec/batch\n",
      "Epoch 1781/2000  Iteration 3561/4000 Training loss: 0.0243 0.4429 sec/batch\n",
      "Epoch 1781/2000  Iteration 3562/4000 Training loss: 0.0136 0.4438 sec/batch\n",
      "Epoch 1782/2000  Iteration 3563/4000 Training loss: 0.0233 0.4434 sec/batch\n",
      "Epoch 1782/2000  Iteration 3564/4000 Training loss: 0.0131 0.4440 sec/batch\n",
      "Epoch 1783/2000  Iteration 3565/4000 Training loss: 0.0235 0.4434 sec/batch\n",
      "Epoch 1783/2000  Iteration 3566/4000 Training loss: 0.0131 0.4437 sec/batch\n",
      "Epoch 1784/2000  Iteration 3567/4000 Training loss: 0.0226 0.4433 sec/batch\n",
      "Epoch 1784/2000  Iteration 3568/4000 Training loss: 0.0128 0.4432 sec/batch\n",
      "Epoch 1785/2000  Iteration 3569/4000 Training loss: 0.0238 0.4428 sec/batch\n",
      "Epoch 1785/2000  Iteration 3570/4000 Training loss: 0.0132 0.4437 sec/batch\n",
      "Epoch 1786/2000  Iteration 3571/4000 Training loss: 0.0225 0.4432 sec/batch\n",
      "Epoch 1786/2000  Iteration 3572/4000 Training loss: 0.0125 0.4437 sec/batch\n",
      "Epoch 1787/2000  Iteration 3573/4000 Training loss: 0.0231 0.4429 sec/batch\n",
      "Epoch 1787/2000  Iteration 3574/4000 Training loss: 0.0127 0.4437 sec/batch\n",
      "Epoch 1788/2000  Iteration 3575/4000 Training loss: 0.0231 0.4429 sec/batch\n",
      "Epoch 1788/2000  Iteration 3576/4000 Training loss: 0.0128 0.4434 sec/batch\n",
      "Epoch 1789/2000  Iteration 3577/4000 Training loss: 0.0234 0.4431 sec/batch\n",
      "Epoch 1789/2000  Iteration 3578/4000 Training loss: 0.0128 0.4438 sec/batch\n",
      "Epoch 1790/2000  Iteration 3579/4000 Training loss: 0.0237 0.4433 sec/batch\n",
      "Epoch 1790/2000  Iteration 3580/4000 Training loss: 0.0131 0.4434 sec/batch\n",
      "Epoch 1791/2000  Iteration 3581/4000 Training loss: 0.0224 0.4432 sec/batch\n",
      "Epoch 1791/2000  Iteration 3582/4000 Training loss: 0.0120 0.4437 sec/batch\n",
      "Epoch 1792/2000  Iteration 3583/4000 Training loss: 0.0224 0.4430 sec/batch\n",
      "Epoch 1792/2000  Iteration 3584/4000 Training loss: 0.0120 0.4431 sec/batch\n",
      "Epoch 1793/2000  Iteration 3585/4000 Training loss: 0.0243 0.4431 sec/batch\n",
      "Epoch 1793/2000  Iteration 3586/4000 Training loss: 0.0130 0.4444 sec/batch\n",
      "Epoch 1794/2000  Iteration 3587/4000 Training loss: 0.0228 0.4434 sec/batch\n",
      "Epoch 1794/2000  Iteration 3588/4000 Training loss: 0.0121 0.4431 sec/batch\n",
      "Epoch 1795/2000  Iteration 3589/4000 Training loss: 0.0222 0.4430 sec/batch\n",
      "Epoch 1795/2000  Iteration 3590/4000 Training loss: 0.0116 0.4435 sec/batch\n",
      "Epoch 1796/2000  Iteration 3591/4000 Training loss: 0.0223 0.4430 sec/batch\n",
      "Epoch 1796/2000  Iteration 3592/4000 Training loss: 0.0118 0.4431 sec/batch\n",
      "Epoch 1797/2000  Iteration 3593/4000 Training loss: 0.0213 0.4430 sec/batch\n",
      "Epoch 1797/2000  Iteration 3594/4000 Training loss: 0.0117 0.4430 sec/batch\n",
      "Epoch 1798/2000  Iteration 3595/4000 Training loss: 0.0217 0.4430 sec/batch\n",
      "Epoch 1798/2000  Iteration 3596/4000 Training loss: 0.0116 0.4433 sec/batch\n",
      "Epoch 1799/2000  Iteration 3597/4000 Training loss: 0.0222 0.4431 sec/batch\n",
      "Epoch 1799/2000  Iteration 3598/4000 Training loss: 0.0118 0.4438 sec/batch\n",
      "Epoch 1800/2000  Iteration 3599/4000 Training loss: 0.0229 0.4433 sec/batch\n",
      "Epoch 1800/2000  Iteration 3600/4000 Training loss: 0.0124 0.4435 sec/batch\n",
      "Validation loss: 6.42035 Saving checkpoint!\n",
      "Epoch 1801/2000  Iteration 3601/4000 Training loss: 0.0223 0.4439 sec/batch\n",
      "Epoch 1801/2000  Iteration 3602/4000 Training loss: 0.0119 0.4435 sec/batch\n",
      "Epoch 1802/2000  Iteration 3603/4000 Training loss: 0.0209 0.4434 sec/batch\n",
      "Epoch 1802/2000  Iteration 3604/4000 Training loss: 0.0110 0.4431 sec/batch\n",
      "Epoch 1803/2000  Iteration 3605/4000 Training loss: 0.0215 0.4432 sec/batch\n",
      "Epoch 1803/2000  Iteration 3606/4000 Training loss: 0.0114 0.4437 sec/batch\n",
      "Epoch 1804/2000  Iteration 3607/4000 Training loss: 0.0223 0.4435 sec/batch\n",
      "Epoch 1804/2000  Iteration 3608/4000 Training loss: 0.0118 0.4439 sec/batch\n",
      "Epoch 1805/2000  Iteration 3609/4000 Training loss: 0.0218 0.4431 sec/batch\n",
      "Epoch 1805/2000  Iteration 3610/4000 Training loss: 0.0115 0.4432 sec/batch\n",
      "Epoch 1806/2000  Iteration 3611/4000 Training loss: 0.0203 0.4432 sec/batch\n",
      "Epoch 1806/2000  Iteration 3612/4000 Training loss: 0.0108 0.4436 sec/batch\n",
      "Epoch 1807/2000  Iteration 3613/4000 Training loss: 0.0218 0.4431 sec/batch\n",
      "Epoch 1807/2000  Iteration 3614/4000 Training loss: 0.0115 0.4445 sec/batch\n",
      "Epoch 1808/2000  Iteration 3615/4000 Training loss: 0.0219 0.4432 sec/batch\n",
      "Epoch 1808/2000  Iteration 3616/4000 Training loss: 0.0115 0.4435 sec/batch\n",
      "Epoch 1809/2000  Iteration 3617/4000 Training loss: 0.0216 0.4437 sec/batch\n",
      "Epoch 1809/2000  Iteration 3618/4000 Training loss: 0.0113 0.4436 sec/batch\n",
      "Epoch 1810/2000  Iteration 3619/4000 Training loss: 0.0226 0.4434 sec/batch\n",
      "Epoch 1810/2000  Iteration 3620/4000 Training loss: 0.0117 0.4435 sec/batch\n",
      "Epoch 1811/2000  Iteration 3621/4000 Training loss: 0.0223 0.4433 sec/batch\n",
      "Epoch 1811/2000  Iteration 3622/4000 Training loss: 0.0117 0.4433 sec/batch\n",
      "Epoch 1812/2000  Iteration 3623/4000 Training loss: 0.0211 0.4434 sec/batch\n",
      "Epoch 1812/2000  Iteration 3624/4000 Training loss: 0.0112 0.4431 sec/batch\n",
      "Epoch 1813/2000  Iteration 3625/4000 Training loss: 0.0220 0.4436 sec/batch\n",
      "Epoch 1813/2000  Iteration 3626/4000 Training loss: 0.0117 0.4434 sec/batch\n",
      "Epoch 1814/2000  Iteration 3627/4000 Training loss: 0.0216 0.4432 sec/batch\n",
      "Epoch 1814/2000  Iteration 3628/4000 Training loss: 0.0113 0.4435 sec/batch\n",
      "Epoch 1815/2000  Iteration 3629/4000 Training loss: 0.0206 0.4438 sec/batch\n",
      "Epoch 1815/2000  Iteration 3630/4000 Training loss: 0.0110 0.4435 sec/batch\n",
      "Epoch 1816/2000  Iteration 3631/4000 Training loss: 0.0216 0.4434 sec/batch\n",
      "Epoch 1816/2000  Iteration 3632/4000 Training loss: 0.0112 0.4435 sec/batch\n",
      "Epoch 1817/2000  Iteration 3633/4000 Training loss: 0.0217 0.4433 sec/batch\n",
      "Epoch 1817/2000  Iteration 3634/4000 Training loss: 0.0114 0.4434 sec/batch\n",
      "Epoch 1818/2000  Iteration 3635/4000 Training loss: 0.0212 0.4434 sec/batch\n",
      "Epoch 1818/2000  Iteration 3636/4000 Training loss: 0.0110 0.4436 sec/batch\n",
      "Epoch 1819/2000  Iteration 3637/4000 Training loss: 0.0212 0.4429 sec/batch\n",
      "Epoch 1819/2000  Iteration 3638/4000 Training loss: 0.0112 0.4439 sec/batch\n",
      "Epoch 1820/2000  Iteration 3639/4000 Training loss: 0.0210 0.4432 sec/batch\n",
      "Epoch 1820/2000  Iteration 3640/4000 Training loss: 0.0109 0.4436 sec/batch\n",
      "Epoch 1821/2000  Iteration 3641/4000 Training loss: 0.0219 0.4436 sec/batch\n",
      "Epoch 1821/2000  Iteration 3642/4000 Training loss: 0.0113 0.4433 sec/batch\n",
      "Epoch 1822/2000  Iteration 3643/4000 Training loss: 0.0215 0.4432 sec/batch\n",
      "Epoch 1822/2000  Iteration 3644/4000 Training loss: 0.0111 0.4438 sec/batch\n",
      "Epoch 1823/2000  Iteration 3645/4000 Training loss: 0.0217 0.4432 sec/batch\n",
      "Epoch 1823/2000  Iteration 3646/4000 Training loss: 0.0113 0.4438 sec/batch\n",
      "Epoch 1824/2000  Iteration 3647/4000 Training loss: 0.0206 0.4435 sec/batch\n",
      "Epoch 1824/2000  Iteration 3648/4000 Training loss: 0.0106 0.4434 sec/batch\n",
      "Epoch 1825/2000  Iteration 3649/4000 Training loss: 0.0217 0.4428 sec/batch\n",
      "Epoch 1825/2000  Iteration 3650/4000 Training loss: 0.0112 0.4436 sec/batch\n",
      "Epoch 1826/2000  Iteration 3651/4000 Training loss: 0.0205 0.4436 sec/batch\n",
      "Epoch 1826/2000  Iteration 3652/4000 Training loss: 0.0106 0.4437 sec/batch\n",
      "Epoch 1827/2000  Iteration 3653/4000 Training loss: 0.0209 0.4431 sec/batch\n",
      "Epoch 1827/2000  Iteration 3654/4000 Training loss: 0.0108 0.4432 sec/batch\n",
      "Epoch 1828/2000  Iteration 3655/4000 Training loss: 0.0215 0.4430 sec/batch\n",
      "Epoch 1828/2000  Iteration 3656/4000 Training loss: 0.0111 0.4435 sec/batch\n",
      "Epoch 1829/2000  Iteration 3657/4000 Training loss: 0.0218 0.4434 sec/batch\n",
      "Epoch 1829/2000  Iteration 3658/4000 Training loss: 0.0112 0.4430 sec/batch\n",
      "Epoch 1830/2000  Iteration 3659/4000 Training loss: 0.0215 0.4434 sec/batch\n",
      "Epoch 1830/2000  Iteration 3660/4000 Training loss: 0.0112 0.4438 sec/batch\n",
      "Epoch 1831/2000  Iteration 3661/4000 Training loss: 0.0205 0.4439 sec/batch\n",
      "Epoch 1831/2000  Iteration 3662/4000 Training loss: 0.0106 0.4442 sec/batch\n",
      "Epoch 1832/2000  Iteration 3663/4000 Training loss: 0.0214 0.4438 sec/batch\n",
      "Epoch 1832/2000  Iteration 3664/4000 Training loss: 0.0111 0.4438 sec/batch\n",
      "Epoch 1833/2000  Iteration 3665/4000 Training loss: 0.0204 0.4436 sec/batch\n",
      "Epoch 1833/2000  Iteration 3666/4000 Training loss: 0.0105 0.4438 sec/batch\n",
      "Epoch 1834/2000  Iteration 3667/4000 Training loss: 0.0212 0.4438 sec/batch\n",
      "Epoch 1834/2000  Iteration 3668/4000 Training loss: 0.0110 0.4443 sec/batch\n",
      "Epoch 1835/2000  Iteration 3669/4000 Training loss: 0.0217 0.4438 sec/batch\n",
      "Epoch 1835/2000  Iteration 3670/4000 Training loss: 0.0112 0.4440 sec/batch\n",
      "Epoch 1836/2000  Iteration 3671/4000 Training loss: 0.0209 0.4441 sec/batch\n",
      "Epoch 1836/2000  Iteration 3672/4000 Training loss: 0.0108 0.4448 sec/batch\n",
      "Epoch 1837/2000  Iteration 3673/4000 Training loss: 0.0207 0.4435 sec/batch\n",
      "Epoch 1837/2000  Iteration 3674/4000 Training loss: 0.0107 0.4438 sec/batch\n",
      "Epoch 1838/2000  Iteration 3675/4000 Training loss: 0.0216 0.4433 sec/batch\n",
      "Epoch 1838/2000  Iteration 3676/4000 Training loss: 0.0112 0.4439 sec/batch\n",
      "Epoch 1839/2000  Iteration 3677/4000 Training loss: 0.0210 0.4440 sec/batch\n",
      "Epoch 1839/2000  Iteration 3678/4000 Training loss: 0.0108 0.4441 sec/batch\n",
      "Epoch 1840/2000  Iteration 3679/4000 Training loss: 0.0221 0.4439 sec/batch\n",
      "Epoch 1840/2000  Iteration 3680/4000 Training loss: 0.0114 0.4440 sec/batch\n",
      "Epoch 1841/2000  Iteration 3681/4000 Training loss: 0.0207 0.4437 sec/batch\n",
      "Epoch 1841/2000  Iteration 3682/4000 Training loss: 0.0106 0.4442 sec/batch\n",
      "Epoch 1842/2000  Iteration 3683/4000 Training loss: 0.0204 0.4430 sec/batch\n",
      "Epoch 1842/2000  Iteration 3684/4000 Training loss: 0.0105 0.4434 sec/batch\n",
      "Epoch 1843/2000  Iteration 3685/4000 Training loss: 0.0223 0.4435 sec/batch\n",
      "Epoch 1843/2000  Iteration 3686/4000 Training loss: 0.0114 0.4435 sec/batch\n",
      "Epoch 1844/2000  Iteration 3687/4000 Training loss: 0.0215 0.4433 sec/batch\n",
      "Epoch 1844/2000  Iteration 3688/4000 Training loss: 0.0110 0.4433 sec/batch\n",
      "Epoch 1845/2000  Iteration 3689/4000 Training loss: 0.0218 0.4435 sec/batch\n",
      "Epoch 1845/2000  Iteration 3690/4000 Training loss: 0.0111 0.4435 sec/batch\n",
      "Epoch 1846/2000  Iteration 3691/4000 Training loss: 0.0213 0.4431 sec/batch\n",
      "Epoch 1846/2000  Iteration 3692/4000 Training loss: 0.0109 0.4437 sec/batch\n",
      "Epoch 1847/2000  Iteration 3693/4000 Training loss: 0.0204 0.4435 sec/batch\n",
      "Epoch 1847/2000  Iteration 3694/4000 Training loss: 0.0104 0.4433 sec/batch\n",
      "Epoch 1848/2000  Iteration 3695/4000 Training loss: 0.0218 0.4434 sec/batch\n",
      "Epoch 1848/2000  Iteration 3696/4000 Training loss: 0.0112 0.4443 sec/batch\n",
      "Epoch 1849/2000  Iteration 3697/4000 Training loss: 0.0219 0.4434 sec/batch\n",
      "Epoch 1849/2000  Iteration 3698/4000 Training loss: 0.0112 0.4435 sec/batch\n",
      "Epoch 1850/2000  Iteration 3699/4000 Training loss: 0.0211 0.4430 sec/batch\n",
      "Epoch 1850/2000  Iteration 3700/4000 Training loss: 0.0108 0.4435 sec/batch\n",
      "Epoch 1851/2000  Iteration 3701/4000 Training loss: 0.0208 0.4430 sec/batch\n",
      "Epoch 1851/2000  Iteration 3702/4000 Training loss: 0.0108 0.4437 sec/batch\n",
      "Epoch 1852/2000  Iteration 3703/4000 Training loss: 0.0211 0.4432 sec/batch\n",
      "Epoch 1852/2000  Iteration 3704/4000 Training loss: 0.0108 0.4435 sec/batch\n",
      "Epoch 1853/2000  Iteration 3705/4000 Training loss: 0.0202 0.4431 sec/batch\n",
      "Epoch 1853/2000  Iteration 3706/4000 Training loss: 0.0104 0.4439 sec/batch\n",
      "Epoch 1854/2000  Iteration 3707/4000 Training loss: 0.0196 0.4432 sec/batch\n",
      "Epoch 1854/2000  Iteration 3708/4000 Training loss: 0.0101 0.4437 sec/batch\n",
      "Epoch 1855/2000  Iteration 3709/4000 Training loss: 0.0213 0.4432 sec/batch\n",
      "Epoch 1855/2000  Iteration 3710/4000 Training loss: 0.0108 0.4435 sec/batch\n",
      "Epoch 1856/2000  Iteration 3711/4000 Training loss: 0.0212 0.4433 sec/batch\n",
      "Epoch 1856/2000  Iteration 3712/4000 Training loss: 0.0109 0.4437 sec/batch\n",
      "Epoch 1857/2000  Iteration 3713/4000 Training loss: 0.0197 0.4431 sec/batch\n",
      "Epoch 1857/2000  Iteration 3714/4000 Training loss: 0.0101 0.4436 sec/batch\n",
      "Epoch 1858/2000  Iteration 3715/4000 Training loss: 0.0209 0.4432 sec/batch\n",
      "Epoch 1858/2000  Iteration 3716/4000 Training loss: 0.0108 0.4434 sec/batch\n",
      "Epoch 1859/2000  Iteration 3717/4000 Training loss: 0.0210 0.4432 sec/batch\n",
      "Epoch 1859/2000  Iteration 3718/4000 Training loss: 0.0109 0.4437 sec/batch\n",
      "Epoch 1860/2000  Iteration 3719/4000 Training loss: 0.0205 0.4434 sec/batch\n",
      "Epoch 1860/2000  Iteration 3720/4000 Training loss: 0.0105 0.4434 sec/batch\n",
      "Epoch 1861/2000  Iteration 3721/4000 Training loss: 0.0211 0.4433 sec/batch\n",
      "Epoch 1861/2000  Iteration 3722/4000 Training loss: 0.0108 0.4433 sec/batch\n",
      "Epoch 1862/2000  Iteration 3723/4000 Training loss: 0.0223 0.4434 sec/batch\n",
      "Epoch 1862/2000  Iteration 3724/4000 Training loss: 0.0114 0.4432 sec/batch\n",
      "Epoch 1863/2000  Iteration 3725/4000 Training loss: 0.0209 0.4435 sec/batch\n",
      "Epoch 1863/2000  Iteration 3726/4000 Training loss: 0.0107 0.4435 sec/batch\n",
      "Epoch 1864/2000  Iteration 3727/4000 Training loss: 0.0208 0.4434 sec/batch\n",
      "Epoch 1864/2000  Iteration 3728/4000 Training loss: 0.0106 0.4437 sec/batch\n",
      "Epoch 1865/2000  Iteration 3729/4000 Training loss: 0.0217 0.4431 sec/batch\n",
      "Epoch 1865/2000  Iteration 3730/4000 Training loss: 0.0112 0.4433 sec/batch\n",
      "Epoch 1866/2000  Iteration 3731/4000 Training loss: 0.0212 0.4434 sec/batch\n",
      "Epoch 1866/2000  Iteration 3732/4000 Training loss: 0.0108 0.4437 sec/batch\n",
      "Epoch 1867/2000  Iteration 3733/4000 Training loss: 0.0219 0.4435 sec/batch\n",
      "Epoch 1867/2000  Iteration 3734/4000 Training loss: 0.0111 0.4438 sec/batch\n",
      "Epoch 1868/2000  Iteration 3735/4000 Training loss: 0.0209 0.4430 sec/batch\n",
      "Epoch 1868/2000  Iteration 3736/4000 Training loss: 0.0107 0.4432 sec/batch\n",
      "Epoch 1869/2000  Iteration 3737/4000 Training loss: 0.0213 0.4428 sec/batch\n",
      "Epoch 1869/2000  Iteration 3738/4000 Training loss: 0.0108 0.4435 sec/batch\n",
      "Epoch 1870/2000  Iteration 3739/4000 Training loss: 0.0211 0.4435 sec/batch\n",
      "Epoch 1870/2000  Iteration 3740/4000 Training loss: 0.0108 0.4434 sec/batch\n",
      "Epoch 1871/2000  Iteration 3741/4000 Training loss: 0.0220 0.4435 sec/batch\n",
      "Epoch 1871/2000  Iteration 3742/4000 Training loss: 0.0112 0.4434 sec/batch\n",
      "Epoch 1872/2000  Iteration 3743/4000 Training loss: 0.0209 0.4431 sec/batch\n",
      "Epoch 1872/2000  Iteration 3744/4000 Training loss: 0.0107 0.4440 sec/batch\n",
      "Epoch 1873/2000  Iteration 3745/4000 Training loss: 0.0200 0.4437 sec/batch\n",
      "Epoch 1873/2000  Iteration 3746/4000 Training loss: 0.0102 0.4435 sec/batch\n",
      "Epoch 1874/2000  Iteration 3747/4000 Training loss: 0.0210 0.4433 sec/batch\n",
      "Epoch 1874/2000  Iteration 3748/4000 Training loss: 0.0107 0.4433 sec/batch\n",
      "Epoch 1875/2000  Iteration 3749/4000 Training loss: 0.0201 0.4431 sec/batch\n",
      "Epoch 1875/2000  Iteration 3750/4000 Training loss: 0.0103 0.4433 sec/batch\n",
      "Epoch 1876/2000  Iteration 3751/4000 Training loss: 0.0217 0.4437 sec/batch\n",
      "Epoch 1876/2000  Iteration 3752/4000 Training loss: 0.0111 0.4439 sec/batch\n",
      "Epoch 1877/2000  Iteration 3753/4000 Training loss: 0.0201 0.4435 sec/batch\n",
      "Epoch 1877/2000  Iteration 3754/4000 Training loss: 0.0102 0.4443 sec/batch\n",
      "Epoch 1878/2000  Iteration 3755/4000 Training loss: 0.0208 0.4438 sec/batch\n",
      "Epoch 1878/2000  Iteration 3756/4000 Training loss: 0.0106 0.4442 sec/batch\n",
      "Epoch 1879/2000  Iteration 3757/4000 Training loss: 0.0209 0.4463 sec/batch\n",
      "Epoch 1879/2000  Iteration 3758/4000 Training loss: 0.0106 0.4437 sec/batch\n",
      "Epoch 1880/2000  Iteration 3759/4000 Training loss: 0.0209 0.4434 sec/batch\n",
      "Epoch 1880/2000  Iteration 3760/4000 Training loss: 0.0107 0.4456 sec/batch\n",
      "Epoch 1881/2000  Iteration 3761/4000 Training loss: 0.0203 0.4441 sec/batch\n",
      "Epoch 1881/2000  Iteration 3762/4000 Training loss: 0.0104 0.4436 sec/batch\n",
      "Epoch 1882/2000  Iteration 3763/4000 Training loss: 0.0208 0.4441 sec/batch\n",
      "Epoch 1882/2000  Iteration 3764/4000 Training loss: 0.0106 0.4433 sec/batch\n",
      "Epoch 1883/2000  Iteration 3765/4000 Training loss: 0.0202 0.4440 sec/batch\n",
      "Epoch 1883/2000  Iteration 3766/4000 Training loss: 0.0104 0.4439 sec/batch\n",
      "Epoch 1884/2000  Iteration 3767/4000 Training loss: 0.0222 0.4435 sec/batch\n",
      "Epoch 1884/2000  Iteration 3768/4000 Training loss: 0.0112 0.4435 sec/batch\n",
      "Epoch 1885/2000  Iteration 3769/4000 Training loss: 0.0206 0.4441 sec/batch\n",
      "Epoch 1885/2000  Iteration 3770/4000 Training loss: 0.0105 0.4438 sec/batch\n",
      "Epoch 1886/2000  Iteration 3771/4000 Training loss: 0.0195 0.4430 sec/batch\n",
      "Epoch 1886/2000  Iteration 3772/4000 Training loss: 0.0099 0.4436 sec/batch\n",
      "Epoch 1887/2000  Iteration 3773/4000 Training loss: 0.0215 0.4433 sec/batch\n",
      "Epoch 1887/2000  Iteration 3774/4000 Training loss: 0.0110 0.4434 sec/batch\n",
      "Epoch 1888/2000  Iteration 3775/4000 Training loss: 0.0214 0.4432 sec/batch\n",
      "Epoch 1888/2000  Iteration 3776/4000 Training loss: 0.0109 0.4430 sec/batch\n",
      "Epoch 1889/2000  Iteration 3777/4000 Training loss: 0.0210 0.4436 sec/batch\n",
      "Epoch 1889/2000  Iteration 3778/4000 Training loss: 0.0108 0.4439 sec/batch\n",
      "Epoch 1890/2000  Iteration 3779/4000 Training loss: 0.0202 0.4434 sec/batch\n",
      "Epoch 1890/2000  Iteration 3780/4000 Training loss: 0.0103 0.4439 sec/batch\n",
      "Epoch 1891/2000  Iteration 3781/4000 Training loss: 0.0217 0.4432 sec/batch\n",
      "Epoch 1891/2000  Iteration 3782/4000 Training loss: 0.0110 0.4438 sec/batch\n",
      "Epoch 1892/2000  Iteration 3783/4000 Training loss: 0.0210 0.4435 sec/batch\n",
      "Epoch 1892/2000  Iteration 3784/4000 Training loss: 0.0107 0.4439 sec/batch\n",
      "Epoch 1893/2000  Iteration 3785/4000 Training loss: 0.0202 0.4433 sec/batch\n",
      "Epoch 1893/2000  Iteration 3786/4000 Training loss: 0.0103 0.4437 sec/batch\n",
      "Epoch 1894/2000  Iteration 3787/4000 Training loss: 0.0206 0.4433 sec/batch\n",
      "Epoch 1894/2000  Iteration 3788/4000 Training loss: 0.0105 0.4436 sec/batch\n",
      "Epoch 1895/2000  Iteration 3789/4000 Training loss: 0.0207 0.4432 sec/batch\n",
      "Epoch 1895/2000  Iteration 3790/4000 Training loss: 0.0105 0.4434 sec/batch\n",
      "Epoch 1896/2000  Iteration 3791/4000 Training loss: 0.0204 0.4434 sec/batch\n",
      "Epoch 1896/2000  Iteration 3792/4000 Training loss: 0.0105 0.4436 sec/batch\n",
      "Epoch 1897/2000  Iteration 3793/4000 Training loss: 0.0205 0.4438 sec/batch\n",
      "Epoch 1897/2000  Iteration 3794/4000 Training loss: 0.0104 0.4437 sec/batch\n",
      "Epoch 1898/2000  Iteration 3795/4000 Training loss: 0.0215 0.4433 sec/batch\n",
      "Epoch 1898/2000  Iteration 3796/4000 Training loss: 0.0109 0.4434 sec/batch\n",
      "Epoch 1899/2000  Iteration 3797/4000 Training loss: 0.0204 0.4432 sec/batch\n",
      "Epoch 1899/2000  Iteration 3798/4000 Training loss: 0.0104 0.4439 sec/batch\n",
      "Epoch 1900/2000  Iteration 3799/4000 Training loss: 0.0209 0.4432 sec/batch\n",
      "Epoch 1900/2000  Iteration 3800/4000 Training loss: 0.0106 0.4432 sec/batch\n",
      "Validation loss: 6.64837 Saving checkpoint!\n",
      "Epoch 1901/2000  Iteration 3801/4000 Training loss: 0.0204 0.4438 sec/batch\n",
      "Epoch 1901/2000  Iteration 3802/4000 Training loss: 0.0104 0.4437 sec/batch\n",
      "Epoch 1902/2000  Iteration 3803/4000 Training loss: 0.0211 0.4429 sec/batch\n",
      "Epoch 1902/2000  Iteration 3804/4000 Training loss: 0.0107 0.4436 sec/batch\n",
      "Epoch 1903/2000  Iteration 3805/4000 Training loss: 0.0204 0.4429 sec/batch\n",
      "Epoch 1903/2000  Iteration 3806/4000 Training loss: 0.0104 0.4433 sec/batch\n",
      "Epoch 1904/2000  Iteration 3807/4000 Training loss: 0.0205 0.4431 sec/batch\n",
      "Epoch 1904/2000  Iteration 3808/4000 Training loss: 0.0104 0.4460 sec/batch\n",
      "Epoch 1905/2000  Iteration 3809/4000 Training loss: 0.0208 0.4434 sec/batch\n",
      "Epoch 1905/2000  Iteration 3810/4000 Training loss: 0.0105 0.4433 sec/batch\n",
      "Epoch 1906/2000  Iteration 3811/4000 Training loss: 0.0202 0.4429 sec/batch\n",
      "Epoch 1906/2000  Iteration 3812/4000 Training loss: 0.0103 0.4437 sec/batch\n",
      "Epoch 1907/2000  Iteration 3813/4000 Training loss: 0.0208 0.4431 sec/batch\n",
      "Epoch 1907/2000  Iteration 3814/4000 Training loss: 0.0106 0.4435 sec/batch\n",
      "Epoch 1908/2000  Iteration 3815/4000 Training loss: 0.0214 0.4435 sec/batch\n",
      "Epoch 1908/2000  Iteration 3816/4000 Training loss: 0.0108 0.4432 sec/batch\n",
      "Epoch 1909/2000  Iteration 3817/4000 Training loss: 0.0210 0.4434 sec/batch\n",
      "Epoch 1909/2000  Iteration 3818/4000 Training loss: 0.0107 0.4433 sec/batch\n",
      "Epoch 1910/2000  Iteration 3819/4000 Training loss: 0.0203 0.4435 sec/batch\n",
      "Epoch 1910/2000  Iteration 3820/4000 Training loss: 0.0103 0.4435 sec/batch\n",
      "Epoch 1911/2000  Iteration 3821/4000 Training loss: 0.0207 0.4435 sec/batch\n",
      "Epoch 1911/2000  Iteration 3822/4000 Training loss: 0.0105 0.4431 sec/batch\n",
      "Epoch 1912/2000  Iteration 3823/4000 Training loss: 0.0207 0.4436 sec/batch\n",
      "Epoch 1912/2000  Iteration 3824/4000 Training loss: 0.0105 0.4436 sec/batch\n",
      "Epoch 1913/2000  Iteration 3825/4000 Training loss: 0.0212 0.4435 sec/batch\n",
      "Epoch 1913/2000  Iteration 3826/4000 Training loss: 0.0107 0.4436 sec/batch\n",
      "Epoch 1914/2000  Iteration 3827/4000 Training loss: 0.0210 0.4430 sec/batch\n",
      "Epoch 1914/2000  Iteration 3828/4000 Training loss: 0.0107 0.4432 sec/batch\n",
      "Epoch 1915/2000  Iteration 3829/4000 Training loss: 0.0207 0.4433 sec/batch\n",
      "Epoch 1915/2000  Iteration 3830/4000 Training loss: 0.0105 0.4433 sec/batch\n",
      "Epoch 1916/2000  Iteration 3831/4000 Training loss: 0.0202 0.4434 sec/batch\n",
      "Epoch 1916/2000  Iteration 3832/4000 Training loss: 0.0102 0.4437 sec/batch\n",
      "Epoch 1917/2000  Iteration 3833/4000 Training loss: 0.0224 0.4433 sec/batch\n",
      "Epoch 1917/2000  Iteration 3834/4000 Training loss: 0.0113 0.4436 sec/batch\n",
      "Epoch 1918/2000  Iteration 3835/4000 Training loss: 0.0205 0.4434 sec/batch\n",
      "Epoch 1918/2000  Iteration 3836/4000 Training loss: 0.0104 0.4435 sec/batch\n",
      "Epoch 1919/2000  Iteration 3837/4000 Training loss: 0.0201 0.4432 sec/batch\n",
      "Epoch 1919/2000  Iteration 3838/4000 Training loss: 0.0102 0.4434 sec/batch\n",
      "Epoch 1920/2000  Iteration 3839/4000 Training loss: 0.0209 0.4434 sec/batch\n",
      "Epoch 1920/2000  Iteration 3840/4000 Training loss: 0.0107 0.4430 sec/batch\n",
      "Epoch 1921/2000  Iteration 3841/4000 Training loss: 0.0196 0.4433 sec/batch\n",
      "Epoch 1921/2000  Iteration 3842/4000 Training loss: 0.0100 0.4435 sec/batch\n",
      "Epoch 1922/2000  Iteration 3843/4000 Training loss: 0.0205 0.4431 sec/batch\n",
      "Epoch 1922/2000  Iteration 3844/4000 Training loss: 0.0104 0.4434 sec/batch\n",
      "Epoch 1923/2000  Iteration 3845/4000 Training loss: 0.0203 0.4432 sec/batch\n",
      "Epoch 1923/2000  Iteration 3846/4000 Training loss: 0.0103 0.4433 sec/batch\n",
      "Epoch 1924/2000  Iteration 3847/4000 Training loss: 0.0209 0.4430 sec/batch\n",
      "Epoch 1924/2000  Iteration 3848/4000 Training loss: 0.0106 0.4434 sec/batch\n",
      "Epoch 1925/2000  Iteration 3849/4000 Training loss: 0.0202 0.4426 sec/batch\n",
      "Epoch 1925/2000  Iteration 3850/4000 Training loss: 0.0102 0.4436 sec/batch\n",
      "Epoch 1926/2000  Iteration 3851/4000 Training loss: 0.0205 0.4432 sec/batch\n",
      "Epoch 1926/2000  Iteration 3852/4000 Training loss: 0.0104 0.4437 sec/batch\n",
      "Epoch 1927/2000  Iteration 3853/4000 Training loss: 0.0219 0.4433 sec/batch\n",
      "Epoch 1927/2000  Iteration 3854/4000 Training loss: 0.0112 0.4436 sec/batch\n",
      "Epoch 1928/2000  Iteration 3855/4000 Training loss: 0.0207 0.4428 sec/batch\n",
      "Epoch 1928/2000  Iteration 3856/4000 Training loss: 0.0105 0.4431 sec/batch\n",
      "Epoch 1929/2000  Iteration 3857/4000 Training loss: 0.0216 0.4430 sec/batch\n",
      "Epoch 1929/2000  Iteration 3858/4000 Training loss: 0.0110 0.4431 sec/batch\n",
      "Epoch 1930/2000  Iteration 3859/4000 Training loss: 0.0207 0.4429 sec/batch\n",
      "Epoch 1930/2000  Iteration 3860/4000 Training loss: 0.0105 0.4436 sec/batch\n",
      "Epoch 1931/2000  Iteration 3861/4000 Training loss: 0.0206 0.4431 sec/batch\n",
      "Epoch 1931/2000  Iteration 3862/4000 Training loss: 0.0104 0.4435 sec/batch\n",
      "Epoch 1932/2000  Iteration 3863/4000 Training loss: 0.0192 0.4429 sec/batch\n",
      "Epoch 1932/2000  Iteration 3864/4000 Training loss: 0.0097 0.4435 sec/batch\n",
      "Epoch 1933/2000  Iteration 3865/4000 Training loss: 0.0220 0.4432 sec/batch\n",
      "Epoch 1933/2000  Iteration 3866/4000 Training loss: 0.0112 0.4438 sec/batch\n",
      "Epoch 1934/2000  Iteration 3867/4000 Training loss: 0.0210 0.4441 sec/batch\n",
      "Epoch 1934/2000  Iteration 3868/4000 Training loss: 0.0107 0.4436 sec/batch\n",
      "Epoch 1935/2000  Iteration 3869/4000 Training loss: 0.0200 0.4428 sec/batch\n",
      "Epoch 1935/2000  Iteration 3870/4000 Training loss: 0.0102 0.4430 sec/batch\n",
      "Epoch 1936/2000  Iteration 3871/4000 Training loss: 0.0208 0.4431 sec/batch\n",
      "Epoch 1936/2000  Iteration 3872/4000 Training loss: 0.0106 0.4437 sec/batch\n",
      "Epoch 1937/2000  Iteration 3873/4000 Training loss: 0.0199 0.4435 sec/batch\n",
      "Epoch 1937/2000  Iteration 3874/4000 Training loss: 0.0101 0.4431 sec/batch\n",
      "Epoch 1938/2000  Iteration 3875/4000 Training loss: 0.0205 0.4432 sec/batch\n",
      "Epoch 1938/2000  Iteration 3876/4000 Training loss: 0.0104 0.4434 sec/batch\n",
      "Epoch 1939/2000  Iteration 3877/4000 Training loss: 0.0210 0.4430 sec/batch\n",
      "Epoch 1939/2000  Iteration 3878/4000 Training loss: 0.0106 0.4436 sec/batch\n",
      "Epoch 1940/2000  Iteration 3879/4000 Training loss: 0.0211 0.4433 sec/batch\n",
      "Epoch 1940/2000  Iteration 3880/4000 Training loss: 0.0107 0.4433 sec/batch\n",
      "Epoch 1941/2000  Iteration 3881/4000 Training loss: 0.0207 0.4432 sec/batch\n",
      "Epoch 1941/2000  Iteration 3882/4000 Training loss: 0.0105 0.4435 sec/batch\n",
      "Epoch 1942/2000  Iteration 3883/4000 Training loss: 0.0203 0.4433 sec/batch\n",
      "Epoch 1942/2000  Iteration 3884/4000 Training loss: 0.0103 0.4437 sec/batch\n",
      "Epoch 1943/2000  Iteration 3885/4000 Training loss: 0.0209 0.4429 sec/batch\n",
      "Epoch 1943/2000  Iteration 3886/4000 Training loss: 0.0106 0.4438 sec/batch\n",
      "Epoch 1944/2000  Iteration 3887/4000 Training loss: 0.0195 0.4433 sec/batch\n",
      "Epoch 1944/2000  Iteration 3888/4000 Training loss: 0.0099 0.4436 sec/batch\n",
      "Epoch 1945/2000  Iteration 3889/4000 Training loss: 0.0207 0.4465 sec/batch\n",
      "Epoch 1945/2000  Iteration 3890/4000 Training loss: 0.0105 0.4459 sec/batch\n",
      "Epoch 1946/2000  Iteration 3891/4000 Training loss: 0.0206 0.4434 sec/batch\n",
      "Epoch 1946/2000  Iteration 3892/4000 Training loss: 0.0105 0.4434 sec/batch\n",
      "Epoch 1947/2000  Iteration 3893/4000 Training loss: 0.0211 0.4436 sec/batch\n",
      "Epoch 1947/2000  Iteration 3894/4000 Training loss: 0.0106 0.4437 sec/batch\n",
      "Epoch 1948/2000  Iteration 3895/4000 Training loss: 0.0199 0.4433 sec/batch\n",
      "Epoch 1948/2000  Iteration 3896/4000 Training loss: 0.0101 0.4438 sec/batch\n",
      "Epoch 1949/2000  Iteration 3897/4000 Training loss: 0.0215 0.4433 sec/batch\n",
      "Epoch 1949/2000  Iteration 3898/4000 Training loss: 0.0109 0.4434 sec/batch\n",
      "Epoch 1950/2000  Iteration 3899/4000 Training loss: 0.0211 0.4431 sec/batch\n",
      "Epoch 1950/2000  Iteration 3900/4000 Training loss: 0.0107 0.4434 sec/batch\n",
      "Epoch 1951/2000  Iteration 3901/4000 Training loss: 0.0203 0.4436 sec/batch\n",
      "Epoch 1951/2000  Iteration 3902/4000 Training loss: 0.0103 0.4436 sec/batch\n",
      "Epoch 1952/2000  Iteration 3903/4000 Training loss: 0.0216 0.4435 sec/batch\n",
      "Epoch 1952/2000  Iteration 3904/4000 Training loss: 0.0109 0.4435 sec/batch\n",
      "Epoch 1953/2000  Iteration 3905/4000 Training loss: 0.0209 0.4438 sec/batch\n",
      "Epoch 1953/2000  Iteration 3906/4000 Training loss: 0.0106 0.4438 sec/batch\n",
      "Epoch 1954/2000  Iteration 3907/4000 Training loss: 0.0208 0.4430 sec/batch\n",
      "Epoch 1954/2000  Iteration 3908/4000 Training loss: 0.0105 0.4434 sec/batch\n",
      "Epoch 1955/2000  Iteration 3909/4000 Training loss: 0.0190 0.4435 sec/batch\n",
      "Epoch 1955/2000  Iteration 3910/4000 Training loss: 0.0097 0.4431 sec/batch\n",
      "Epoch 1956/2000  Iteration 3911/4000 Training loss: 0.0202 0.4433 sec/batch\n",
      "Epoch 1956/2000  Iteration 3912/4000 Training loss: 0.0102 0.4438 sec/batch\n",
      "Epoch 1957/2000  Iteration 3913/4000 Training loss: 0.0205 0.4434 sec/batch\n",
      "Epoch 1957/2000  Iteration 3914/4000 Training loss: 0.0104 0.4435 sec/batch\n",
      "Epoch 1958/2000  Iteration 3915/4000 Training loss: 0.0204 0.4435 sec/batch\n",
      "Epoch 1958/2000  Iteration 3916/4000 Training loss: 0.0103 0.4436 sec/batch\n",
      "Epoch 1959/2000  Iteration 3917/4000 Training loss: 0.0207 0.4429 sec/batch\n",
      "Epoch 1959/2000  Iteration 3918/4000 Training loss: 0.0105 0.4437 sec/batch\n",
      "Epoch 1960/2000  Iteration 3919/4000 Training loss: 0.0209 0.4431 sec/batch\n",
      "Epoch 1960/2000  Iteration 3920/4000 Training loss: 0.0106 0.4437 sec/batch\n",
      "Epoch 1961/2000  Iteration 3921/4000 Training loss: 0.0197 0.4437 sec/batch\n",
      "Epoch 1961/2000  Iteration 3922/4000 Training loss: 0.0100 0.4439 sec/batch\n",
      "Epoch 1962/2000  Iteration 3923/4000 Training loss: 0.0203 0.4436 sec/batch\n",
      "Epoch 1962/2000  Iteration 3924/4000 Training loss: 0.0103 0.4436 sec/batch\n",
      "Epoch 1963/2000  Iteration 3925/4000 Training loss: 0.0204 0.4434 sec/batch\n",
      "Epoch 1963/2000  Iteration 3926/4000 Training loss: 0.0103 0.4433 sec/batch\n",
      "Epoch 1964/2000  Iteration 3927/4000 Training loss: 0.0200 0.4437 sec/batch\n",
      "Epoch 1964/2000  Iteration 3928/4000 Training loss: 0.0102 0.4438 sec/batch\n",
      "Epoch 1965/2000  Iteration 3929/4000 Training loss: 0.0208 0.4438 sec/batch\n",
      "Epoch 1965/2000  Iteration 3930/4000 Training loss: 0.0106 0.4434 sec/batch\n",
      "Epoch 1966/2000  Iteration 3931/4000 Training loss: 0.0199 0.4435 sec/batch\n",
      "Epoch 1966/2000  Iteration 3932/4000 Training loss: 0.0101 0.4438 sec/batch\n",
      "Epoch 1967/2000  Iteration 3933/4000 Training loss: 0.0191 0.4430 sec/batch\n",
      "Epoch 1967/2000  Iteration 3934/4000 Training loss: 0.0097 0.4433 sec/batch\n",
      "Epoch 1968/2000  Iteration 3935/4000 Training loss: 0.0211 0.4438 sec/batch\n",
      "Epoch 1968/2000  Iteration 3936/4000 Training loss: 0.0107 0.4437 sec/batch\n",
      "Epoch 1969/2000  Iteration 3937/4000 Training loss: 0.0200 0.4432 sec/batch\n",
      "Epoch 1969/2000  Iteration 3938/4000 Training loss: 0.0101 0.4437 sec/batch\n",
      "Epoch 1970/2000  Iteration 3939/4000 Training loss: 0.0204 0.4432 sec/batch\n",
      "Epoch 1970/2000  Iteration 3940/4000 Training loss: 0.0103 0.4434 sec/batch\n",
      "Epoch 1971/2000  Iteration 3941/4000 Training loss: 0.0201 0.4431 sec/batch\n",
      "Epoch 1971/2000  Iteration 3942/4000 Training loss: 0.0103 0.4437 sec/batch\n",
      "Epoch 1972/2000  Iteration 3943/4000 Training loss: 0.0197 0.4435 sec/batch\n",
      "Epoch 1972/2000  Iteration 3944/4000 Training loss: 0.0100 0.4441 sec/batch\n",
      "Epoch 1973/2000  Iteration 3945/4000 Training loss: 0.0205 0.4432 sec/batch\n",
      "Epoch 1973/2000  Iteration 3946/4000 Training loss: 0.0104 0.4438 sec/batch\n",
      "Epoch 1974/2000  Iteration 3947/4000 Training loss: 0.0216 0.4436 sec/batch\n",
      "Epoch 1974/2000  Iteration 3948/4000 Training loss: 0.0109 0.4435 sec/batch\n",
      "Epoch 1975/2000  Iteration 3949/4000 Training loss: 0.0203 0.4434 sec/batch\n",
      "Epoch 1975/2000  Iteration 3950/4000 Training loss: 0.0103 0.4468 sec/batch\n",
      "Epoch 1976/2000  Iteration 3951/4000 Training loss: 0.0203 0.4434 sec/batch\n",
      "Epoch 1976/2000  Iteration 3952/4000 Training loss: 0.0104 0.4436 sec/batch\n",
      "Epoch 1977/2000  Iteration 3953/4000 Training loss: 0.0207 0.4433 sec/batch\n",
      "Epoch 1977/2000  Iteration 3954/4000 Training loss: 0.0106 0.4432 sec/batch\n",
      "Epoch 1978/2000  Iteration 3955/4000 Training loss: 0.0207 0.4429 sec/batch\n",
      "Epoch 1978/2000  Iteration 3956/4000 Training loss: 0.0105 0.4431 sec/batch\n",
      "Epoch 1979/2000  Iteration 3957/4000 Training loss: 0.0202 0.4430 sec/batch\n",
      "Epoch 1979/2000  Iteration 3958/4000 Training loss: 0.0103 0.4433 sec/batch\n",
      "Epoch 1980/2000  Iteration 3959/4000 Training loss: 0.0204 0.4432 sec/batch\n",
      "Epoch 1980/2000  Iteration 3960/4000 Training loss: 0.0104 0.4437 sec/batch\n",
      "Epoch 1981/2000  Iteration 3961/4000 Training loss: 0.0203 0.4432 sec/batch\n",
      "Epoch 1981/2000  Iteration 3962/4000 Training loss: 0.0103 0.4431 sec/batch\n",
      "Epoch 1982/2000  Iteration 3963/4000 Training loss: 0.0194 0.4433 sec/batch\n",
      "Epoch 1982/2000  Iteration 3964/4000 Training loss: 0.0098 0.4436 sec/batch\n",
      "Epoch 1983/2000  Iteration 3965/4000 Training loss: 0.0205 0.4429 sec/batch\n",
      "Epoch 1983/2000  Iteration 3966/4000 Training loss: 0.0104 0.4435 sec/batch\n",
      "Epoch 1984/2000  Iteration 3967/4000 Training loss: 0.0197 0.4434 sec/batch\n",
      "Epoch 1984/2000  Iteration 3968/4000 Training loss: 0.0101 0.4436 sec/batch\n",
      "Epoch 1985/2000  Iteration 3969/4000 Training loss: 0.0208 0.4434 sec/batch\n",
      "Epoch 1985/2000  Iteration 3970/4000 Training loss: 0.0107 0.4434 sec/batch\n",
      "Epoch 1986/2000  Iteration 3971/4000 Training loss: 0.0198 0.4438 sec/batch\n",
      "Epoch 1986/2000  Iteration 3972/4000 Training loss: 0.0100 0.4435 sec/batch\n",
      "Epoch 1987/2000  Iteration 3973/4000 Training loss: 0.0204 0.4437 sec/batch\n",
      "Epoch 1987/2000  Iteration 3974/4000 Training loss: 0.0104 0.4437 sec/batch\n",
      "Epoch 1988/2000  Iteration 3975/4000 Training loss: 0.0204 0.4436 sec/batch\n",
      "Epoch 1988/2000  Iteration 3976/4000 Training loss: 0.0103 0.4431 sec/batch\n",
      "Epoch 1989/2000  Iteration 3977/4000 Training loss: 0.0204 0.4431 sec/batch\n",
      "Epoch 1989/2000  Iteration 3978/4000 Training loss: 0.0104 0.4437 sec/batch\n",
      "Epoch 1990/2000  Iteration 3979/4000 Training loss: 0.0198 0.4435 sec/batch\n",
      "Epoch 1990/2000  Iteration 3980/4000 Training loss: 0.0100 0.4436 sec/batch\n",
      "Epoch 1991/2000  Iteration 3981/4000 Training loss: 0.0195 0.4434 sec/batch\n",
      "Epoch 1991/2000  Iteration 3982/4000 Training loss: 0.0099 0.4436 sec/batch\n",
      "Epoch 1992/2000  Iteration 3983/4000 Training loss: 0.0206 0.4437 sec/batch\n",
      "Epoch 1992/2000  Iteration 3984/4000 Training loss: 0.0104 0.4432 sec/batch\n",
      "Epoch 1993/2000  Iteration 3985/4000 Training loss: 0.0203 0.4431 sec/batch\n",
      "Epoch 1993/2000  Iteration 3986/4000 Training loss: 0.0103 0.4437 sec/batch\n",
      "Epoch 1994/2000  Iteration 3987/4000 Training loss: 0.0198 0.4431 sec/batch\n",
      "Epoch 1994/2000  Iteration 3988/4000 Training loss: 0.0101 0.4433 sec/batch\n",
      "Epoch 1995/2000  Iteration 3989/4000 Training loss: 0.0201 0.4438 sec/batch\n",
      "Epoch 1995/2000  Iteration 3990/4000 Training loss: 0.0102 0.4433 sec/batch\n",
      "Epoch 1996/2000  Iteration 3991/4000 Training loss: 0.0208 0.4432 sec/batch\n",
      "Epoch 1996/2000  Iteration 3992/4000 Training loss: 0.0105 0.4436 sec/batch\n",
      "Epoch 1997/2000  Iteration 3993/4000 Training loss: 0.0208 0.4433 sec/batch\n",
      "Epoch 1997/2000  Iteration 3994/4000 Training loss: 0.0106 0.4431 sec/batch\n",
      "Epoch 1998/2000  Iteration 3995/4000 Training loss: 0.0202 0.4434 sec/batch\n",
      "Epoch 1998/2000  Iteration 3996/4000 Training loss: 0.0103 0.4435 sec/batch\n",
      "Epoch 1999/2000  Iteration 3997/4000 Training loss: 0.0204 0.4434 sec/batch\n",
      "Epoch 1999/2000  Iteration 3998/4000 Training loss: 0.0103 0.4433 sec/batch\n",
      "Epoch 2000/2000  Iteration 3999/4000 Training loss: 0.0200 0.4434 sec/batch\n",
      "Epoch 2000/2000  Iteration 4000/4000 Training loss: 0.0102 0.4433 sec/batch\n",
      "Validation loss: 6.74258 Saving checkpoint!\n"
     ]
    }
   ],
   "source": [
    "epochs = 2000\n",
    "# Save every N iterations\n",
    "save_every_n = 200\n",
    "train_x, train_y, val_x, val_y = split_data(chars, batch_size, num_steps)\n",
    "\n",
    "model = build_rnn(len(vocab), \n",
    "                  batch_size=batch_size,\n",
    "                  num_steps=num_steps,\n",
    "                  learning_rate=learning_rate,\n",
    "                  lstm_size=lstm_size,\n",
    "                  num_layers=num_layers)\n",
    "\n",
    "saver = tf.train.Saver(max_to_keep=100)\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    # Use the line below to load a checkpoint and resume training\n",
    "    #saver.restore(sess, 'checkpoints/______.ckpt')\n",
    "    \n",
    "    n_batches = int(train_x.shape[1]/num_steps)\n",
    "    iterations = n_batches * epochs\n",
    "    for e in range(epochs):\n",
    "        \n",
    "        # Train network\n",
    "        new_state = sess.run(model.initial_state)\n",
    "        loss = 0\n",
    "        for b, (x, y) in enumerate(get_batch([train_x, train_y], num_steps), 1):\n",
    "            iteration = e*n_batches + b\n",
    "            start = time.time()\n",
    "            feed = {model.inputs: x,\n",
    "                    model.targets: y,\n",
    "                    model.keep_prob: keep_prob,\n",
    "                    model.initial_state: new_state}\n",
    "            batch_loss, new_state, _ = sess.run([model.cost, model.final_state, model.optimizer], \n",
    "                                                 feed_dict=feed)\n",
    "            loss += batch_loss\n",
    "            end = time.time()\n",
    "            print('Epoch {}/{} '.format(e+1, epochs),\n",
    "                  'Iteration {}/{}'.format(iteration, iterations),\n",
    "                  'Training loss: {:.4f}'.format(loss/b),\n",
    "                  '{:.4f} sec/batch'.format((end-start)))\n",
    "        \n",
    "            \n",
    "            if (iteration%save_every_n == 0) or (iteration == iterations):\n",
    "                # Check performance, notice dropout has been set to 1\n",
    "                val_loss = []\n",
    "                new_state = sess.run(model.initial_state)\n",
    "                for x, y in get_batch([val_x, val_y], num_steps):\n",
    "                    feed = {model.inputs: x,\n",
    "                            model.targets: y,\n",
    "                            model.keep_prob: 1.,\n",
    "                            model.initial_state: new_state}\n",
    "                    batch_loss, new_state = sess.run([model.cost, model.final_state], feed_dict=feed)\n",
    "                    val_loss.append(batch_loss)\n",
    "\n",
    "                print('Validation loss:', np.mean(val_loss),\n",
    "                      'Saving checkpoint!')\n",
    "                saver.save(sess, \"checkpoints/i{}_l{}_v{:.3f}.ckpt\".format(iteration, lstm_size, np.mean(val_loss)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Saved checkpoints\n",
    "\n",
    "Read up on saving and loading checkpoints here: https://www.tensorflow.org/programmers_guide/variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "model_checkpoint_path: \"checkpoints/i4000_l512_v6.743.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/i200_l512_v2.275.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/i400_l512_v2.898.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/i600_l512_v4.508.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/i800_l512_v5.091.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/i1000_l512_v5.437.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/i1200_l512_v5.694.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/i1400_l512_v5.844.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/i1600_l512_v6.018.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/i1800_l512_v6.126.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/i2000_l512_v6.215.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/i2200_l512_v6.366.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/i2400_l512_v6.456.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/i2600_l512_v6.391.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/i2800_l512_v6.463.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/i3000_l512_v6.663.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/i3200_l512_v6.704.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/i3400_l512_v6.493.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/i3600_l512_v6.420.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/i3800_l512_v6.648.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/i4000_l512_v6.743.ckpt\""
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.train.get_checkpoint_state('checkpoints')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sampling\n",
    "\n",
    "Now that the network is trained, we'll can use it to generate new text. The idea is that we pass in a character, then the network will predict the next character. We can use the new one, to predict the next one. And we keep doing this to generate all new text. I also included some functionality to prime the network with some text by passing in a string and building up a state from that.\n",
    "\n",
    "The network gives us predictions for each character. To reduce noise and make things a little less random, I'm going to only choose a new character from the top N most likely characters.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def pick_top_n(preds, vocab_size, top_n=5):\n",
    "    p = np.squeeze(preds)\n",
    "    p[np.argsort(p)[:-top_n]] = 0\n",
    "    p = p / np.sum(p)\n",
    "    c = np.random.choice(vocab_size, 1, p=p)[0]\n",
    "    return c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sample(checkpoint, n_samples, lstm_size, vocab_size, prime=\"The \"):\n",
    "    samples = [c for c in prime]\n",
    "    model = build_rnn(vocab_size, lstm_size=lstm_size, sampling=True)\n",
    "    saver = tf.train.Saver()\n",
    "    with tf.Session() as sess:\n",
    "        saver.restore(sess, checkpoint)\n",
    "        new_state = sess.run(model.initial_state)\n",
    "        for c in prime:\n",
    "            x = np.zeros((1, 1))\n",
    "            x[0,0] = vocab_to_int[c]\n",
    "            feed = {model.inputs: x,\n",
    "                    model.keep_prob: 1.,\n",
    "                    model.initial_state: new_state}\n",
    "            preds, new_state = sess.run([model.preds, model.final_state], \n",
    "                                         feed_dict=feed)\n",
    "\n",
    "        c = pick_top_n(preds, len(vocab))\n",
    "        samples.append(int_to_vocab[c])\n",
    "\n",
    "        for i in range(n_samples):\n",
    "            x[0,0] = c\n",
    "            feed = {model.inputs: x,\n",
    "                    model.keep_prob: 1.,\n",
    "                    model.initial_state: new_state}\n",
    "            preds, new_state = sess.run([model.preds, model.final_state], \n",
    "                                         feed_dict=feed)\n",
    "\n",
    "            c = pick_top_n(preds, len(vocab))\n",
    "            samples.append(int_to_vocab[c])\n",
    "        \n",
    "    return ''.join(samples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, pass in the path to a checkpoint and sample from the network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Farting it neatly to the side as if he never gave up grooming habits from a more innocent youth. He could be mistaken for an exhausted salesman of a specialized product, his clientele expecting a certaice, tne cons of e whought I tain bet ar meay. \n",
      "I cens. I had not has ano hou dans, in it das forling I and another what in tha surely bad etored of the with to lent onde trep andered, but same of the seet anozy I and has do caiked in hind booked in thing tho extr atr acrust not he was expeating though wh se ading sime lo him. I cad a ficl shine ven co ment eve the some to bit  frecle lef an and somat e was geten of the nows parting the mase to he was treind to tear what I same to fe. Bt of re was not to and roal the spante on the shat fow at we lite let and becpare on the start with I gottront of my aure some has seen around foreal wede thile lence ondertande was evers of fainig reating mithl fareing. I cented he caits, the maghe some headqaare the date an myoon to metront in colo leve and atched soid to pist ar the sound and his plowe the pores of the seer atwed the sund beter the paster woml Iserant. I head me fain anderedound bouke aling. I kene mele on the now. I heurnd the vert on cratro that yea ders aid dost hid, hat beat conteider air ato his traje and anound to what we like something way ever couts into he me. chere the eart are I tauned at whas cons. Dorat ome tirt I car purtod of my sout wo hour then eelen ande pading val for shich of the so-ging of the segored the sact and the mally sad it. Eestyy haget art be twane the Doons wear, they then stround be the derticling the bagg ou ar anderes tho le. tne ont if had bet reath noft a weold valk at whous things wo dound to whith whoul noad?\" He lag he cansione on the stre. Hin alo he would and ats an I evound mo hind the darting that aswar-ut ift into the soed with the pagt: Datp remy, bes it anound gome arte tas anoy to site and the lagens what he was theen gere on me can, the feeling th in my rontaling rade the si\n"
     ]
    }
   ],
   "source": [
    "checkpoint = \"checkpoints/i4000_l512_v6.743.ckpt\"\n",
    "samp = sample(checkpoint, 2000, lstm_size, len(vocab), prime=\"Far\")\n",
    "print(samp)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:dl]",
   "language": "python",
   "name": "conda-env-dl-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
