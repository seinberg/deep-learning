{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# TV Script Generation\n",
    "In this project, you'll generate your own [Simpsons](https://en.wikipedia.org/wiki/The_Simpsons) TV scripts using RNNs.  You'll be using part of the [Simpsons dataset](https://www.kaggle.com/wcukierski/the-simpsons-by-the-data) of scripts from 27 seasons.  The Neural Network you'll build will generate a new TV script for a scene at [Moe's Tavern](https://simpsonswiki.com/wiki/Moe's_Tavern).\n",
    "## Get the Data\n",
    "The data is already provided for you.  You'll be using a subset of the original dataset.  It consists of only the scenes in Moe's Tavern.  This doesn't include other versions of the tavern, like \"Moe's Cavern\", \"Flaming Moe's\", \"Uncle Moe's Family Feed-Bag\", etc.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL\n",
    "\"\"\"\n",
    "import helper\n",
    "import random\n",
    "\n",
    "#data_dir = './data/simpsons/moes_tavern_lines.txt'\n",
    "#data_dir = './data/all/simpsons_all.csv'\n",
    "data_dir = './data/all/simpsons_norm_names_all.csv'\n",
    "text = helper.load_data(data_dir)\n",
    "# Ignore notice, since we don't use it for analysing the data\n",
    "text = text[81:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Explore the Data\n",
    "Play around with `view_sentence_range` to view different parts of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset Stats\n",
      "Roughly the number of unique words: 139226\n",
      "Number of scenes: 1\n",
      "Average number of sentences in each scene: 157491.0\n",
      "Number of lines: 157492\n",
      "Average number of words in each line: 10.649391715134737\n",
      "\n",
      "The sentences 0 to 10:\n",
      "n all the magazines and all the news shows, it's only natural that you think you have it.\n",
      "Lisa_Simpson: (NEAR TEARS) Where's Mr. Bergstrom?\n",
      "Miss_Hoover: I don't know. Although I'd sure like to talk to him. He didn't touch my lesson plan. What did he teach you?\n",
      "Lisa_Simpson: That life is worth living.\n",
      "Edna_Krabappel-Flanders: The polls will be open from now until the end of recess. Now, (SOUR) just in case any of you have decided to put any thought into this, we'll have our final statements. Martin?\n",
      "Martin_Prince: (HOARSE WHISPER) I don't think there's anything left to say.\n",
      "Edna_Krabappel-Flanders: Bart?\n",
      "Bart_Simpson: Victory party under the slide!\n",
      "(Apartment_Building: Ext. apartment building - day)\n",
      "Lisa_Simpson: (CALLING) Mr. Bergstrom! Mr. Bergstrom!\n"
     ]
    }
   ],
   "source": [
    "view_sentence_range = (0, 10)\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL\n",
    "\"\"\"\n",
    "import numpy as np\n",
    "\n",
    "print('Dataset Stats')\n",
    "print('Roughly the number of unique words: {}'.format(len({word: None for word in text.split()})))\n",
    "scenes = text.split('\\n\\n')\n",
    "print('Number of scenes: {}'.format(len(scenes)))\n",
    "sentence_count_scene = [scene.count('\\n') for scene in scenes]\n",
    "print('Average number of sentences in each scene: {}'.format(np.average(sentence_count_scene)))\n",
    "\n",
    "sentences = [sentence for scene in scenes for sentence in scene.split('\\n')]\n",
    "print('Number of lines: {}'.format(len(sentences)))\n",
    "word_count_sentence = [len(sentence.split()) for sentence in sentences]\n",
    "print('Average number of words in each line: {}'.format(np.average(word_count_sentence)))\n",
    "\n",
    "print()\n",
    "print('The sentences {} to {}:'.format(*view_sentence_range))\n",
    "print('\\n'.join(text.split('\\n')[view_sentence_range[0]:view_sentence_range[1]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Implement Preprocessing Functions\n",
    "The first thing to do to any dataset is preprocessing.  Implement the following preprocessing functions below:\n",
    "- Lookup Table\n",
    "- Tokenize Punctuation\n",
    "\n",
    "### Lookup Table\n",
    "To create a word embedding, you first need to transform the words to ids.  In this function, create two dictionaries:\n",
    "- Dictionary to go from the words to an id, we'll call `vocab_to_int`\n",
    "- Dictionary to go from the id to word, we'll call `int_to_vocab`\n",
    "\n",
    "Return these dictionaries in the following tuple `(vocab_to_int, int_to_vocab)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "71\n",
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from collections import Counter\n",
    "import problem_unittests as tests\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "def create_lookup_tables(text, min_count=1):\n",
    "    \"\"\"\n",
    "    Create lookup tables for vocabulary\n",
    "    :param text: The text of tv scripts split into words\n",
    "    :return: A tuple of dicts (vocab_to_int, int_to_vocab)\n",
    "    \"\"\"\n",
    "    words = text\n",
    "    #cv = CountVectorizer()\n",
    "    #vectorized = cv.fit_transform(text)\n",
    "    #print(vectorized)\n",
    "    word_counts = Counter(words)\n",
    "    #word_counts_2 = Counter(word_counts)\n",
    "\n",
    "    for k in list(word_counts):\n",
    "        if word_counts[k] < min_count:\n",
    "            del word_counts[k]\n",
    "    sorted_vocab = sorted(word_counts, key=word_counts.get, reverse=True)\n",
    "\n",
    "    int_to_vocab = {ii: word for ii, word in enumerate(sorted_vocab)}\n",
    "    vocab_to_int = {word: ii for ii, word in int_to_vocab.items()}\n",
    "    print(len(sorted_vocab))\n",
    "\n",
    "    return vocab_to_int, int_to_vocab\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_create_lookup_tables(create_lookup_tables)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Tokenize Punctuation\n",
    "We'll be splitting the script into a word array using spaces as delimiters.  However, punctuations like periods and exclamation marks make it hard for the neural network to distinguish between the word \"bye\" and \"bye!\".\n",
    "\n",
    "Implement the function `token_lookup` to return a dict that will be used to tokenize symbols like \"!\" into \"||Exclamation_Mark||\".  Create a dictionary for the following symbols where the symbol is the key and value is the token:\n",
    "- Period ( . )\n",
    "- Comma ( , )\n",
    "- Quotation Mark ( \" )\n",
    "- Semicolon ( ; )\n",
    "- Exclamation mark ( ! )\n",
    "- Question mark ( ? )\n",
    "- Left Parentheses ( ( )\n",
    "- Right Parentheses ( ) )\n",
    "- Dash ( -- )\n",
    "- Return ( \\n )\n",
    "\n",
    "This dictionary will be used to token the symbols and add the delimiter (space) around it.  This separates the symbols as it's own word, making it easier for the neural network to predict on the next word. Make sure you don't use a token that could be confused as a word. Instead of using the token \"dash\", try using something like \"||dash||\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def token_lookup():\n",
    "    \"\"\"\n",
    "    Generate a dict to turn punctuation into a token.\n",
    "    :return: Tokenize dictionary where the key is the punctuation and the value is the token\n",
    "    \"\"\"\n",
    "    return {\n",
    "        '.': '__period__',\n",
    "        ',': '__comma__',\n",
    "        '\"': '__double_quote__',\n",
    "        ';': '__semi-colon__',\n",
    "        '!': '__exclamation__',\n",
    "        '?': '__question__',\n",
    "        '(': '__open_paren__',\n",
    "        ')': '__close_paren__',\n",
    "        '--': '__dash__',\n",
    "        '\\n': '__endline__'\n",
    "    }\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_tokenize(token_lookup)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Preprocess all the data and save it\n",
    "Running the code cell below will preprocess all the data and save it to file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "62219\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL\n",
    "\"\"\"\n",
    "# Preprocess Training, Validation, and Testing Data\n",
    "helper.preprocess_and_save_data(data_dir, token_lookup, create_lookup_tables)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Check Point\n",
    "This is your first checkpoint. If you ever decide to come back to this notebook or have to restart the notebook, you can start from here. The preprocessed data has been saved to disk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL\n",
    "\"\"\"\n",
    "import helper\n",
    "import numpy as np\n",
    "import problem_unittests as tests\n",
    "\n",
    "int_text, vocab_to_int, int_to_vocab, token_dict = helper.load_preprocess()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Build the Neural Network\n",
    "You'll build the components necessary to build a RNN by implementing the following functions below:\n",
    "- get_inputs\n",
    "- get_init_cell\n",
    "- get_embed\n",
    "- build_rnn\n",
    "- build_nn\n",
    "- get_batches\n",
    "\n",
    "### Check the Version of TensorFlow and Access to GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow Version: 1.0.1\n",
      "Default GPU Device: /gpu:0\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL\n",
    "\"\"\"\n",
    "from distutils.version import LooseVersion\n",
    "import warnings\n",
    "import tensorflow as tf\n",
    "\n",
    "# Check TensorFlow Version\n",
    "assert LooseVersion(tf.__version__) >= LooseVersion('1.0'), 'Please use TensorFlow version 1.0 or newer'\n",
    "print('TensorFlow Version: {}'.format(tf.__version__))\n",
    "\n",
    "# Check for a GPU\n",
    "if not tf.test.gpu_device_name():\n",
    "    warnings.warn('No GPU found. Please use a GPU to train your neural network.')\n",
    "else:\n",
    "    print('Default GPU Device: {}'.format(tf.test.gpu_device_name()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Input\n",
    "Implement the `get_inputs()` function to create TF Placeholders for the Neural Network.  It should create the following placeholders:\n",
    "- Input text placeholder named \"input\" using the [TF Placeholder](https://www.tensorflow.org/api_docs/python/tf/placeholder) `name` parameter.\n",
    "- Targets placeholder\n",
    "- Learning Rate placeholder\n",
    "\n",
    "Return the placeholders in the following tuple `(Input, Targets, LearningRate)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def get_inputs():\n",
    "    \"\"\"\n",
    "    Create TF Placeholders for input, targets, and learning rate.\n",
    "    :return: Tuple (input, targets, learning rate)\n",
    "    \"\"\"\n",
    "    input_test = tf.placeholder(tf.int32, [None, None], name='input')\n",
    "    targets = tf.placeholder(tf.int32, [None, None], name='targets')\n",
    "    learning_rate = tf.placeholder(tf.float32, name='learning_rate')\n",
    "    return input_test, targets, learning_rate\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_get_inputs(get_inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Build RNN Cell and Initialize\n",
    "Stack one or more [`BasicLSTMCells`](https://www.tensorflow.org/api_docs/python/tf/contrib/rnn/BasicLSTMCell) in a [`MultiRNNCell`](https://www.tensorflow.org/api_docs/python/tf/contrib/rnn/MultiRNNCell).\n",
    "- The Rnn size should be set using `rnn_size`\n",
    "- Initalize Cell State using the MultiRNNCell's [`zero_state()`](https://www.tensorflow.org/api_docs/python/tf/contrib/rnn/MultiRNNCell#zero_state) function\n",
    "    - Apply the name \"initial_state\" to the initial state using [`tf.identity()`](https://www.tensorflow.org/api_docs/python/tf/identity)\n",
    "\n",
    "Return the cell and initial state in the following tuple `(Cell, InitialState)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def get_init_cell(batch_size, rnn_size, lstm_layers=1, keep_prob=1.0):\n",
    "    \"\"\"\n",
    "    Create an RNN Cell and initialize it.\n",
    "    :param batch_size: Size of batches\n",
    "    :param rnn_size: Size of RNNs\n",
    "    :param lstm_layers: Number of layers to apply to LSTM\n",
    "    :param keep_prob: Dropout keep probability for cell\n",
    "    :return: Tuple (cell, initialize state)\n",
    "    \"\"\"\n",
    "    # A basic LSTM cell\n",
    "    lstm = tf.contrib.rnn.BasicLSTMCell(rnn_size)\n",
    "\n",
    "    # Add dropout to the cell\n",
    "    dropout_wrapper = tf.contrib.rnn.DropoutWrapper(lstm, output_keep_prob=keep_prob)\n",
    "\n",
    "    # Stack up multiple LSTM layers, for deep learning\n",
    "    cell = tf.contrib.rnn.MultiRNNCell([dropout_wrapper] * lstm_layers)\n",
    "    \n",
    "    # Getting an initial state of all zeros\n",
    "    initial_state = tf.identity(cell.zero_state(batch_size, tf.float32), 'initial_state')\n",
    "    return cell, initial_state\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_get_init_cell(get_init_cell)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Word Embedding\n",
    "Apply embedding to `input_data` using TensorFlow.  Return the embedded sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def get_embed(input_data, vocab_size, embed_dim):\n",
    "    \"\"\"\n",
    "    Create embedding for <input_data>.\n",
    "    :param input_data: TF placeholder for text input.\n",
    "    :param vocab_size: Number of words in vocabulary.\n",
    "    :param embed_dim: Number of embedding dimensions\n",
    "    :return: Embedded input.\n",
    "    \"\"\"\n",
    "    #embedding = tf.Variable(tf.random_uniform((vocab_size, embed_dim), -1, 1))\n",
    "    #embed = tf.nn.embedding_lookup(embedding, input_data)\n",
    "    #return embed\n",
    "    # consider using:\n",
    "    return tf.contrib.layers.embed_sequence(\n",
    "        input_data, vocab_size=vocab_size, embed_dim=embed_dim)\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_get_embed(get_embed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Build RNN\n",
    "You created a RNN Cell in the `get_init_cell()` function.  Time to use the cell to create a RNN.\n",
    "- Build the RNN using the [`tf.nn.dynamic_rnn()`](https://www.tensorflow.org/api_docs/python/tf/nn/dynamic_rnn)\n",
    " - Apply the name \"final_state\" to the final state using [`tf.identity()`](https://www.tensorflow.org/api_docs/python/tf/identity)\n",
    "\n",
    "Return the outputs and final_state state in the following tuple `(Outputs, FinalState)` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def build_rnn(cell, inputs):\n",
    "    \"\"\"\n",
    "    Create a RNN using a RNN Cell\n",
    "    :param cell: RNN Cell\n",
    "    :param inputs: Input text data\n",
    "    :return: Tuple (Outputs, Final State)\n",
    "    \"\"\"\n",
    "    # note: third argument is placeholder for initial_state\n",
    "    outputs, final_state = tf.nn.dynamic_rnn(\n",
    "        cell=cell, inputs=inputs, dtype=tf.float32)\n",
    "    final_state = tf.identity(final_state, 'final_state')\n",
    "    return outputs, final_state\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_build_rnn(build_rnn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Build the Neural Network\n",
    "Apply the functions you implemented above to:\n",
    "- Apply embedding to `input_data` using your `get_embed(input_data, vocab_size, embed_dim)` function.\n",
    "- Build RNN using `cell` and your `build_rnn(cell, inputs)` function.\n",
    "- Apply a fully connected layer with a linear activation and `vocab_size` as the number of outputs.\n",
    "\n",
    "Return the logits and final state in the following tuple (Logits, FinalState) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def build_nn(cell, rnn_size, input_data, vocab_size, embed_dim):\n",
    "    \"\"\"\n",
    "    Build part of the neural network\n",
    "    :param cell: RNN cell\n",
    "    :param embed_dim: Size of word embeddings to use\n",
    "    :param input_data: Input data\n",
    "    :param vocab_size: Vocabulary size\n",
    "    :param embed_dim: Number of embedding dimensions\n",
    "    :return: Tuple (Logits, FinalState)\n",
    "    \"\"\"\n",
    "    embedding = get_embed(input_data, vocab_size, embed_dim)\n",
    "    lstm_outputs, final_state = build_rnn(cell, embedding)\n",
    "\n",
    "    logits = tf.contrib.layers.fully_connected(\n",
    "        lstm_outputs,\n",
    "        vocab_size,\n",
    "        weights_initializer=tf.truncated_normal_initializer(stddev=0.1),\n",
    "        biases_initializer=tf.zeros_initializer(),\n",
    "        activation_fn=None)\n",
    "\n",
    "    return logits, final_state\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_build_nn(build_nn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Batches\n",
    "Implement `get_batches` to create batches of input and targets using `int_text`.  The batches should be a Numpy array with the shape `(number of batches, 2, batch size, sequence length)`. Each batch contains two elements:\n",
    "- The first element is a single batch of **input** with the shape `[batch size, sequence length]`\n",
    "- The second element is a single batch of **targets** with the shape `[batch size, sequence length]`\n",
    "\n",
    "If you can't fill the last batch with enough data, drop the last batch.\n",
    "\n",
    "For exmple, `get_batches([1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15], 2, 3)` would return a Numpy array of the following:\n",
    "```\n",
    "[\n",
    "  # First Batch\n",
    "  [\n",
    "    # Batch of Input\n",
    "    [[ 1  2  3], [ 7  8  9]],\n",
    "    # Batch of targets\n",
    "    [[ 2  3  4], [ 8  9 10]]\n",
    "  ],\n",
    " \n",
    "  # Second Batch\n",
    "  [\n",
    "    # Batch of Input\n",
    "    [[ 4  5  6], [10 11 12]],\n",
    "    # Batch of targets\n",
    "    [[ 5  6  7], [11 12 13]]\n",
    "  ]\n",
    "]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def get_batches(int_text, batch_size, seq_length):\n",
    "    \"\"\"\n",
    "    Return batches of input and target\n",
    "    :param int_text: Text with the words replaced by their ids\n",
    "    :param batch_size: The size of batch\n",
    "    :param seq_length: The length of sequence\n",
    "    :return: Batches as a Numpy array\n",
    "    \"\"\"\n",
    "    n_batches = (len(int_text)-1)//(batch_size * seq_length)    \n",
    "    int_text = int_text[:n_batches * batch_size * seq_length + 1]\n",
    "\n",
    "    int_text_input_seq = [int_text[i*seq_length:i*seq_length+seq_length] for i in range(0, n_batches * batch_size)]\n",
    "    int_text = int_text[1:]\n",
    "    int_text_output = [int_text[i*seq_length:i*seq_length+seq_length] for i in range(0, n_batches * batch_size)]\n",
    "\n",
    "    all_data = []\n",
    "    for row in range(n_batches):\n",
    "        input_cols = []\n",
    "        target_cols = []\n",
    "        for col in range(batch_size):\n",
    "            input_cols.append(int_text_input_seq[col * n_batches + row])\n",
    "            target_cols.append(int_text_output[col * n_batches + row])\n",
    "        all_data.append([input_cols, target_cols])\n",
    "\n",
    "    return np.array(all_data)\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_get_batches(get_batches)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Neural Network Training\n",
    "### Hyperparameters\n",
    "Tune the following parameters:\n",
    "\n",
    "- Set `num_epochs` to the number of epochs.\n",
    "- Set `batch_size` to the batch size.\n",
    "- Set `rnn_size` to the size of the RNNs.\n",
    "- Set `embed_dim` to the size of the text word embeddings.\n",
    "- Set `seq_length` to the length of sequence.\n",
    "- Set `learning_rate` to the learning rate.\n",
    "- Set `show_every_n_batches` to the number of batches the neural network should print progress."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# reminder: tune hyper params according to advice at\n",
    "# check out https://nd101.slack.com/messages/C3PJV4741/convo/C3PJV4741-1490412688.590254/\n",
    "\n",
    "\n",
    "# Number of Epochs\n",
    "num_epochs = 50\n",
    "# Batch Size\n",
    "batch_size = 128\n",
    "# RNN Size\n",
    "rnn_size = 512\n",
    "# Number of embedding dimensions\n",
    "embed_dim = 300\n",
    "# Sequence Length\n",
    "seq_length = 16\n",
    "# Learning Rate\n",
    "learning_rate = 0.01\n",
    "# Show stats for every n number of batches\n",
    "show_every_n_batches = 25\n",
    "keep_prob = 1.0\n",
    "lstm_layers = 2\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "save_dir = './save'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Build the Graph\n",
    "Build the graph using the neural network you implemented."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL\n",
    "\"\"\"\n",
    "from tensorflow.contrib import seq2seq\n",
    "\n",
    "train_graph = tf.Graph()\n",
    "with train_graph.as_default():\n",
    "    vocab_size = len(int_to_vocab)\n",
    "    input_text, targets, lr = get_inputs()\n",
    "    input_data_shape = tf.shape(input_text)\n",
    "    cell, initial_state = get_init_cell(input_data_shape[0], rnn_size, lstm_layers=lstm_layers, keep_prob=keep_prob)\n",
    "    logits, final_state = build_nn(cell, embed_dim, input_text, vocab_size)\n",
    "\n",
    "    # Probabilities for generating words\n",
    "    probs = tf.nn.softmax(logits, name='probs')\n",
    "\n",
    "    # Loss function\n",
    "    cost = seq2seq.sequence_loss(\n",
    "        logits,\n",
    "        targets,\n",
    "        tf.ones([input_data_shape[0], input_data_shape[1]]))\n",
    "\n",
    "    # Optimizer\n",
    "    optimizer = tf.train.AdamOptimizer(lr)\n",
    "\n",
    "    # Gradient Clipping\n",
    "    gradients = optimizer.compute_gradients(cost)\n",
    "    capped_gradients = [(tf.clip_by_value(grad, -1., 1.), var) for grad, var in gradients]\n",
    "    train_op = optimizer.apply_gradients(capped_gradients)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Train\n",
    "Train the neural network on the preprocessed data.  If you have a hard time getting a good loss, check the [forms](https://discussions.udacity.com/) to see if anyone is having the same problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   0 Batch    0/1173   train_loss = 11.039\n",
      "Epoch   0 Batch   25/1173   train_loss = 6.307\n",
      "Epoch   0 Batch   50/1173   train_loss = 5.730\n",
      "Epoch   0 Batch   75/1173   train_loss = 5.603\n",
      "Epoch   0 Batch  100/1173   train_loss = 5.301\n",
      "Epoch   0 Batch  125/1173   train_loss = 5.266\n",
      "Epoch   0 Batch  150/1173   train_loss = 5.090\n",
      "Epoch   0 Batch  175/1173   train_loss = 5.056\n",
      "Epoch   0 Batch  200/1173   train_loss = 5.012\n",
      "Epoch   0 Batch  225/1173   train_loss = 5.050\n",
      "Epoch   0 Batch  250/1173   train_loss = 4.836\n",
      "Epoch   0 Batch  275/1173   train_loss = 5.038\n",
      "Epoch   0 Batch  300/1173   train_loss = 4.655\n",
      "Epoch   0 Batch  325/1173   train_loss = 4.738\n",
      "Epoch   0 Batch  350/1173   train_loss = 4.915\n",
      "Epoch   0 Batch  375/1173   train_loss = 4.749\n",
      "Epoch   0 Batch  400/1173   train_loss = 4.825\n",
      "Epoch   0 Batch  425/1173   train_loss = 4.911\n",
      "Epoch   0 Batch  450/1173   train_loss = 4.921\n",
      "Epoch   0 Batch  475/1173   train_loss = 4.750\n",
      "Epoch   0 Batch  500/1173   train_loss = 4.682\n",
      "Epoch   0 Batch  525/1173   train_loss = 4.673\n",
      "Epoch   0 Batch  550/1173   train_loss = 4.615\n",
      "Epoch   0 Batch  575/1173   train_loss = 4.552\n",
      "Epoch   0 Batch  600/1173   train_loss = 4.683\n",
      "Epoch   0 Batch  625/1173   train_loss = 4.623\n",
      "Epoch   0 Batch  650/1173   train_loss = 4.520\n",
      "Epoch   0 Batch  675/1173   train_loss = 4.614\n",
      "Epoch   0 Batch  700/1173   train_loss = 4.506\n",
      "Epoch   0 Batch  725/1173   train_loss = 4.565\n",
      "Epoch   0 Batch  750/1173   train_loss = 4.580\n",
      "Epoch   0 Batch  775/1173   train_loss = 4.462\n",
      "Epoch   0 Batch  800/1173   train_loss = 4.466\n",
      "Epoch   0 Batch  825/1173   train_loss = 4.433\n",
      "Epoch   0 Batch  850/1173   train_loss = 4.683\n",
      "Epoch   0 Batch  875/1173   train_loss = 4.400\n",
      "Epoch   0 Batch  900/1173   train_loss = 4.494\n",
      "Epoch   0 Batch  925/1173   train_loss = 4.682\n",
      "Epoch   0 Batch  950/1173   train_loss = 4.612\n",
      "Epoch   0 Batch  975/1173   train_loss = 4.561\n",
      "Epoch   0 Batch 1000/1173   train_loss = 4.605\n",
      "Epoch   0 Batch 1025/1173   train_loss = 4.629\n",
      "Epoch   0 Batch 1050/1173   train_loss = 4.534\n",
      "Epoch   0 Batch 1075/1173   train_loss = 4.519\n",
      "Epoch   0 Batch 1100/1173   train_loss = 4.319\n",
      "Epoch   0 Batch 1125/1173   train_loss = 4.412\n",
      "Epoch   0 Batch 1150/1173   train_loss = 4.627\n",
      "Epoch   1 Batch    2/1173   train_loss = 4.477\n",
      "Epoch   1 Batch   27/1173   train_loss = 4.360\n",
      "Epoch   1 Batch   52/1173   train_loss = 4.509\n",
      "Epoch   1 Batch   77/1173   train_loss = 4.342\n",
      "Epoch   1 Batch  102/1173   train_loss = 4.507\n",
      "Epoch   1 Batch  127/1173   train_loss = 4.491\n",
      "Epoch   1 Batch  152/1173   train_loss = 4.526\n",
      "Epoch   1 Batch  177/1173   train_loss = 4.286\n",
      "Epoch   1 Batch  202/1173   train_loss = 4.270\n",
      "Epoch   1 Batch  227/1173   train_loss = 4.254\n",
      "Epoch   1 Batch  252/1173   train_loss = 4.205\n",
      "Epoch   1 Batch  277/1173   train_loss = 4.270\n",
      "Epoch   1 Batch  302/1173   train_loss = 4.192\n",
      "Epoch   1 Batch  327/1173   train_loss = 4.224\n",
      "Epoch   1 Batch  352/1173   train_loss = 4.266\n",
      "Epoch   1 Batch  377/1173   train_loss = 4.353\n",
      "Epoch   1 Batch  402/1173   train_loss = 4.333\n",
      "Epoch   1 Batch  427/1173   train_loss = 4.107\n",
      "Epoch   1 Batch  452/1173   train_loss = 4.397\n",
      "Epoch   1 Batch  477/1173   train_loss = 4.343\n",
      "Epoch   1 Batch  502/1173   train_loss = 4.191\n",
      "Epoch   1 Batch  527/1173   train_loss = 4.211\n",
      "Epoch   1 Batch  552/1173   train_loss = 4.144\n",
      "Epoch   1 Batch  577/1173   train_loss = 4.077\n",
      "Epoch   1 Batch  602/1173   train_loss = 4.242\n",
      "Epoch   1 Batch  627/1173   train_loss = 4.108\n",
      "Epoch   1 Batch  652/1173   train_loss = 4.130\n",
      "Epoch   1 Batch  677/1173   train_loss = 4.286\n",
      "Epoch   1 Batch  702/1173   train_loss = 4.136\n",
      "Epoch   1 Batch  727/1173   train_loss = 4.278\n",
      "Epoch   1 Batch  752/1173   train_loss = 4.070\n",
      "Epoch   1 Batch  777/1173   train_loss = 4.107\n",
      "Epoch   1 Batch  802/1173   train_loss = 4.233\n",
      "Epoch   1 Batch  827/1173   train_loss = 4.072\n",
      "Epoch   1 Batch  852/1173   train_loss = 4.243\n",
      "Epoch   1 Batch  877/1173   train_loss = 4.228\n",
      "Epoch   1 Batch  902/1173   train_loss = 4.211\n",
      "Epoch   1 Batch  927/1173   train_loss = 4.182\n",
      "Epoch   1 Batch  952/1173   train_loss = 4.130\n",
      "Epoch   1 Batch  977/1173   train_loss = 4.212\n",
      "Epoch   1 Batch 1002/1173   train_loss = 4.285\n",
      "Epoch   1 Batch 1027/1173   train_loss = 4.338\n",
      "Epoch   1 Batch 1052/1173   train_loss = 4.175\n",
      "Epoch   1 Batch 1077/1173   train_loss = 4.156\n",
      "Epoch   1 Batch 1102/1173   train_loss = 4.224\n",
      "Epoch   1 Batch 1127/1173   train_loss = 4.123\n",
      "Epoch   1 Batch 1152/1173   train_loss = 4.045\n",
      "Epoch   2 Batch    4/1173   train_loss = 3.993\n",
      "Epoch   2 Batch   29/1173   train_loss = 4.362\n",
      "Epoch   2 Batch   54/1173   train_loss = 4.235\n",
      "Epoch   2 Batch   79/1173   train_loss = 4.068\n",
      "Epoch   2 Batch  104/1173   train_loss = 4.097\n",
      "Epoch   2 Batch  129/1173   train_loss = 4.126\n",
      "Epoch   2 Batch  154/1173   train_loss = 4.171\n",
      "Epoch   2 Batch  179/1173   train_loss = 4.178\n",
      "Epoch   2 Batch  204/1173   train_loss = 4.170\n",
      "Epoch   2 Batch  229/1173   train_loss = 4.035\n",
      "Epoch   2 Batch  254/1173   train_loss = 4.041\n",
      "Epoch   2 Batch  279/1173   train_loss = 4.051\n",
      "Epoch   2 Batch  304/1173   train_loss = 4.115\n",
      "Epoch   2 Batch  329/1173   train_loss = 4.156\n",
      "Epoch   2 Batch  354/1173   train_loss = 4.046\n",
      "Epoch   2 Batch  379/1173   train_loss = 4.078\n",
      "Epoch   2 Batch  404/1173   train_loss = 4.201\n",
      "Epoch   2 Batch  429/1173   train_loss = 3.916\n",
      "Epoch   2 Batch  454/1173   train_loss = 4.154\n",
      "Epoch   2 Batch  479/1173   train_loss = 4.073\n",
      "Epoch   2 Batch  504/1173   train_loss = 3.991\n",
      "Epoch   2 Batch  529/1173   train_loss = 4.169\n",
      "Epoch   2 Batch  554/1173   train_loss = 4.038\n",
      "Epoch   2 Batch  579/1173   train_loss = 4.193\n",
      "Epoch   2 Batch  604/1173   train_loss = 4.072\n",
      "Epoch   2 Batch  629/1173   train_loss = 4.094\n",
      "Epoch   2 Batch  654/1173   train_loss = 4.091\n",
      "Epoch   2 Batch  679/1173   train_loss = 4.204\n",
      "Epoch   2 Batch  704/1173   train_loss = 3.941\n",
      "Epoch   2 Batch  729/1173   train_loss = 3.953\n",
      "Epoch   2 Batch  754/1173   train_loss = 4.183\n",
      "Epoch   2 Batch  779/1173   train_loss = 4.025\n",
      "Epoch   2 Batch  804/1173   train_loss = 4.150\n",
      "Epoch   2 Batch  829/1173   train_loss = 4.111\n",
      "Epoch   2 Batch  854/1173   train_loss = 4.219\n",
      "Epoch   2 Batch  879/1173   train_loss = 4.035\n",
      "Epoch   2 Batch  904/1173   train_loss = 3.965\n",
      "Epoch   2 Batch  929/1173   train_loss = 4.009\n",
      "Epoch   2 Batch  954/1173   train_loss = 3.895\n",
      "Epoch   2 Batch  979/1173   train_loss = 3.957\n",
      "Epoch   2 Batch 1004/1173   train_loss = 4.014\n",
      "Epoch   2 Batch 1029/1173   train_loss = 4.098\n",
      "Epoch   2 Batch 1054/1173   train_loss = 3.922\n",
      "Epoch   2 Batch 1079/1173   train_loss = 4.017\n",
      "Epoch   2 Batch 1104/1173   train_loss = 4.016\n",
      "Epoch   2 Batch 1129/1173   train_loss = 3.935\n",
      "Epoch   2 Batch 1154/1173   train_loss = 3.967\n",
      "Epoch   3 Batch    6/1173   train_loss = 4.079\n",
      "Epoch   3 Batch   31/1173   train_loss = 4.012\n",
      "Epoch   3 Batch   56/1173   train_loss = 4.105\n",
      "Epoch   3 Batch   81/1173   train_loss = 4.222\n",
      "Epoch   3 Batch  106/1173   train_loss = 3.978\n",
      "Epoch   3 Batch  131/1173   train_loss = 4.001\n",
      "Epoch   3 Batch  156/1173   train_loss = 3.987\n",
      "Epoch   3 Batch  181/1173   train_loss = 4.028\n",
      "Epoch   3 Batch  206/1173   train_loss = 3.955\n",
      "Epoch   3 Batch  231/1173   train_loss = 4.072\n",
      "Epoch   3 Batch  256/1173   train_loss = 3.923\n",
      "Epoch   3 Batch  281/1173   train_loss = 3.897\n",
      "Epoch   3 Batch  306/1173   train_loss = 4.138\n",
      "Epoch   3 Batch  331/1173   train_loss = 4.033\n",
      "Epoch   3 Batch  356/1173   train_loss = 3.919\n",
      "Epoch   3 Batch  381/1173   train_loss = 3.930\n",
      "Epoch   3 Batch  406/1173   train_loss = 4.096\n",
      "Epoch   3 Batch  431/1173   train_loss = 3.955\n",
      "Epoch   3 Batch  456/1173   train_loss = 4.052\n",
      "Epoch   3 Batch  481/1173   train_loss = 4.031\n",
      "Epoch   3 Batch  506/1173   train_loss = 3.986\n",
      "Epoch   3 Batch  531/1173   train_loss = 4.035\n",
      "Epoch   3 Batch  556/1173   train_loss = 3.859\n",
      "Epoch   3 Batch  581/1173   train_loss = 3.962\n",
      "Epoch   3 Batch  606/1173   train_loss = 4.056\n",
      "Epoch   3 Batch  631/1173   train_loss = 3.995\n",
      "Epoch   3 Batch  656/1173   train_loss = 4.115\n",
      "Epoch   3 Batch  681/1173   train_loss = 4.018\n",
      "Epoch   3 Batch  706/1173   train_loss = 3.946\n",
      "Epoch   3 Batch  731/1173   train_loss = 4.018\n",
      "Epoch   3 Batch  756/1173   train_loss = 4.103\n",
      "Epoch   3 Batch  781/1173   train_loss = 3.939\n",
      "Epoch   3 Batch  806/1173   train_loss = 3.923\n",
      "Epoch   3 Batch  831/1173   train_loss = 4.017\n",
      "Epoch   3 Batch  856/1173   train_loss = 4.084\n",
      "Epoch   3 Batch  881/1173   train_loss = 3.979\n",
      "Epoch   3 Batch  906/1173   train_loss = 3.765\n",
      "Epoch   3 Batch  931/1173   train_loss = 3.927\n",
      "Epoch   3 Batch  956/1173   train_loss = 3.850\n",
      "Epoch   3 Batch  981/1173   train_loss = 3.738\n",
      "Epoch   3 Batch 1006/1173   train_loss = 3.894\n",
      "Epoch   3 Batch 1031/1173   train_loss = 3.917\n",
      "Epoch   3 Batch 1056/1173   train_loss = 3.958\n",
      "Epoch   3 Batch 1081/1173   train_loss = 3.941\n",
      "Epoch   3 Batch 1106/1173   train_loss = 3.899\n",
      "Epoch   3 Batch 1131/1173   train_loss = 3.821\n",
      "Epoch   3 Batch 1156/1173   train_loss = 3.835\n",
      "Epoch   4 Batch    8/1173   train_loss = 3.827\n",
      "Epoch   4 Batch   33/1173   train_loss = 3.861\n",
      "Epoch   4 Batch   58/1173   train_loss = 4.000\n",
      "Epoch   4 Batch   83/1173   train_loss = 4.008\n",
      "Epoch   4 Batch  108/1173   train_loss = 3.874\n",
      "Epoch   4 Batch  133/1173   train_loss = 3.904\n",
      "Epoch   4 Batch  158/1173   train_loss = 3.733\n",
      "Epoch   4 Batch  183/1173   train_loss = 3.878\n",
      "Epoch   4 Batch  208/1173   train_loss = 3.962\n",
      "Epoch   4 Batch  233/1173   train_loss = 3.856\n",
      "Epoch   4 Batch  258/1173   train_loss = 3.829\n",
      "Epoch   4 Batch  283/1173   train_loss = 3.874\n",
      "Epoch   4 Batch  308/1173   train_loss = 3.832\n",
      "Epoch   4 Batch  333/1173   train_loss = 4.023\n",
      "Epoch   4 Batch  358/1173   train_loss = 3.878\n",
      "Epoch   4 Batch  383/1173   train_loss = 3.876\n",
      "Epoch   4 Batch  408/1173   train_loss = 4.036\n",
      "Epoch   4 Batch  433/1173   train_loss = 3.906\n",
      "Epoch   4 Batch  458/1173   train_loss = 3.907\n",
      "Epoch   4 Batch  483/1173   train_loss = 3.973\n",
      "Epoch   4 Batch  508/1173   train_loss = 3.877\n",
      "Epoch   4 Batch  533/1173   train_loss = 3.898\n",
      "Epoch   4 Batch  558/1173   train_loss = 3.707\n",
      "Epoch   4 Batch  583/1173   train_loss = 3.894\n",
      "Epoch   4 Batch  608/1173   train_loss = 3.941\n",
      "Epoch   4 Batch  633/1173   train_loss = 3.919\n",
      "Epoch   4 Batch  658/1173   train_loss = 3.847\n",
      "Epoch   4 Batch  683/1173   train_loss = 3.781\n",
      "Epoch   4 Batch  708/1173   train_loss = 3.841\n",
      "Epoch   4 Batch  733/1173   train_loss = 3.990\n",
      "Epoch   4 Batch  758/1173   train_loss = 3.938\n",
      "Epoch   4 Batch  783/1173   train_loss = 4.008\n",
      "Epoch   4 Batch  808/1173   train_loss = 3.822\n",
      "Epoch   4 Batch  833/1173   train_loss = 3.857\n",
      "Epoch   4 Batch  858/1173   train_loss = 3.785\n",
      "Epoch   4 Batch  883/1173   train_loss = 3.889\n",
      "Epoch   4 Batch  908/1173   train_loss = 3.806\n",
      "Epoch   4 Batch  933/1173   train_loss = 3.851\n",
      "Epoch   4 Batch  958/1173   train_loss = 3.722\n",
      "Epoch   4 Batch  983/1173   train_loss = 3.790\n",
      "Epoch   4 Batch 1008/1173   train_loss = 3.831\n",
      "Epoch   4 Batch 1033/1173   train_loss = 3.873\n",
      "Epoch   4 Batch 1058/1173   train_loss = 3.932\n",
      "Epoch   4 Batch 1083/1173   train_loss = 3.894\n",
      "Epoch   4 Batch 1108/1173   train_loss = 3.694\n",
      "Epoch   4 Batch 1133/1173   train_loss = 3.772\n",
      "Epoch   4 Batch 1158/1173   train_loss = 3.721\n",
      "Epoch   5 Batch   10/1173   train_loss = 3.899\n",
      "Epoch   5 Batch   35/1173   train_loss = 3.931\n",
      "Epoch   5 Batch   60/1173   train_loss = 3.959\n",
      "Epoch   5 Batch   85/1173   train_loss = 3.932\n",
      "Epoch   5 Batch  110/1173   train_loss = 3.826\n",
      "Epoch   5 Batch  135/1173   train_loss = 3.863\n",
      "Epoch   5 Batch  160/1173   train_loss = 3.908\n",
      "Epoch   5 Batch  185/1173   train_loss = 3.911\n",
      "Epoch   5 Batch  210/1173   train_loss = 3.760\n",
      "Epoch   5 Batch  235/1173   train_loss = 3.868\n",
      "Epoch   5 Batch  260/1173   train_loss = 3.747\n",
      "Epoch   5 Batch  285/1173   train_loss = 3.822\n",
      "Epoch   5 Batch  310/1173   train_loss = 3.567\n",
      "Epoch   5 Batch  335/1173   train_loss = 3.853\n",
      "Epoch   5 Batch  360/1173   train_loss = 3.813\n",
      "Epoch   5 Batch  385/1173   train_loss = 3.750\n",
      "Epoch   5 Batch  410/1173   train_loss = 3.818\n",
      "Epoch   5 Batch  435/1173   train_loss = 3.735\n",
      "Epoch   5 Batch  460/1173   train_loss = 3.751\n",
      "Epoch   5 Batch  485/1173   train_loss = 3.684\n",
      "Epoch   5 Batch  510/1173   train_loss = 3.913\n",
      "Epoch   5 Batch  535/1173   train_loss = 3.868\n",
      "Epoch   5 Batch  560/1173   train_loss = 3.930\n",
      "Epoch   5 Batch  585/1173   train_loss = 3.729\n",
      "Epoch   5 Batch  610/1173   train_loss = 3.900\n",
      "Epoch   5 Batch  635/1173   train_loss = 3.768\n",
      "Epoch   5 Batch  660/1173   train_loss = 3.729\n",
      "Epoch   5 Batch  685/1173   train_loss = 3.773\n",
      "Epoch   5 Batch  710/1173   train_loss = 3.682\n",
      "Epoch   5 Batch  735/1173   train_loss = 3.670\n",
      "Epoch   5 Batch  760/1173   train_loss = 3.924\n",
      "Epoch   5 Batch  785/1173   train_loss = 3.761\n",
      "Epoch   5 Batch  810/1173   train_loss = 3.908\n",
      "Epoch   5 Batch  835/1173   train_loss = 3.713\n",
      "Epoch   5 Batch  860/1173   train_loss = 3.712\n",
      "Epoch   5 Batch  885/1173   train_loss = 3.826\n",
      "Epoch   5 Batch  910/1173   train_loss = 3.721\n",
      "Epoch   5 Batch  935/1173   train_loss = 3.819\n",
      "Epoch   5 Batch  960/1173   train_loss = 3.790\n",
      "Epoch   5 Batch  985/1173   train_loss = 3.811\n",
      "Epoch   5 Batch 1010/1173   train_loss = 3.861\n",
      "Epoch   5 Batch 1035/1173   train_loss = 3.709\n",
      "Epoch   5 Batch 1060/1173   train_loss = 3.621\n",
      "Epoch   5 Batch 1085/1173   train_loss = 3.681\n",
      "Epoch   5 Batch 1110/1173   train_loss = 3.585\n",
      "Epoch   5 Batch 1135/1173   train_loss = 3.788\n",
      "Epoch   5 Batch 1160/1173   train_loss = 3.795\n",
      "Epoch   6 Batch   12/1173   train_loss = 3.841\n",
      "Epoch   6 Batch   37/1173   train_loss = 3.776\n",
      "Epoch   6 Batch   62/1173   train_loss = 3.862\n",
      "Epoch   6 Batch   87/1173   train_loss = 3.818\n",
      "Epoch   6 Batch  112/1173   train_loss = 3.795\n",
      "Epoch   6 Batch  137/1173   train_loss = 3.762\n",
      "Epoch   6 Batch  162/1173   train_loss = 3.832\n",
      "Epoch   6 Batch  187/1173   train_loss = 3.800\n",
      "Epoch   6 Batch  212/1173   train_loss = 3.818\n",
      "Epoch   6 Batch  237/1173   train_loss = 3.783\n",
      "Epoch   6 Batch  262/1173   train_loss = 3.842\n",
      "Epoch   6 Batch  287/1173   train_loss = 3.738\n",
      "Epoch   6 Batch  312/1173   train_loss = 3.662\n",
      "Epoch   6 Batch  337/1173   train_loss = 3.790\n",
      "Epoch   6 Batch  362/1173   train_loss = 3.798\n",
      "Epoch   6 Batch  387/1173   train_loss = 3.689\n",
      "Epoch   6 Batch  412/1173   train_loss = 3.602\n",
      "Epoch   6 Batch  437/1173   train_loss = 3.822\n",
      "Epoch   6 Batch  462/1173   train_loss = 3.785\n",
      "Epoch   6 Batch  487/1173   train_loss = 3.676\n",
      "Epoch   6 Batch  512/1173   train_loss = 3.726\n",
      "Epoch   6 Batch  537/1173   train_loss = 3.750\n",
      "Epoch   6 Batch  562/1173   train_loss = 3.792\n",
      "Epoch   6 Batch  587/1173   train_loss = 3.682\n",
      "Epoch   6 Batch  612/1173   train_loss = 3.677\n",
      "Epoch   6 Batch  637/1173   train_loss = 3.711\n",
      "Epoch   6 Batch  662/1173   train_loss = 3.797\n",
      "Epoch   6 Batch  687/1173   train_loss = 3.659\n",
      "Epoch   6 Batch  712/1173   train_loss = 3.659\n",
      "Epoch   6 Batch  737/1173   train_loss = 3.936\n",
      "Epoch   6 Batch  762/1173   train_loss = 3.868\n",
      "Epoch   6 Batch  787/1173   train_loss = 3.814\n",
      "Epoch   6 Batch  812/1173   train_loss = 3.714\n",
      "Epoch   6 Batch  837/1173   train_loss = 3.728\n",
      "Epoch   6 Batch  862/1173   train_loss = 3.697\n",
      "Epoch   6 Batch  887/1173   train_loss = 3.752\n",
      "Epoch   6 Batch  912/1173   train_loss = 3.670\n",
      "Epoch   6 Batch  937/1173   train_loss = 3.779\n",
      "Epoch   6 Batch  962/1173   train_loss = 3.766\n",
      "Epoch   6 Batch  987/1173   train_loss = 3.679\n",
      "Epoch   6 Batch 1012/1173   train_loss = 3.706\n",
      "Epoch   6 Batch 1037/1173   train_loss = 3.710\n",
      "Epoch   6 Batch 1062/1173   train_loss = 3.693\n",
      "Epoch   6 Batch 1087/1173   train_loss = 3.660\n",
      "Epoch   6 Batch 1112/1173   train_loss = 3.608\n",
      "Epoch   6 Batch 1137/1173   train_loss = 3.656\n",
      "Epoch   6 Batch 1162/1173   train_loss = 3.698\n",
      "Epoch   7 Batch   14/1173   train_loss = 3.737\n",
      "Epoch   7 Batch   39/1173   train_loss = 3.718\n",
      "Epoch   7 Batch   64/1173   train_loss = 3.737\n",
      "Epoch   7 Batch   89/1173   train_loss = 3.828\n",
      "Epoch   7 Batch  114/1173   train_loss = 3.674\n",
      "Epoch   7 Batch  139/1173   train_loss = 3.716\n",
      "Epoch   7 Batch  164/1173   train_loss = 3.703\n",
      "Epoch   7 Batch  189/1173   train_loss = 3.829\n",
      "Epoch   7 Batch  214/1173   train_loss = 3.668\n",
      "Epoch   7 Batch  239/1173   train_loss = 3.759\n",
      "Epoch   7 Batch  264/1173   train_loss = 3.483\n",
      "Epoch   7 Batch  289/1173   train_loss = 3.704\n",
      "Epoch   7 Batch  314/1173   train_loss = 3.827\n",
      "Epoch   7 Batch  339/1173   train_loss = 3.748\n",
      "Epoch   7 Batch  364/1173   train_loss = 3.788\n",
      "Epoch   7 Batch  389/1173   train_loss = 3.565\n",
      "Epoch   7 Batch  414/1173   train_loss = 3.711\n",
      "Epoch   7 Batch  439/1173   train_loss = 3.651\n",
      "Epoch   7 Batch  464/1173   train_loss = 3.774\n",
      "Epoch   7 Batch  489/1173   train_loss = 3.774\n",
      "Epoch   7 Batch  514/1173   train_loss = 3.809\n",
      "Epoch   7 Batch  539/1173   train_loss = 3.779\n",
      "Epoch   7 Batch  564/1173   train_loss = 3.628\n",
      "Epoch   7 Batch  589/1173   train_loss = 3.684\n",
      "Epoch   7 Batch  614/1173   train_loss = 3.637\n",
      "Epoch   7 Batch  639/1173   train_loss = 3.711\n",
      "Epoch   7 Batch  664/1173   train_loss = 3.651\n",
      "Epoch   7 Batch  689/1173   train_loss = 3.589\n",
      "Epoch   7 Batch  714/1173   train_loss = 3.571\n",
      "Epoch   7 Batch  739/1173   train_loss = 3.669\n",
      "Epoch   7 Batch  764/1173   train_loss = 3.630\n",
      "Epoch   7 Batch  789/1173   train_loss = 3.708\n",
      "Epoch   7 Batch  814/1173   train_loss = 3.578\n",
      "Epoch   7 Batch  839/1173   train_loss = 3.666\n",
      "Epoch   7 Batch  864/1173   train_loss = 3.635\n",
      "Epoch   7 Batch  889/1173   train_loss = 3.684\n",
      "Epoch   7 Batch  914/1173   train_loss = 3.640\n",
      "Epoch   7 Batch  939/1173   train_loss = 3.674\n",
      "Epoch   7 Batch  964/1173   train_loss = 3.658\n",
      "Epoch   7 Batch  989/1173   train_loss = 3.630\n",
      "Epoch   7 Batch 1014/1173   train_loss = 3.650\n",
      "Epoch   7 Batch 1039/1173   train_loss = 3.680\n",
      "Epoch   7 Batch 1064/1173   train_loss = 3.443\n",
      "Epoch   7 Batch 1089/1173   train_loss = 3.740\n",
      "Epoch   7 Batch 1114/1173   train_loss = 3.527\n",
      "Epoch   7 Batch 1139/1173   train_loss = 3.632\n",
      "Epoch   7 Batch 1164/1173   train_loss = 3.498\n",
      "Epoch   8 Batch   16/1173   train_loss = 3.665\n",
      "Epoch   8 Batch   41/1173   train_loss = 3.524\n",
      "Epoch   8 Batch   66/1173   train_loss = 3.589\n",
      "Epoch   8 Batch   91/1173   train_loss = 3.808\n",
      "Epoch   8 Batch  116/1173   train_loss = 3.785\n",
      "Epoch   8 Batch  141/1173   train_loss = 3.630\n",
      "Epoch   8 Batch  166/1173   train_loss = 3.643\n",
      "Epoch   8 Batch  191/1173   train_loss = 3.529\n",
      "Epoch   8 Batch  216/1173   train_loss = 3.632\n",
      "Epoch   8 Batch  241/1173   train_loss = 3.604\n",
      "Epoch   8 Batch  266/1173   train_loss = 3.547\n",
      "Epoch   8 Batch  291/1173   train_loss = 3.690\n",
      "Epoch   8 Batch  316/1173   train_loss = 3.673\n",
      "Epoch   8 Batch  341/1173   train_loss = 3.747\n",
      "Epoch   8 Batch  366/1173   train_loss = 3.636\n",
      "Epoch   8 Batch  391/1173   train_loss = 3.695\n",
      "Epoch   8 Batch  416/1173   train_loss = 3.713\n",
      "Epoch   8 Batch  441/1173   train_loss = 3.667\n",
      "Epoch   8 Batch  466/1173   train_loss = 3.594\n",
      "Epoch   8 Batch  491/1173   train_loss = 3.514\n",
      "Epoch   8 Batch  516/1173   train_loss = 3.724\n",
      "Epoch   8 Batch  541/1173   train_loss = 3.688\n",
      "Epoch   8 Batch  566/1173   train_loss = 3.788\n",
      "Epoch   8 Batch  591/1173   train_loss = 3.630\n",
      "Epoch   8 Batch  616/1173   train_loss = 3.620\n",
      "Epoch   8 Batch  641/1173   train_loss = 3.613\n",
      "Epoch   8 Batch  666/1173   train_loss = 3.619\n",
      "Epoch   8 Batch  691/1173   train_loss = 3.516\n",
      "Epoch   8 Batch  716/1173   train_loss = 3.651\n",
      "Epoch   8 Batch  741/1173   train_loss = 3.801\n",
      "Epoch   8 Batch  766/1173   train_loss = 3.722\n",
      "Epoch   8 Batch  791/1173   train_loss = 3.657\n",
      "Epoch   8 Batch  816/1173   train_loss = 3.694\n",
      "Epoch   8 Batch  841/1173   train_loss = 3.715\n",
      "Epoch   8 Batch  866/1173   train_loss = 3.595\n",
      "Epoch   8 Batch  891/1173   train_loss = 3.619\n",
      "Epoch   8 Batch  916/1173   train_loss = 3.657\n",
      "Epoch   8 Batch  941/1173   train_loss = 3.503\n",
      "Epoch   8 Batch  966/1173   train_loss = 3.691\n",
      "Epoch   8 Batch  991/1173   train_loss = 3.710\n",
      "Epoch   8 Batch 1016/1173   train_loss = 3.506\n",
      "Epoch   8 Batch 1041/1173   train_loss = 3.539\n",
      "Epoch   8 Batch 1066/1173   train_loss = 3.626\n",
      "Epoch   8 Batch 1091/1173   train_loss = 3.520\n",
      "Epoch   8 Batch 1116/1173   train_loss = 3.642\n",
      "Epoch   8 Batch 1141/1173   train_loss = 3.673\n",
      "Epoch   8 Batch 1166/1173   train_loss = 3.647\n",
      "Epoch   9 Batch   18/1173   train_loss = 3.692\n",
      "Epoch   9 Batch   43/1173   train_loss = 3.571\n",
      "Epoch   9 Batch   68/1173   train_loss = 3.673\n",
      "Epoch   9 Batch   93/1173   train_loss = 3.715\n",
      "Epoch   9 Batch  118/1173   train_loss = 3.782\n",
      "Epoch   9 Batch  143/1173   train_loss = 3.603\n",
      "Epoch   9 Batch  168/1173   train_loss = 3.394\n",
      "Epoch   9 Batch  193/1173   train_loss = 3.538\n",
      "Epoch   9 Batch  218/1173   train_loss = 3.500\n",
      "Epoch   9 Batch  243/1173   train_loss = 3.620\n",
      "Epoch   9 Batch  268/1173   train_loss = 3.537\n",
      "Epoch   9 Batch  293/1173   train_loss = 3.727\n",
      "Epoch   9 Batch  318/1173   train_loss = 3.538\n",
      "Epoch   9 Batch  343/1173   train_loss = 3.662\n",
      "Epoch   9 Batch  368/1173   train_loss = 3.580\n",
      "Epoch   9 Batch  393/1173   train_loss = 3.691\n",
      "Epoch   9 Batch  418/1173   train_loss = 3.608\n",
      "Epoch   9 Batch  443/1173   train_loss = 3.692\n",
      "Epoch   9 Batch  468/1173   train_loss = 3.630\n",
      "Epoch   9 Batch  493/1173   train_loss = 3.710\n",
      "Epoch   9 Batch  518/1173   train_loss = 3.636\n",
      "Epoch   9 Batch  543/1173   train_loss = 3.650\n",
      "Epoch   9 Batch  568/1173   train_loss = 3.630\n",
      "Epoch   9 Batch  593/1173   train_loss = 3.583\n",
      "Epoch   9 Batch  618/1173   train_loss = 3.673\n",
      "Epoch   9 Batch  643/1173   train_loss = 3.646\n",
      "Epoch   9 Batch  668/1173   train_loss = 3.483\n",
      "Epoch   9 Batch  693/1173   train_loss = 3.577\n",
      "Epoch   9 Batch  718/1173   train_loss = 3.374\n",
      "Epoch   9 Batch  743/1173   train_loss = 3.567\n",
      "Epoch   9 Batch  768/1173   train_loss = 3.706\n",
      "Epoch   9 Batch  793/1173   train_loss = 3.578\n",
      "Epoch   9 Batch  818/1173   train_loss = 3.587\n",
      "Epoch   9 Batch  843/1173   train_loss = 3.542\n",
      "Epoch   9 Batch  868/1173   train_loss = 3.533\n",
      "Epoch   9 Batch  893/1173   train_loss = 3.497\n",
      "Epoch   9 Batch  918/1173   train_loss = 3.615\n",
      "Epoch   9 Batch  943/1173   train_loss = 3.558\n",
      "Epoch   9 Batch  968/1173   train_loss = 3.639\n",
      "Epoch   9 Batch  993/1173   train_loss = 3.514\n",
      "Epoch   9 Batch 1018/1173   train_loss = 3.510\n",
      "Epoch   9 Batch 1043/1173   train_loss = 3.661\n",
      "Epoch   9 Batch 1068/1173   train_loss = 3.634\n",
      "Epoch   9 Batch 1093/1173   train_loss = 3.576\n",
      "Epoch   9 Batch 1118/1173   train_loss = 3.547\n",
      "Epoch   9 Batch 1143/1173   train_loss = 3.533\n",
      "Epoch   9 Batch 1168/1173   train_loss = 3.484\n",
      "Epoch  10 Batch   20/1173   train_loss = 3.482\n",
      "Epoch  10 Batch   45/1173   train_loss = 3.452\n",
      "Epoch  10 Batch   70/1173   train_loss = 3.565\n",
      "Epoch  10 Batch   95/1173   train_loss = 3.673\n",
      "Epoch  10 Batch  120/1173   train_loss = 3.646\n",
      "Epoch  10 Batch  145/1173   train_loss = 3.638\n",
      "Epoch  10 Batch  170/1173   train_loss = 3.535\n",
      "Epoch  10 Batch  195/1173   train_loss = 3.558\n",
      "Epoch  10 Batch  220/1173   train_loss = 3.505\n",
      "Epoch  10 Batch  245/1173   train_loss = 3.541\n",
      "Epoch  10 Batch  270/1173   train_loss = 3.595\n",
      "Epoch  10 Batch  295/1173   train_loss = 3.514\n",
      "Epoch  10 Batch  320/1173   train_loss = 3.566\n",
      "Epoch  10 Batch  345/1173   train_loss = 3.591\n",
      "Epoch  10 Batch  370/1173   train_loss = 3.557\n",
      "Epoch  10 Batch  395/1173   train_loss = 3.527\n",
      "Epoch  10 Batch  420/1173   train_loss = 3.636\n",
      "Epoch  10 Batch  445/1173   train_loss = 3.473\n",
      "Epoch  10 Batch  470/1173   train_loss = 3.676\n",
      "Epoch  10 Batch  495/1173   train_loss = 3.601\n",
      "Epoch  10 Batch  520/1173   train_loss = 3.590\n",
      "Epoch  10 Batch  545/1173   train_loss = 3.616\n",
      "Epoch  10 Batch  570/1173   train_loss = 3.568\n",
      "Epoch  10 Batch  595/1173   train_loss = 3.778\n",
      "Epoch  10 Batch  620/1173   train_loss = 3.705\n",
      "Epoch  10 Batch  645/1173   train_loss = 3.638\n",
      "Epoch  10 Batch  670/1173   train_loss = 3.544\n",
      "Epoch  10 Batch  695/1173   train_loss = 3.621\n",
      "Epoch  10 Batch  720/1173   train_loss = 3.493\n",
      "Epoch  10 Batch  745/1173   train_loss = 3.622\n",
      "Epoch  10 Batch  770/1173   train_loss = 3.643\n",
      "Epoch  10 Batch  795/1173   train_loss = 3.539\n",
      "Epoch  10 Batch  820/1173   train_loss = 3.573\n",
      "Epoch  10 Batch  845/1173   train_loss = 3.679\n",
      "Epoch  10 Batch  870/1173   train_loss = 3.583\n",
      "Epoch  10 Batch  895/1173   train_loss = 3.661\n",
      "Epoch  10 Batch  920/1173   train_loss = 3.625\n",
      "Epoch  10 Batch  945/1173   train_loss = 3.573\n",
      "Epoch  10 Batch  970/1173   train_loss = 3.433\n",
      "Epoch  10 Batch  995/1173   train_loss = 3.543\n",
      "Epoch  10 Batch 1020/1173   train_loss = 3.482\n",
      "Epoch  10 Batch 1045/1173   train_loss = 3.449\n",
      "Epoch  10 Batch 1070/1173   train_loss = 3.521\n",
      "Epoch  10 Batch 1095/1173   train_loss = 3.568\n",
      "Epoch  10 Batch 1120/1173   train_loss = 3.461\n",
      "Epoch  10 Batch 1145/1173   train_loss = 3.681\n",
      "Epoch  10 Batch 1170/1173   train_loss = 3.571\n",
      "Epoch  11 Batch   22/1173   train_loss = 3.563\n",
      "Epoch  11 Batch   47/1173   train_loss = 3.579\n",
      "Epoch  11 Batch   72/1173   train_loss = 3.573\n",
      "Epoch  11 Batch   97/1173   train_loss = 3.534\n",
      "Epoch  11 Batch  122/1173   train_loss = 3.626\n",
      "Epoch  11 Batch  147/1173   train_loss = 3.620\n",
      "Epoch  11 Batch  172/1173   train_loss = 3.423\n",
      "Epoch  11 Batch  197/1173   train_loss = 3.501\n",
      "Epoch  11 Batch  222/1173   train_loss = 3.646\n",
      "Epoch  11 Batch  247/1173   train_loss = 3.628\n",
      "Epoch  11 Batch  272/1173   train_loss = 3.637\n",
      "Epoch  11 Batch  297/1173   train_loss = 3.664\n",
      "Epoch  11 Batch  322/1173   train_loss = 3.639\n",
      "Epoch  11 Batch  347/1173   train_loss = 3.534\n",
      "Epoch  11 Batch  372/1173   train_loss = 3.665\n",
      "Epoch  11 Batch  397/1173   train_loss = 3.689\n",
      "Epoch  11 Batch  422/1173   train_loss = 3.518\n",
      "Epoch  11 Batch  447/1173   train_loss = 3.730\n",
      "Epoch  11 Batch  472/1173   train_loss = 3.537\n",
      "Epoch  11 Batch  497/1173   train_loss = 3.515\n",
      "Epoch  11 Batch  522/1173   train_loss = 3.553\n",
      "Epoch  11 Batch  547/1173   train_loss = 3.658\n",
      "Epoch  11 Batch  572/1173   train_loss = 3.590\n",
      "Epoch  11 Batch  597/1173   train_loss = 3.465\n",
      "Epoch  11 Batch  622/1173   train_loss = 3.501\n",
      "Epoch  11 Batch  647/1173   train_loss = 3.721\n",
      "Epoch  11 Batch  672/1173   train_loss = 3.521\n",
      "Epoch  11 Batch  697/1173   train_loss = 3.550\n",
      "Epoch  11 Batch  722/1173   train_loss = 3.605\n",
      "Epoch  11 Batch  747/1173   train_loss = 3.620\n",
      "Epoch  11 Batch  772/1173   train_loss = 3.439\n",
      "Epoch  11 Batch  797/1173   train_loss = 3.554\n",
      "Epoch  11 Batch  822/1173   train_loss = 3.568\n",
      "Epoch  11 Batch  847/1173   train_loss = 3.438\n",
      "Epoch  11 Batch  872/1173   train_loss = 3.524\n",
      "Epoch  11 Batch  897/1173   train_loss = 3.570\n",
      "Epoch  11 Batch  922/1173   train_loss = 3.610\n",
      "Epoch  11 Batch  947/1173   train_loss = 3.596\n",
      "Epoch  11 Batch  972/1173   train_loss = 3.484\n",
      "Epoch  11 Batch  997/1173   train_loss = 3.489\n",
      "Epoch  11 Batch 1022/1173   train_loss = 3.553\n",
      "Epoch  11 Batch 1047/1173   train_loss = 3.559\n",
      "Epoch  11 Batch 1072/1173   train_loss = 3.384\n",
      "Epoch  11 Batch 1097/1173   train_loss = 3.485\n",
      "Epoch  11 Batch 1122/1173   train_loss = 3.595\n",
      "Epoch  11 Batch 1147/1173   train_loss = 3.654\n",
      "Epoch  11 Batch 1172/1173   train_loss = 3.728\n",
      "Epoch  12 Batch   24/1173   train_loss = 3.526\n",
      "Epoch  12 Batch   49/1173   train_loss = 3.452\n",
      "Epoch  12 Batch   74/1173   train_loss = 3.699\n",
      "Epoch  12 Batch   99/1173   train_loss = 3.440\n",
      "Epoch  12 Batch  124/1173   train_loss = 3.420\n",
      "Epoch  12 Batch  149/1173   train_loss = 3.749\n",
      "Epoch  12 Batch  174/1173   train_loss = 3.538\n",
      "Epoch  12 Batch  199/1173   train_loss = 3.492\n",
      "Epoch  12 Batch  224/1173   train_loss = 3.601\n",
      "Epoch  12 Batch  249/1173   train_loss = 3.621\n",
      "Epoch  12 Batch  274/1173   train_loss = 3.488\n",
      "Epoch  12 Batch  299/1173   train_loss = 3.589\n",
      "Epoch  12 Batch  324/1173   train_loss = 3.553\n",
      "Epoch  12 Batch  349/1173   train_loss = 3.533\n",
      "Epoch  12 Batch  374/1173   train_loss = 3.595\n",
      "Epoch  12 Batch  399/1173   train_loss = 3.627\n",
      "Epoch  12 Batch  424/1173   train_loss = 3.619\n",
      "Epoch  12 Batch  449/1173   train_loss = 3.555\n",
      "Epoch  12 Batch  474/1173   train_loss = 3.603\n",
      "Epoch  12 Batch  499/1173   train_loss = 3.422\n",
      "Epoch  12 Batch  524/1173   train_loss = 3.547\n",
      "Epoch  12 Batch  549/1173   train_loss = 3.443\n",
      "Epoch  12 Batch  574/1173   train_loss = 3.643\n",
      "Epoch  12 Batch  599/1173   train_loss = 3.377\n",
      "Epoch  12 Batch  624/1173   train_loss = 3.549\n",
      "Epoch  12 Batch  649/1173   train_loss = 3.554\n",
      "Epoch  12 Batch  674/1173   train_loss = 3.573\n",
      "Epoch  12 Batch  699/1173   train_loss = 3.563\n",
      "Epoch  12 Batch  724/1173   train_loss = 3.411\n",
      "Epoch  12 Batch  749/1173   train_loss = 3.579\n",
      "Epoch  12 Batch  774/1173   train_loss = 3.494\n",
      "Epoch  12 Batch  799/1173   train_loss = 3.392\n",
      "Epoch  12 Batch  824/1173   train_loss = 3.433\n",
      "Epoch  12 Batch  849/1173   train_loss = 3.604\n",
      "Epoch  12 Batch  874/1173   train_loss = 3.554\n",
      "Epoch  12 Batch  899/1173   train_loss = 3.401\n",
      "Epoch  12 Batch  924/1173   train_loss = 3.398\n",
      "Epoch  12 Batch  949/1173   train_loss = 3.526\n",
      "Epoch  12 Batch  974/1173   train_loss = 3.333\n",
      "Epoch  12 Batch  999/1173   train_loss = 3.311\n",
      "Epoch  12 Batch 1024/1173   train_loss = 3.476\n",
      "Epoch  12 Batch 1049/1173   train_loss = 3.619\n",
      "Epoch  12 Batch 1074/1173   train_loss = 3.576\n",
      "Epoch  12 Batch 1099/1173   train_loss = 3.464\n",
      "Epoch  12 Batch 1124/1173   train_loss = 3.583\n",
      "Epoch  12 Batch 1149/1173   train_loss = 3.611\n",
      "Epoch  13 Batch    1/1173   train_loss = 3.471\n",
      "Epoch  13 Batch   26/1173   train_loss = 3.490\n",
      "Epoch  13 Batch   51/1173   train_loss = 3.622\n",
      "Epoch  13 Batch   76/1173   train_loss = 3.525\n",
      "Epoch  13 Batch  101/1173   train_loss = 3.641\n",
      "Epoch  13 Batch  126/1173   train_loss = 3.589\n",
      "Epoch  13 Batch  151/1173   train_loss = 3.633\n",
      "Epoch  13 Batch  176/1173   train_loss = 3.500\n",
      "Epoch  13 Batch  201/1173   train_loss = 3.526\n",
      "Epoch  13 Batch  226/1173   train_loss = 3.518\n",
      "Epoch  13 Batch  251/1173   train_loss = 3.569\n",
      "Epoch  13 Batch  276/1173   train_loss = 3.542\n",
      "Epoch  13 Batch  301/1173   train_loss = 3.449\n",
      "Epoch  13 Batch  326/1173   train_loss = 3.677\n",
      "Epoch  13 Batch  351/1173   train_loss = 3.533\n",
      "Epoch  13 Batch  376/1173   train_loss = 3.414\n",
      "Epoch  13 Batch  401/1173   train_loss = 3.614\n",
      "Epoch  13 Batch  426/1173   train_loss = 3.582\n",
      "Epoch  13 Batch  451/1173   train_loss = 3.555\n",
      "Epoch  13 Batch  476/1173   train_loss = 3.488\n",
      "Epoch  13 Batch  501/1173   train_loss = 3.472\n",
      "Epoch  13 Batch  526/1173   train_loss = 3.574\n",
      "Epoch  13 Batch  551/1173   train_loss = 3.532\n",
      "Epoch  13 Batch  576/1173   train_loss = 3.485\n",
      "Epoch  13 Batch  601/1173   train_loss = 3.628\n",
      "Epoch  13 Batch  626/1173   train_loss = 3.519\n",
      "Epoch  13 Batch  651/1173   train_loss = 3.585\n",
      "Epoch  13 Batch  676/1173   train_loss = 3.459\n",
      "Epoch  13 Batch  701/1173   train_loss = 3.664\n",
      "Epoch  13 Batch  726/1173   train_loss = 3.540\n",
      "Epoch  13 Batch  751/1173   train_loss = 3.471\n",
      "Epoch  13 Batch  776/1173   train_loss = 3.430\n",
      "Epoch  13 Batch  801/1173   train_loss = 3.533\n",
      "Epoch  13 Batch  826/1173   train_loss = 3.478\n",
      "Epoch  13 Batch  851/1173   train_loss = 3.507\n",
      "Epoch  13 Batch  876/1173   train_loss = 3.528\n",
      "Epoch  13 Batch  901/1173   train_loss = 3.593\n",
      "Epoch  13 Batch  926/1173   train_loss = 3.504\n",
      "Epoch  13 Batch  951/1173   train_loss = 3.593\n",
      "Epoch  13 Batch  976/1173   train_loss = 3.608\n",
      "Epoch  13 Batch 1001/1173   train_loss = 3.481\n",
      "Epoch  13 Batch 1026/1173   train_loss = 3.407\n",
      "Epoch  13 Batch 1051/1173   train_loss = 3.433\n",
      "Epoch  13 Batch 1076/1173   train_loss = 3.559\n",
      "Epoch  13 Batch 1101/1173   train_loss = 3.459\n",
      "Epoch  13 Batch 1126/1173   train_loss = 3.408\n",
      "Epoch  13 Batch 1151/1173   train_loss = 3.460\n",
      "Epoch  14 Batch    3/1173   train_loss = 3.454\n",
      "Epoch  14 Batch   28/1173   train_loss = 3.481\n",
      "Epoch  14 Batch   53/1173   train_loss = 3.474\n",
      "Epoch  14 Batch   78/1173   train_loss = 3.512\n",
      "Epoch  14 Batch  103/1173   train_loss = 3.541\n",
      "Epoch  14 Batch  128/1173   train_loss = 3.523\n",
      "Epoch  14 Batch  153/1173   train_loss = 3.435\n",
      "Epoch  14 Batch  178/1173   train_loss = 3.439\n",
      "Epoch  14 Batch  203/1173   train_loss = 3.418\n",
      "Epoch  14 Batch  228/1173   train_loss = 3.313\n",
      "Epoch  14 Batch  253/1173   train_loss = 3.503\n",
      "Epoch  14 Batch  278/1173   train_loss = 3.565\n",
      "Epoch  14 Batch  303/1173   train_loss = 3.434\n",
      "Epoch  14 Batch  328/1173   train_loss = 3.356\n",
      "Epoch  14 Batch  353/1173   train_loss = 3.567\n",
      "Epoch  14 Batch  378/1173   train_loss = 3.302\n",
      "Epoch  14 Batch  403/1173   train_loss = 3.581\n",
      "Epoch  14 Batch  428/1173   train_loss = 3.413\n",
      "Epoch  14 Batch  453/1173   train_loss = 3.599\n",
      "Epoch  14 Batch  478/1173   train_loss = 3.471\n",
      "Epoch  14 Batch  503/1173   train_loss = 3.393\n",
      "Epoch  14 Batch  528/1173   train_loss = 3.431\n",
      "Epoch  14 Batch  553/1173   train_loss = 3.440\n",
      "Epoch  14 Batch  578/1173   train_loss = 3.493\n",
      "Epoch  14 Batch  603/1173   train_loss = 3.502\n",
      "Epoch  14 Batch  628/1173   train_loss = 3.437\n",
      "Epoch  14 Batch  653/1173   train_loss = 3.506\n",
      "Epoch  14 Batch  678/1173   train_loss = 3.523\n",
      "Epoch  14 Batch  703/1173   train_loss = 3.537\n",
      "Epoch  14 Batch  728/1173   train_loss = 3.456\n",
      "Epoch  14 Batch  753/1173   train_loss = 3.436\n",
      "Epoch  14 Batch  778/1173   train_loss = 3.625\n",
      "Epoch  14 Batch  803/1173   train_loss = 3.306\n",
      "Epoch  14 Batch  828/1173   train_loss = 3.612\n",
      "Epoch  14 Batch  853/1173   train_loss = 3.519\n",
      "Epoch  14 Batch  878/1173   train_loss = 3.354\n",
      "Epoch  14 Batch  903/1173   train_loss = 3.452\n",
      "Epoch  14 Batch  928/1173   train_loss = 3.591\n",
      "Epoch  14 Batch  953/1173   train_loss = 3.515\n",
      "Epoch  14 Batch  978/1173   train_loss = 3.327\n",
      "Epoch  14 Batch 1003/1173   train_loss = 3.304\n",
      "Epoch  14 Batch 1028/1173   train_loss = 3.454\n",
      "Epoch  14 Batch 1053/1173   train_loss = 3.469\n",
      "Epoch  14 Batch 1078/1173   train_loss = 3.535\n",
      "Epoch  14 Batch 1103/1173   train_loss = 3.561\n",
      "Epoch  14 Batch 1128/1173   train_loss = 3.454\n",
      "Epoch  14 Batch 1153/1173   train_loss = 3.498\n",
      "Epoch  15 Batch    5/1173   train_loss = 3.266\n",
      "Epoch  15 Batch   30/1173   train_loss = 3.493\n",
      "Epoch  15 Batch   55/1173   train_loss = 3.439\n",
      "Epoch  15 Batch   80/1173   train_loss = 3.426\n",
      "Epoch  15 Batch  105/1173   train_loss = 3.435\n",
      "Epoch  15 Batch  130/1173   train_loss = 3.413\n",
      "Epoch  15 Batch  155/1173   train_loss = 3.541\n",
      "Epoch  15 Batch  180/1173   train_loss = 3.545\n",
      "Epoch  15 Batch  205/1173   train_loss = 3.395\n",
      "Epoch  15 Batch  230/1173   train_loss = 3.469\n",
      "Epoch  15 Batch  255/1173   train_loss = 3.496\n",
      "Epoch  15 Batch  280/1173   train_loss = 3.522\n",
      "Epoch  15 Batch  305/1173   train_loss = 3.530\n",
      "Epoch  15 Batch  330/1173   train_loss = 3.405\n",
      "Epoch  15 Batch  355/1173   train_loss = 3.513\n",
      "Epoch  15 Batch  380/1173   train_loss = 3.329\n",
      "Epoch  15 Batch  405/1173   train_loss = 3.526\n",
      "Epoch  15 Batch  430/1173   train_loss = 3.319\n",
      "Epoch  15 Batch  455/1173   train_loss = 3.456\n",
      "Epoch  15 Batch  480/1173   train_loss = 3.396\n",
      "Epoch  15 Batch  505/1173   train_loss = 3.520\n",
      "Epoch  15 Batch  530/1173   train_loss = 3.404\n",
      "Epoch  15 Batch  555/1173   train_loss = 3.476\n",
      "Epoch  15 Batch  580/1173   train_loss = 3.466\n",
      "Epoch  15 Batch  605/1173   train_loss = 3.570\n",
      "Epoch  15 Batch  630/1173   train_loss = 3.448\n",
      "Epoch  15 Batch  655/1173   train_loss = 3.380\n",
      "Epoch  15 Batch  680/1173   train_loss = 3.408\n",
      "Epoch  15 Batch  705/1173   train_loss = 3.468\n",
      "Epoch  15 Batch  730/1173   train_loss = 3.505\n",
      "Epoch  15 Batch  755/1173   train_loss = 3.595\n",
      "Epoch  15 Batch  780/1173   train_loss = 3.398\n",
      "Epoch  15 Batch  805/1173   train_loss = 3.514\n",
      "Epoch  15 Batch  830/1173   train_loss = 3.542\n",
      "Epoch  15 Batch  855/1173   train_loss = 3.466\n",
      "Epoch  15 Batch  880/1173   train_loss = 3.420\n",
      "Epoch  15 Batch  905/1173   train_loss = 3.432\n",
      "Epoch  15 Batch  930/1173   train_loss = 3.322\n",
      "Epoch  15 Batch  955/1173   train_loss = 3.463\n",
      "Epoch  15 Batch  980/1173   train_loss = 3.452\n",
      "Epoch  15 Batch 1005/1173   train_loss = 3.360\n",
      "Epoch  15 Batch 1030/1173   train_loss = 3.418\n",
      "Epoch  15 Batch 1055/1173   train_loss = 3.456\n",
      "Epoch  15 Batch 1080/1173   train_loss = 3.418\n",
      "Epoch  15 Batch 1105/1173   train_loss = 3.484\n",
      "Epoch  15 Batch 1130/1173   train_loss = 3.429\n",
      "Epoch  15 Batch 1155/1173   train_loss = 3.411\n",
      "Epoch  16 Batch    7/1173   train_loss = 3.399\n",
      "Epoch  16 Batch   32/1173   train_loss = 3.497\n",
      "Epoch  16 Batch   57/1173   train_loss = 3.354\n",
      "Epoch  16 Batch   82/1173   train_loss = 3.401\n",
      "Epoch  16 Batch  107/1173   train_loss = 3.516\n",
      "Epoch  16 Batch  132/1173   train_loss = 3.373\n",
      "Epoch  16 Batch  157/1173   train_loss = 3.561\n",
      "Epoch  16 Batch  182/1173   train_loss = 3.477\n",
      "Epoch  16 Batch  207/1173   train_loss = 3.389\n",
      "Epoch  16 Batch  232/1173   train_loss = 3.537\n",
      "Epoch  16 Batch  257/1173   train_loss = 3.419\n",
      "Epoch  16 Batch  282/1173   train_loss = 3.481\n",
      "Epoch  16 Batch  307/1173   train_loss = 3.390\n",
      "Epoch  16 Batch  332/1173   train_loss = 3.388\n",
      "Epoch  16 Batch  357/1173   train_loss = 3.524\n",
      "Epoch  16 Batch  382/1173   train_loss = 3.472\n",
      "Epoch  16 Batch  407/1173   train_loss = 3.544\n",
      "Epoch  16 Batch  432/1173   train_loss = 3.588\n",
      "Epoch  16 Batch  457/1173   train_loss = 3.538\n",
      "Epoch  16 Batch  482/1173   train_loss = 3.493\n",
      "Epoch  16 Batch  507/1173   train_loss = 3.558\n",
      "Epoch  16 Batch  532/1173   train_loss = 3.563\n",
      "Epoch  16 Batch  557/1173   train_loss = 3.431\n",
      "Epoch  16 Batch  582/1173   train_loss = 3.527\n",
      "Epoch  16 Batch  607/1173   train_loss = 3.390\n",
      "Epoch  16 Batch  632/1173   train_loss = 3.489\n",
      "Epoch  16 Batch  657/1173   train_loss = 3.482\n",
      "Epoch  16 Batch  682/1173   train_loss = 3.389\n",
      "Epoch  16 Batch  707/1173   train_loss = 3.425\n",
      "Epoch  16 Batch  732/1173   train_loss = 3.413\n",
      "Epoch  16 Batch  757/1173   train_loss = 3.472\n",
      "Epoch  16 Batch  782/1173   train_loss = 3.370\n",
      "Epoch  16 Batch  807/1173   train_loss = 3.494\n",
      "Epoch  16 Batch  832/1173   train_loss = 3.505\n",
      "Epoch  16 Batch  857/1173   train_loss = 3.506\n",
      "Epoch  16 Batch  882/1173   train_loss = 3.485\n",
      "Epoch  16 Batch  907/1173   train_loss = 3.338\n",
      "Epoch  16 Batch  932/1173   train_loss = 3.333\n",
      "Epoch  16 Batch  957/1173   train_loss = 3.366\n",
      "Epoch  16 Batch  982/1173   train_loss = 3.172\n",
      "Epoch  16 Batch 1007/1173   train_loss = 3.419\n",
      "Epoch  16 Batch 1032/1173   train_loss = 3.287\n",
      "Epoch  16 Batch 1057/1173   train_loss = 3.326\n",
      "Epoch  16 Batch 1082/1173   train_loss = 3.409\n",
      "Epoch  16 Batch 1107/1173   train_loss = 3.469\n",
      "Epoch  16 Batch 1132/1173   train_loss = 3.212\n",
      "Epoch  16 Batch 1157/1173   train_loss = 3.455\n",
      "Epoch  17 Batch    9/1173   train_loss = 3.436\n",
      "Epoch  17 Batch   34/1173   train_loss = 3.374\n",
      "Epoch  17 Batch   59/1173   train_loss = 3.545\n",
      "Epoch  17 Batch   84/1173   train_loss = 3.372\n",
      "Epoch  17 Batch  109/1173   train_loss = 3.528\n",
      "Epoch  17 Batch  134/1173   train_loss = 3.585\n",
      "Epoch  17 Batch  159/1173   train_loss = 3.551\n",
      "Epoch  17 Batch  184/1173   train_loss = 3.499\n",
      "Epoch  17 Batch  209/1173   train_loss = 3.455\n",
      "Epoch  17 Batch  234/1173   train_loss = 3.550\n",
      "Epoch  17 Batch  259/1173   train_loss = 3.371\n",
      "Epoch  17 Batch  284/1173   train_loss = 3.507\n",
      "Epoch  17 Batch  309/1173   train_loss = 3.496\n",
      "Epoch  17 Batch  334/1173   train_loss = 3.421\n",
      "Epoch  17 Batch  359/1173   train_loss = 3.536\n",
      "Epoch  17 Batch  384/1173   train_loss = 3.389\n",
      "Epoch  17 Batch  409/1173   train_loss = 3.345\n",
      "Epoch  17 Batch  434/1173   train_loss = 3.493\n",
      "Epoch  17 Batch  459/1173   train_loss = 3.434\n",
      "Epoch  17 Batch  484/1173   train_loss = 3.499\n",
      "Epoch  17 Batch  509/1173   train_loss = 3.326\n",
      "Epoch  17 Batch  534/1173   train_loss = 3.489\n",
      "Epoch  17 Batch  559/1173   train_loss = 3.432\n",
      "Epoch  17 Batch  584/1173   train_loss = 3.400\n",
      "Epoch  17 Batch  609/1173   train_loss = 3.467\n",
      "Epoch  17 Batch  634/1173   train_loss = 3.434\n",
      "Epoch  17 Batch  659/1173   train_loss = 3.379\n",
      "Epoch  17 Batch  684/1173   train_loss = 3.428\n",
      "Epoch  17 Batch  709/1173   train_loss = 3.414\n",
      "Epoch  17 Batch  734/1173   train_loss = 3.424\n",
      "Epoch  17 Batch  759/1173   train_loss = 3.359\n",
      "Epoch  17 Batch  784/1173   train_loss = 3.415\n",
      "Epoch  17 Batch  809/1173   train_loss = 3.507\n",
      "Epoch  17 Batch  834/1173   train_loss = 3.332\n",
      "Epoch  17 Batch  859/1173   train_loss = 3.506\n",
      "Epoch  17 Batch  884/1173   train_loss = 3.457\n",
      "Epoch  17 Batch  909/1173   train_loss = 3.375\n",
      "Epoch  17 Batch  934/1173   train_loss = 3.411\n",
      "Epoch  17 Batch  959/1173   train_loss = 3.255\n",
      "Epoch  17 Batch  984/1173   train_loss = 3.431\n",
      "Epoch  17 Batch 1009/1173   train_loss = 3.393\n",
      "Epoch  17 Batch 1034/1173   train_loss = 3.530\n",
      "Epoch  17 Batch 1059/1173   train_loss = 3.407\n",
      "Epoch  17 Batch 1084/1173   train_loss = 3.466\n",
      "Epoch  17 Batch 1109/1173   train_loss = 3.318\n",
      "Epoch  17 Batch 1134/1173   train_loss = 3.379\n",
      "Epoch  17 Batch 1159/1173   train_loss = 3.274\n",
      "Epoch  18 Batch   11/1173   train_loss = 3.544\n",
      "Epoch  18 Batch   36/1173   train_loss = 3.632\n",
      "Epoch  18 Batch   61/1173   train_loss = 3.421\n",
      "Epoch  18 Batch   86/1173   train_loss = 3.576\n",
      "Epoch  18 Batch  111/1173   train_loss = 3.320\n",
      "Epoch  18 Batch  136/1173   train_loss = 3.360\n",
      "Epoch  18 Batch  161/1173   train_loss = 3.361\n",
      "Epoch  18 Batch  186/1173   train_loss = 3.444\n",
      "Epoch  18 Batch  211/1173   train_loss = 3.493\n",
      "Epoch  18 Batch  236/1173   train_loss = 3.316\n",
      "Epoch  18 Batch  261/1173   train_loss = 3.391\n",
      "Epoch  18 Batch  286/1173   train_loss = 3.499\n",
      "Epoch  18 Batch  311/1173   train_loss = 3.467\n",
      "Epoch  18 Batch  336/1173   train_loss = 3.380\n",
      "Epoch  18 Batch  361/1173   train_loss = 3.423\n",
      "Epoch  18 Batch  386/1173   train_loss = 3.471\n",
      "Epoch  18 Batch  411/1173   train_loss = 3.372\n",
      "Epoch  18 Batch  436/1173   train_loss = 3.395\n",
      "Epoch  18 Batch  461/1173   train_loss = 3.517\n",
      "Epoch  18 Batch  486/1173   train_loss = 3.510\n",
      "Epoch  18 Batch  511/1173   train_loss = 3.391\n",
      "Epoch  18 Batch  536/1173   train_loss = 3.571\n",
      "Epoch  18 Batch  561/1173   train_loss = 3.379\n",
      "Epoch  18 Batch  586/1173   train_loss = 3.349\n",
      "Epoch  18 Batch  611/1173   train_loss = 3.305\n",
      "Epoch  18 Batch  636/1173   train_loss = 3.394\n",
      "Epoch  18 Batch  661/1173   train_loss = 3.293\n",
      "Epoch  18 Batch  686/1173   train_loss = 3.446\n",
      "Epoch  18 Batch  711/1173   train_loss = 3.387\n",
      "Epoch  18 Batch  736/1173   train_loss = 3.502\n",
      "Epoch  18 Batch  761/1173   train_loss = 3.432\n",
      "Epoch  18 Batch  786/1173   train_loss = 3.411\n",
      "Epoch  18 Batch  811/1173   train_loss = 3.477\n",
      "Epoch  18 Batch  836/1173   train_loss = 3.459\n",
      "Epoch  18 Batch  861/1173   train_loss = 3.376\n",
      "Epoch  18 Batch  886/1173   train_loss = 3.382\n",
      "Epoch  18 Batch  911/1173   train_loss = 3.460\n",
      "Epoch  18 Batch  936/1173   train_loss = 3.359\n",
      "Epoch  18 Batch  961/1173   train_loss = 3.384\n",
      "Epoch  18 Batch  986/1173   train_loss = 3.447\n",
      "Epoch  18 Batch 1011/1173   train_loss = 3.366\n",
      "Epoch  18 Batch 1036/1173   train_loss = 3.346\n",
      "Epoch  18 Batch 1061/1173   train_loss = 3.378\n",
      "Epoch  18 Batch 1086/1173   train_loss = 3.426\n",
      "Epoch  18 Batch 1111/1173   train_loss = 3.395\n",
      "Epoch  18 Batch 1136/1173   train_loss = 3.419\n",
      "Epoch  18 Batch 1161/1173   train_loss = 3.337\n",
      "Epoch  19 Batch   13/1173   train_loss = 3.369\n",
      "Epoch  19 Batch   38/1173   train_loss = 3.491\n",
      "Epoch  19 Batch   63/1173   train_loss = 3.432\n",
      "Epoch  19 Batch   88/1173   train_loss = 3.391\n",
      "Epoch  19 Batch  113/1173   train_loss = 3.388\n",
      "Epoch  19 Batch  138/1173   train_loss = 3.489\n",
      "Epoch  19 Batch  163/1173   train_loss = 3.465\n",
      "Epoch  19 Batch  188/1173   train_loss = 3.462\n",
      "Epoch  19 Batch  213/1173   train_loss = 3.432\n",
      "Epoch  19 Batch  238/1173   train_loss = 3.387\n",
      "Epoch  19 Batch  263/1173   train_loss = 3.460\n",
      "Epoch  19 Batch  288/1173   train_loss = 3.388\n",
      "Epoch  19 Batch  313/1173   train_loss = 3.403\n",
      "Epoch  19 Batch  338/1173   train_loss = 3.443\n",
      "Epoch  19 Batch  363/1173   train_loss = 3.335\n",
      "Epoch  19 Batch  388/1173   train_loss = 3.369\n",
      "Epoch  19 Batch  413/1173   train_loss = 3.272\n",
      "Epoch  19 Batch  438/1173   train_loss = 3.437\n",
      "Epoch  19 Batch  463/1173   train_loss = 3.402\n",
      "Epoch  19 Batch  488/1173   train_loss = 3.495\n",
      "Epoch  19 Batch  513/1173   train_loss = 3.391\n",
      "Epoch  19 Batch  538/1173   train_loss = 3.536\n",
      "Epoch  19 Batch  563/1173   train_loss = 3.429\n",
      "Epoch  19 Batch  588/1173   train_loss = 3.416\n",
      "Epoch  19 Batch  613/1173   train_loss = 3.443\n",
      "Epoch  19 Batch  638/1173   train_loss = 3.273\n",
      "Epoch  19 Batch  663/1173   train_loss = 3.467\n",
      "Epoch  19 Batch  688/1173   train_loss = 3.379\n",
      "Epoch  19 Batch  713/1173   train_loss = 3.387\n",
      "Epoch  19 Batch  738/1173   train_loss = 3.504\n",
      "Epoch  19 Batch  763/1173   train_loss = 3.330\n",
      "Epoch  19 Batch  788/1173   train_loss = 3.356\n",
      "Epoch  19 Batch  813/1173   train_loss = 3.461\n",
      "Epoch  19 Batch  838/1173   train_loss = 3.431\n",
      "Epoch  19 Batch  863/1173   train_loss = 3.494\n",
      "Epoch  19 Batch  888/1173   train_loss = 3.359\n",
      "Epoch  19 Batch  913/1173   train_loss = 3.358\n",
      "Epoch  19 Batch  938/1173   train_loss = 3.421\n",
      "Epoch  19 Batch  963/1173   train_loss = 3.417\n",
      "Epoch  19 Batch  988/1173   train_loss = 3.359\n",
      "Epoch  19 Batch 1013/1173   train_loss = 3.352\n",
      "Epoch  19 Batch 1038/1173   train_loss = 3.538\n",
      "Epoch  19 Batch 1063/1173   train_loss = 3.240\n",
      "Epoch  19 Batch 1088/1173   train_loss = 3.433\n",
      "Epoch  19 Batch 1113/1173   train_loss = 3.446\n",
      "Epoch  19 Batch 1138/1173   train_loss = 3.433\n",
      "Epoch  19 Batch 1163/1173   train_loss = 3.292\n",
      "Epoch  20 Batch   15/1173   train_loss = 3.452\n",
      "Epoch  20 Batch   40/1173   train_loss = 3.309\n",
      "Epoch  20 Batch   65/1173   train_loss = 3.489\n",
      "Epoch  20 Batch   90/1173   train_loss = 3.442\n",
      "Epoch  20 Batch  115/1173   train_loss = 3.402\n",
      "Epoch  20 Batch  140/1173   train_loss = 3.356\n",
      "Epoch  20 Batch  165/1173   train_loss = 3.512\n",
      "Epoch  20 Batch  190/1173   train_loss = 3.422\n",
      "Epoch  20 Batch  215/1173   train_loss = 3.451\n",
      "Epoch  20 Batch  240/1173   train_loss = 3.428\n",
      "Epoch  20 Batch  265/1173   train_loss = 3.358\n",
      "Epoch  20 Batch  290/1173   train_loss = 3.319\n",
      "Epoch  20 Batch  315/1173   train_loss = 3.486\n",
      "Epoch  20 Batch  340/1173   train_loss = 3.476\n",
      "Epoch  20 Batch  365/1173   train_loss = 3.403\n",
      "Epoch  20 Batch  390/1173   train_loss = 3.347\n",
      "Epoch  20 Batch  415/1173   train_loss = 3.561\n",
      "Epoch  20 Batch  440/1173   train_loss = 3.384\n",
      "Epoch  20 Batch  465/1173   train_loss = 3.439\n",
      "Epoch  20 Batch  490/1173   train_loss = 3.405\n",
      "Epoch  20 Batch  515/1173   train_loss = 3.378\n",
      "Epoch  20 Batch  540/1173   train_loss = 3.505\n",
      "Epoch  20 Batch  565/1173   train_loss = 3.398\n",
      "Epoch  20 Batch  590/1173   train_loss = 3.390\n",
      "Epoch  20 Batch  615/1173   train_loss = 3.383\n",
      "Epoch  20 Batch  640/1173   train_loss = 3.487\n",
      "Epoch  20 Batch  665/1173   train_loss = 3.307\n",
      "Epoch  20 Batch  690/1173   train_loss = 3.356\n",
      "Epoch  20 Batch  715/1173   train_loss = 3.406\n",
      "Epoch  20 Batch  740/1173   train_loss = 3.357\n",
      "Epoch  20 Batch  765/1173   train_loss = 3.399\n",
      "Epoch  20 Batch  790/1173   train_loss = 3.443\n",
      "Epoch  20 Batch  815/1173   train_loss = 3.474\n",
      "Epoch  20 Batch  840/1173   train_loss = 3.409\n",
      "Epoch  20 Batch  865/1173   train_loss = 3.581\n",
      "Epoch  20 Batch  890/1173   train_loss = 3.423\n",
      "Epoch  20 Batch  915/1173   train_loss = 3.431\n",
      "Epoch  20 Batch  940/1173   train_loss = 3.441\n",
      "Epoch  20 Batch  965/1173   train_loss = 3.417\n",
      "Epoch  20 Batch  990/1173   train_loss = 3.324\n",
      "Epoch  20 Batch 1015/1173   train_loss = 3.378\n",
      "Epoch  20 Batch 1040/1173   train_loss = 3.362\n",
      "Epoch  20 Batch 1065/1173   train_loss = 3.307\n",
      "Epoch  20 Batch 1090/1173   train_loss = 3.408\n",
      "Epoch  20 Batch 1115/1173   train_loss = 3.261\n",
      "Epoch  20 Batch 1140/1173   train_loss = 3.310\n",
      "Epoch  20 Batch 1165/1173   train_loss = 3.407\n",
      "Epoch  21 Batch   17/1173   train_loss = 3.347\n",
      "Epoch  21 Batch   42/1173   train_loss = 3.338\n",
      "Epoch  21 Batch   67/1173   train_loss = 3.439\n",
      "Epoch  21 Batch   92/1173   train_loss = 3.425\n",
      "Epoch  21 Batch  117/1173   train_loss = 3.429\n",
      "Epoch  21 Batch  142/1173   train_loss = 3.434\n",
      "Epoch  21 Batch  167/1173   train_loss = 3.431\n",
      "Epoch  21 Batch  192/1173   train_loss = 3.279\n",
      "Epoch  21 Batch  217/1173   train_loss = 3.261\n",
      "Epoch  21 Batch  242/1173   train_loss = 3.447\n",
      "Epoch  21 Batch  267/1173   train_loss = 3.444\n",
      "Epoch  21 Batch  292/1173   train_loss = 3.249\n",
      "Epoch  21 Batch  317/1173   train_loss = 3.512\n",
      "Epoch  21 Batch  342/1173   train_loss = 3.400\n",
      "Epoch  21 Batch  367/1173   train_loss = 3.368\n",
      "Epoch  21 Batch  392/1173   train_loss = 3.289\n",
      "Epoch  21 Batch  417/1173   train_loss = 3.488\n",
      "Epoch  21 Batch  442/1173   train_loss = 3.457\n",
      "Epoch  21 Batch  467/1173   train_loss = 3.388\n",
      "Epoch  21 Batch  492/1173   train_loss = 3.409\n",
      "Epoch  21 Batch  517/1173   train_loss = 3.321\n",
      "Epoch  21 Batch  542/1173   train_loss = 3.539\n",
      "Epoch  21 Batch  567/1173   train_loss = 3.316\n",
      "Epoch  21 Batch  592/1173   train_loss = 3.352\n",
      "Epoch  21 Batch  617/1173   train_loss = 3.329\n",
      "Epoch  21 Batch  642/1173   train_loss = 3.449\n",
      "Epoch  21 Batch  667/1173   train_loss = 3.335\n",
      "Epoch  21 Batch  692/1173   train_loss = 3.222\n",
      "Epoch  21 Batch  717/1173   train_loss = 3.483\n",
      "Epoch  21 Batch  742/1173   train_loss = 3.410\n",
      "Epoch  21 Batch  767/1173   train_loss = 3.346\n",
      "Epoch  21 Batch  792/1173   train_loss = 3.290\n",
      "Epoch  21 Batch  817/1173   train_loss = 3.549\n",
      "Epoch  21 Batch  842/1173   train_loss = 3.360\n",
      "Epoch  21 Batch  867/1173   train_loss = 3.384\n",
      "Epoch  21 Batch  892/1173   train_loss = 3.482\n",
      "Epoch  21 Batch  917/1173   train_loss = 3.408\n",
      "Epoch  21 Batch  942/1173   train_loss = 3.365\n",
      "Epoch  21 Batch  967/1173   train_loss = 3.327\n",
      "Epoch  21 Batch  992/1173   train_loss = 3.266\n",
      "Epoch  21 Batch 1017/1173   train_loss = 3.360\n",
      "Epoch  21 Batch 1042/1173   train_loss = 3.301\n",
      "Epoch  21 Batch 1067/1173   train_loss = 3.407\n",
      "Epoch  21 Batch 1092/1173   train_loss = 3.424\n",
      "Epoch  21 Batch 1117/1173   train_loss = 3.348\n",
      "Epoch  21 Batch 1142/1173   train_loss = 3.371\n",
      "Epoch  21 Batch 1167/1173   train_loss = 3.262\n",
      "Epoch  22 Batch   19/1173   train_loss = 3.322\n",
      "Epoch  22 Batch   44/1173   train_loss = 3.430\n",
      "Epoch  22 Batch   69/1173   train_loss = 3.312\n",
      "Epoch  22 Batch   94/1173   train_loss = 3.508\n",
      "Epoch  22 Batch  119/1173   train_loss = 3.399\n",
      "Epoch  22 Batch  144/1173   train_loss = 3.490\n",
      "Epoch  22 Batch  169/1173   train_loss = 3.408\n",
      "Epoch  22 Batch  194/1173   train_loss = 3.402\n",
      "Epoch  22 Batch  219/1173   train_loss = 3.378\n",
      "Epoch  22 Batch  244/1173   train_loss = 3.443\n",
      "Epoch  22 Batch  269/1173   train_loss = 3.468\n",
      "Epoch  22 Batch  294/1173   train_loss = 3.356\n",
      "Epoch  22 Batch  319/1173   train_loss = 3.399\n",
      "Epoch  22 Batch  344/1173   train_loss = 3.249\n",
      "Epoch  22 Batch  369/1173   train_loss = 3.242\n",
      "Epoch  22 Batch  394/1173   train_loss = 3.363\n",
      "Epoch  22 Batch  419/1173   train_loss = 3.402\n",
      "Epoch  22 Batch  444/1173   train_loss = 3.244\n",
      "Epoch  22 Batch  469/1173   train_loss = 3.458\n",
      "Epoch  22 Batch  494/1173   train_loss = 3.330\n",
      "Epoch  22 Batch  519/1173   train_loss = 3.296\n",
      "Epoch  22 Batch  544/1173   train_loss = 3.388\n",
      "Epoch  22 Batch  569/1173   train_loss = 3.306\n",
      "Epoch  22 Batch  594/1173   train_loss = 3.403\n",
      "Epoch  22 Batch  619/1173   train_loss = 3.489\n",
      "Epoch  22 Batch  644/1173   train_loss = 3.257\n",
      "Epoch  22 Batch  669/1173   train_loss = 3.301\n",
      "Epoch  22 Batch  694/1173   train_loss = 3.389\n",
      "Epoch  22 Batch  719/1173   train_loss = 3.219\n",
      "Epoch  22 Batch  744/1173   train_loss = 3.347\n",
      "Epoch  22 Batch  769/1173   train_loss = 3.405\n",
      "Epoch  22 Batch  794/1173   train_loss = 3.294\n",
      "Epoch  22 Batch  819/1173   train_loss = 3.406\n",
      "Epoch  22 Batch  844/1173   train_loss = 3.375\n",
      "Epoch  22 Batch  869/1173   train_loss = 3.267\n",
      "Epoch  22 Batch  894/1173   train_loss = 3.406\n",
      "Epoch  22 Batch  919/1173   train_loss = 3.352\n",
      "Epoch  22 Batch  944/1173   train_loss = 3.369\n",
      "Epoch  22 Batch  969/1173   train_loss = 3.269\n",
      "Epoch  22 Batch  994/1173   train_loss = 3.340\n",
      "Epoch  22 Batch 1019/1173   train_loss = 3.259\n",
      "Epoch  22 Batch 1044/1173   train_loss = 3.531\n",
      "Epoch  22 Batch 1069/1173   train_loss = 3.278\n",
      "Epoch  22 Batch 1094/1173   train_loss = 3.222\n",
      "Epoch  22 Batch 1119/1173   train_loss = 3.226\n",
      "Epoch  22 Batch 1144/1173   train_loss = 3.330\n",
      "Epoch  22 Batch 1169/1173   train_loss = 3.280\n",
      "Epoch  23 Batch   21/1173   train_loss = 3.239\n",
      "Epoch  23 Batch   46/1173   train_loss = 3.346\n",
      "Epoch  23 Batch   71/1173   train_loss = 3.484\n",
      "Epoch  23 Batch   96/1173   train_loss = 3.253\n",
      "Epoch  23 Batch  121/1173   train_loss = 3.370\n",
      "Epoch  23 Batch  146/1173   train_loss = 3.477\n",
      "Epoch  23 Batch  171/1173   train_loss = 3.385\n",
      "Epoch  23 Batch  196/1173   train_loss = 3.470\n",
      "Epoch  23 Batch  221/1173   train_loss = 3.370\n",
      "Epoch  23 Batch  246/1173   train_loss = 3.502\n",
      "Epoch  23 Batch  271/1173   train_loss = 3.413\n",
      "Epoch  23 Batch  296/1173   train_loss = 3.357\n",
      "Epoch  23 Batch  321/1173   train_loss = 3.384\n",
      "Epoch  23 Batch  346/1173   train_loss = 3.439\n",
      "Epoch  23 Batch  371/1173   train_loss = 3.367\n",
      "Epoch  23 Batch  396/1173   train_loss = 3.394\n",
      "Epoch  23 Batch  421/1173   train_loss = 3.234\n",
      "Epoch  23 Batch  446/1173   train_loss = 3.342\n",
      "Epoch  23 Batch  471/1173   train_loss = 3.333\n",
      "Epoch  23 Batch  496/1173   train_loss = 3.452\n",
      "Epoch  23 Batch  521/1173   train_loss = 3.286\n",
      "Epoch  23 Batch  546/1173   train_loss = 3.436\n",
      "Epoch  23 Batch  571/1173   train_loss = 3.435\n",
      "Epoch  23 Batch  596/1173   train_loss = 3.401\n",
      "Epoch  23 Batch  621/1173   train_loss = 3.382\n",
      "Epoch  23 Batch  646/1173   train_loss = 3.454\n",
      "Epoch  23 Batch  671/1173   train_loss = 3.462\n",
      "Epoch  23 Batch  696/1173   train_loss = 3.424\n",
      "Epoch  23 Batch  721/1173   train_loss = 3.324\n",
      "Epoch  23 Batch  746/1173   train_loss = 3.303\n",
      "Epoch  23 Batch  771/1173   train_loss = 3.299\n",
      "Epoch  23 Batch  796/1173   train_loss = 3.212\n",
      "Epoch  23 Batch  821/1173   train_loss = 3.324\n",
      "Epoch  23 Batch  846/1173   train_loss = 3.315\n",
      "Epoch  23 Batch  871/1173   train_loss = 3.350\n",
      "Epoch  23 Batch  896/1173   train_loss = 3.403\n",
      "Epoch  23 Batch  921/1173   train_loss = 3.396\n",
      "Epoch  23 Batch  946/1173   train_loss = 3.437\n",
      "Epoch  23 Batch  971/1173   train_loss = 3.250\n",
      "Epoch  23 Batch  996/1173   train_loss = 3.379\n",
      "Epoch  23 Batch 1021/1173   train_loss = 3.406\n",
      "Epoch  23 Batch 1046/1173   train_loss = 3.321\n",
      "Epoch  23 Batch 1071/1173   train_loss = 3.346\n",
      "Epoch  23 Batch 1096/1173   train_loss = 3.382\n",
      "Epoch  23 Batch 1121/1173   train_loss = 3.392\n",
      "Epoch  23 Batch 1146/1173   train_loss = 3.341\n",
      "Epoch  23 Batch 1171/1173   train_loss = 3.330\n",
      "Epoch  24 Batch   23/1173   train_loss = 3.201\n",
      "Epoch  24 Batch   48/1173   train_loss = 3.345\n",
      "Epoch  24 Batch   73/1173   train_loss = 3.301\n",
      "Epoch  24 Batch   98/1173   train_loss = 3.370\n",
      "Epoch  24 Batch  123/1173   train_loss = 3.372\n",
      "Epoch  24 Batch  148/1173   train_loss = 3.334\n",
      "Epoch  24 Batch  173/1173   train_loss = 3.398\n",
      "Epoch  24 Batch  198/1173   train_loss = 3.326\n",
      "Epoch  24 Batch  223/1173   train_loss = 3.322\n",
      "Epoch  24 Batch  248/1173   train_loss = 3.348\n",
      "Epoch  24 Batch  273/1173   train_loss = 3.400\n",
      "Epoch  24 Batch  298/1173   train_loss = 3.376\n",
      "Epoch  24 Batch  323/1173   train_loss = 3.348\n",
      "Epoch  24 Batch  348/1173   train_loss = 3.424\n",
      "Epoch  24 Batch  373/1173   train_loss = 3.363\n",
      "Epoch  24 Batch  398/1173   train_loss = 3.400\n",
      "Epoch  24 Batch  423/1173   train_loss = 3.339\n",
      "Epoch  24 Batch  448/1173   train_loss = 3.396\n",
      "Epoch  24 Batch  473/1173   train_loss = 3.309\n",
      "Epoch  24 Batch  498/1173   train_loss = 3.444\n",
      "Epoch  24 Batch  523/1173   train_loss = 3.373\n",
      "Epoch  24 Batch  548/1173   train_loss = 3.353\n",
      "Epoch  24 Batch  573/1173   train_loss = 3.485\n",
      "Epoch  24 Batch  598/1173   train_loss = 3.374\n",
      "Epoch  24 Batch  623/1173   train_loss = 3.429\n",
      "Epoch  24 Batch  648/1173   train_loss = 3.411\n",
      "Epoch  24 Batch  673/1173   train_loss = 3.256\n",
      "Epoch  24 Batch  698/1173   train_loss = 3.440\n",
      "Epoch  24 Batch  723/1173   train_loss = 3.420\n",
      "Epoch  24 Batch  748/1173   train_loss = 3.263\n",
      "Epoch  24 Batch  773/1173   train_loss = 3.379\n",
      "Epoch  24 Batch  798/1173   train_loss = 3.342\n",
      "Epoch  24 Batch  823/1173   train_loss = 3.286\n",
      "Epoch  24 Batch  848/1173   train_loss = 3.358\n",
      "Epoch  24 Batch  873/1173   train_loss = 3.209\n",
      "Epoch  24 Batch  898/1173   train_loss = 3.528\n",
      "Epoch  24 Batch  923/1173   train_loss = 3.473\n",
      "Epoch  24 Batch  948/1173   train_loss = 3.428\n",
      "Epoch  24 Batch  973/1173   train_loss = 3.184\n",
      "Epoch  24 Batch  998/1173   train_loss = 3.323\n",
      "Epoch  24 Batch 1023/1173   train_loss = 3.373\n",
      "Epoch  24 Batch 1048/1173   train_loss = 3.361\n",
      "Epoch  24 Batch 1073/1173   train_loss = 3.414\n",
      "Epoch  24 Batch 1098/1173   train_loss = 3.373\n",
      "Epoch  24 Batch 1123/1173   train_loss = 3.254\n",
      "Epoch  24 Batch 1148/1173   train_loss = 3.226\n",
      "Epoch  25 Batch    0/1173   train_loss = 3.250\n",
      "Epoch  25 Batch   25/1173   train_loss = 3.319\n",
      "Epoch  25 Batch   50/1173   train_loss = 3.318\n",
      "Epoch  25 Batch   75/1173   train_loss = 3.494\n",
      "Epoch  25 Batch  100/1173   train_loss = 3.275\n",
      "Epoch  25 Batch  125/1173   train_loss = 3.424\n",
      "Epoch  25 Batch  150/1173   train_loss = 3.387\n",
      "Epoch  25 Batch  175/1173   train_loss = 3.352\n",
      "Epoch  25 Batch  200/1173   train_loss = 3.439\n",
      "Epoch  25 Batch  225/1173   train_loss = 3.492\n",
      "Epoch  25 Batch  250/1173   train_loss = 3.326\n",
      "Epoch  25 Batch  275/1173   train_loss = 3.406\n",
      "Epoch  25 Batch  300/1173   train_loss = 3.268\n",
      "Epoch  25 Batch  325/1173   train_loss = 3.308\n",
      "Epoch  25 Batch  350/1173   train_loss = 3.522\n",
      "Epoch  25 Batch  375/1173   train_loss = 3.345\n",
      "Epoch  25 Batch  400/1173   train_loss = 3.421\n",
      "Epoch  25 Batch  425/1173   train_loss = 3.424\n",
      "Epoch  25 Batch  450/1173   train_loss = 3.446\n",
      "Epoch  25 Batch  475/1173   train_loss = 3.334\n",
      "Epoch  25 Batch  500/1173   train_loss = 3.384\n",
      "Epoch  25 Batch  525/1173   train_loss = 3.220\n",
      "Epoch  25 Batch  550/1173   train_loss = 3.371\n",
      "Epoch  25 Batch  575/1173   train_loss = 3.348\n",
      "Epoch  25 Batch  600/1173   train_loss = 3.318\n",
      "Epoch  25 Batch  625/1173   train_loss = 3.360\n",
      "Epoch  25 Batch  650/1173   train_loss = 3.365\n",
      "Epoch  25 Batch  675/1173   train_loss = 3.312\n",
      "Epoch  25 Batch  700/1173   train_loss = 3.362\n",
      "Epoch  25 Batch  725/1173   train_loss = 3.286\n",
      "Epoch  25 Batch  750/1173   train_loss = 3.350\n",
      "Epoch  25 Batch  775/1173   train_loss = 3.270\n",
      "Epoch  25 Batch  800/1173   train_loss = 3.244\n",
      "Epoch  25 Batch  825/1173   train_loss = 3.279\n",
      "Epoch  25 Batch  850/1173   train_loss = 3.439\n",
      "Epoch  25 Batch  875/1173   train_loss = 3.229\n",
      "Epoch  25 Batch  900/1173   train_loss = 3.395\n",
      "Epoch  25 Batch  925/1173   train_loss = 3.496\n",
      "Epoch  25 Batch  950/1173   train_loss = 3.356\n",
      "Epoch  25 Batch  975/1173   train_loss = 3.358\n",
      "Epoch  25 Batch 1000/1173   train_loss = 3.335\n",
      "Epoch  25 Batch 1025/1173   train_loss = 3.347\n",
      "Epoch  25 Batch 1050/1173   train_loss = 3.292\n",
      "Epoch  25 Batch 1075/1173   train_loss = 3.334\n",
      "Epoch  25 Batch 1100/1173   train_loss = 3.240\n",
      "Epoch  25 Batch 1125/1173   train_loss = 3.331\n",
      "Epoch  25 Batch 1150/1173   train_loss = 3.311\n",
      "Epoch  26 Batch    2/1173   train_loss = 3.314\n",
      "Epoch  26 Batch   27/1173   train_loss = 3.255\n",
      "Epoch  26 Batch   52/1173   train_loss = 3.453\n",
      "Epoch  26 Batch   77/1173   train_loss = 3.220\n",
      "Epoch  26 Batch  102/1173   train_loss = 3.454\n",
      "Epoch  26 Batch  127/1173   train_loss = 3.405\n",
      "Epoch  26 Batch  152/1173   train_loss = 3.503\n",
      "Epoch  26 Batch  177/1173   train_loss = 3.297\n",
      "Epoch  26 Batch  202/1173   train_loss = 3.337\n",
      "Epoch  26 Batch  227/1173   train_loss = 3.340\n",
      "Epoch  26 Batch  252/1173   train_loss = 3.293\n",
      "Epoch  26 Batch  277/1173   train_loss = 3.385\n",
      "Epoch  26 Batch  302/1173   train_loss = 3.306\n",
      "Epoch  26 Batch  327/1173   train_loss = 3.345\n",
      "Epoch  26 Batch  352/1173   train_loss = 3.325\n",
      "Epoch  26 Batch  377/1173   train_loss = 3.381\n",
      "Epoch  26 Batch  402/1173   train_loss = 3.374\n",
      "Epoch  26 Batch  427/1173   train_loss = 3.251\n",
      "Epoch  26 Batch  452/1173   train_loss = 3.382\n",
      "Epoch  26 Batch  477/1173   train_loss = 3.328\n",
      "Epoch  26 Batch  502/1173   train_loss = 3.359\n",
      "Epoch  26 Batch  527/1173   train_loss = 3.243\n",
      "Epoch  26 Batch  552/1173   train_loss = 3.310\n",
      "Epoch  26 Batch  577/1173   train_loss = 3.219\n",
      "Epoch  26 Batch  602/1173   train_loss = 3.311\n",
      "Epoch  26 Batch  627/1173   train_loss = 3.327\n",
      "Epoch  26 Batch  652/1173   train_loss = 3.119\n",
      "Epoch  26 Batch  677/1173   train_loss = 3.441\n",
      "Epoch  26 Batch  702/1173   train_loss = 3.330\n",
      "Epoch  26 Batch  727/1173   train_loss = 3.356\n",
      "Epoch  26 Batch  752/1173   train_loss = 3.245\n",
      "Epoch  26 Batch  777/1173   train_loss = 3.281\n",
      "Epoch  26 Batch  802/1173   train_loss = 3.368\n",
      "Epoch  26 Batch  827/1173   train_loss = 3.281\n",
      "Epoch  26 Batch  852/1173   train_loss = 3.390\n",
      "Epoch  26 Batch  877/1173   train_loss = 3.261\n",
      "Epoch  26 Batch  902/1173   train_loss = 3.333\n",
      "Epoch  26 Batch  927/1173   train_loss = 3.345\n",
      "Epoch  26 Batch  952/1173   train_loss = 3.279\n",
      "Epoch  26 Batch  977/1173   train_loss = 3.338\n",
      "Epoch  26 Batch 1002/1173   train_loss = 3.301\n",
      "Epoch  26 Batch 1027/1173   train_loss = 3.340\n",
      "Epoch  26 Batch 1052/1173   train_loss = 3.234\n",
      "Epoch  26 Batch 1077/1173   train_loss = 3.270\n",
      "Epoch  26 Batch 1102/1173   train_loss = 3.283\n",
      "Epoch  26 Batch 1127/1173   train_loss = 3.312\n",
      "Epoch  26 Batch 1152/1173   train_loss = 3.160\n",
      "Epoch  27 Batch    4/1173   train_loss = 3.296\n",
      "Epoch  27 Batch   29/1173   train_loss = 3.331\n",
      "Epoch  27 Batch   54/1173   train_loss = 3.364\n",
      "Epoch  27 Batch   79/1173   train_loss = 3.297\n",
      "Epoch  27 Batch  104/1173   train_loss = 3.276\n",
      "Epoch  27 Batch  129/1173   train_loss = 3.284\n",
      "Epoch  27 Batch  154/1173   train_loss = 3.362\n",
      "Epoch  27 Batch  179/1173   train_loss = 3.412\n",
      "Epoch  27 Batch  204/1173   train_loss = 3.345\n",
      "Epoch  27 Batch  229/1173   train_loss = 3.306\n",
      "Epoch  27 Batch  254/1173   train_loss = 3.256\n",
      "Epoch  27 Batch  279/1173   train_loss = 3.221\n",
      "Epoch  27 Batch  304/1173   train_loss = 3.337\n",
      "Epoch  27 Batch  329/1173   train_loss = 3.464\n",
      "Epoch  27 Batch  354/1173   train_loss = 3.263\n",
      "Epoch  27 Batch  379/1173   train_loss = 3.307\n",
      "Epoch  27 Batch  404/1173   train_loss = 3.336\n",
      "Epoch  27 Batch  429/1173   train_loss = 3.232\n",
      "Epoch  27 Batch  454/1173   train_loss = 3.401\n",
      "Epoch  27 Batch  479/1173   train_loss = 3.320\n",
      "Epoch  27 Batch  504/1173   train_loss = 3.391\n",
      "Epoch  27 Batch  529/1173   train_loss = 3.408\n",
      "Epoch  27 Batch  554/1173   train_loss = 3.343\n",
      "Epoch  27 Batch  579/1173   train_loss = 3.411\n",
      "Epoch  27 Batch  604/1173   train_loss = 3.422\n",
      "Epoch  27 Batch  629/1173   train_loss = 3.354\n",
      "Epoch  27 Batch  654/1173   train_loss = 3.305\n",
      "Epoch  27 Batch  679/1173   train_loss = 3.471\n",
      "Epoch  27 Batch  704/1173   train_loss = 3.259\n",
      "Epoch  27 Batch  729/1173   train_loss = 3.225\n",
      "Epoch  27 Batch  754/1173   train_loss = 3.409\n",
      "Epoch  27 Batch  779/1173   train_loss = 3.257\n",
      "Epoch  27 Batch  804/1173   train_loss = 3.377\n",
      "Epoch  27 Batch  829/1173   train_loss = 3.425\n",
      "Epoch  27 Batch  854/1173   train_loss = 3.469\n",
      "Epoch  27 Batch  879/1173   train_loss = 3.207\n",
      "Epoch  27 Batch  904/1173   train_loss = 3.283\n",
      "Epoch  27 Batch  929/1173   train_loss = 3.320\n",
      "Epoch  27 Batch  954/1173   train_loss = 3.225\n",
      "Epoch  27 Batch  979/1173   train_loss = 3.322\n",
      "Epoch  27 Batch 1004/1173   train_loss = 3.273\n",
      "Epoch  27 Batch 1029/1173   train_loss = 3.356\n",
      "Epoch  27 Batch 1054/1173   train_loss = 3.242\n",
      "Epoch  27 Batch 1079/1173   train_loss = 3.353\n",
      "Epoch  27 Batch 1104/1173   train_loss = 3.338\n",
      "Epoch  27 Batch 1129/1173   train_loss = 3.164\n",
      "Epoch  27 Batch 1154/1173   train_loss = 3.239\n",
      "Epoch  28 Batch    6/1173   train_loss = 3.389\n",
      "Epoch  28 Batch   31/1173   train_loss = 3.208\n",
      "Epoch  28 Batch   56/1173   train_loss = 3.275\n",
      "Epoch  28 Batch   81/1173   train_loss = 3.471\n",
      "Epoch  28 Batch  106/1173   train_loss = 3.285\n",
      "Epoch  28 Batch  131/1173   train_loss = 3.337\n",
      "Epoch  28 Batch  156/1173   train_loss = 3.327\n",
      "Epoch  28 Batch  181/1173   train_loss = 3.406\n",
      "Epoch  28 Batch  206/1173   train_loss = 3.309\n",
      "Epoch  28 Batch  231/1173   train_loss = 3.510\n",
      "Epoch  28 Batch  256/1173   train_loss = 3.241\n",
      "Epoch  28 Batch  281/1173   train_loss = 3.208\n",
      "Epoch  28 Batch  306/1173   train_loss = 3.405\n",
      "Epoch  28 Batch  331/1173   train_loss = 3.374\n",
      "Epoch  28 Batch  356/1173   train_loss = 3.259\n",
      "Epoch  28 Batch  381/1173   train_loss = 3.238\n",
      "Epoch  28 Batch  406/1173   train_loss = 3.405\n",
      "Epoch  28 Batch  431/1173   train_loss = 3.313\n",
      "Epoch  28 Batch  456/1173   train_loss = 3.437\n",
      "Epoch  28 Batch  481/1173   train_loss = 3.291\n",
      "Epoch  28 Batch  506/1173   train_loss = 3.440\n",
      "Epoch  28 Batch  531/1173   train_loss = 3.361\n",
      "Epoch  28 Batch  556/1173   train_loss = 3.160\n",
      "Epoch  28 Batch  581/1173   train_loss = 3.388\n",
      "Epoch  28 Batch  606/1173   train_loss = 3.436\n",
      "Epoch  28 Batch  631/1173   train_loss = 3.361\n",
      "Epoch  28 Batch  656/1173   train_loss = 3.346\n",
      "Epoch  28 Batch  681/1173   train_loss = 3.404\n",
      "Epoch  28 Batch  706/1173   train_loss = 3.397\n",
      "Epoch  28 Batch  731/1173   train_loss = 3.320\n",
      "Epoch  28 Batch  756/1173   train_loss = 3.345\n",
      "Epoch  28 Batch  781/1173   train_loss = 3.263\n",
      "Epoch  28 Batch  806/1173   train_loss = 3.270\n",
      "Epoch  28 Batch  831/1173   train_loss = 3.350\n",
      "Epoch  28 Batch  856/1173   train_loss = 3.366\n",
      "Epoch  28 Batch  881/1173   train_loss = 3.383\n",
      "Epoch  28 Batch  906/1173   train_loss = 3.232\n",
      "Epoch  28 Batch  931/1173   train_loss = 3.301\n",
      "Epoch  28 Batch  956/1173   train_loss = 3.268\n",
      "Epoch  28 Batch  981/1173   train_loss = 3.269\n",
      "Epoch  28 Batch 1006/1173   train_loss = 3.272\n",
      "Epoch  28 Batch 1031/1173   train_loss = 3.301\n",
      "Epoch  28 Batch 1056/1173   train_loss = 3.332\n",
      "Epoch  28 Batch 1081/1173   train_loss = 3.310\n",
      "Epoch  28 Batch 1106/1173   train_loss = 3.354\n",
      "Epoch  28 Batch 1131/1173   train_loss = 3.192\n",
      "Epoch  28 Batch 1156/1173   train_loss = 3.316\n",
      "Epoch  29 Batch    8/1173   train_loss = 3.292\n",
      "Epoch  29 Batch   33/1173   train_loss = 3.164\n",
      "Epoch  29 Batch   58/1173   train_loss = 3.316\n",
      "Epoch  29 Batch   83/1173   train_loss = 3.357\n",
      "Epoch  29 Batch  108/1173   train_loss = 3.340\n",
      "Epoch  29 Batch  133/1173   train_loss = 3.333\n",
      "Epoch  29 Batch  158/1173   train_loss = 3.262\n",
      "Epoch  29 Batch  183/1173   train_loss = 3.295\n",
      "Epoch  29 Batch  208/1173   train_loss = 3.386\n",
      "Epoch  29 Batch  233/1173   train_loss = 3.288\n",
      "Epoch  29 Batch  258/1173   train_loss = 3.272\n",
      "Epoch  29 Batch  283/1173   train_loss = 3.286\n",
      "Epoch  29 Batch  308/1173   train_loss = 3.331\n",
      "Epoch  29 Batch  333/1173   train_loss = 3.530\n",
      "Epoch  29 Batch  358/1173   train_loss = 3.259\n",
      "Epoch  29 Batch  383/1173   train_loss = 3.343\n",
      "Epoch  29 Batch  408/1173   train_loss = 3.377\n",
      "Epoch  29 Batch  433/1173   train_loss = 3.280\n",
      "Epoch  29 Batch  458/1173   train_loss = 3.351\n",
      "Epoch  29 Batch  483/1173   train_loss = 3.342\n",
      "Epoch  29 Batch  508/1173   train_loss = 3.326\n",
      "Epoch  29 Batch  533/1173   train_loss = 3.341\n",
      "Epoch  29 Batch  558/1173   train_loss = 3.161\n",
      "Epoch  29 Batch  583/1173   train_loss = 3.285\n",
      "Epoch  29 Batch  608/1173   train_loss = 3.400\n",
      "Epoch  29 Batch  633/1173   train_loss = 3.329\n",
      "Epoch  29 Batch  658/1173   train_loss = 3.237\n",
      "Epoch  29 Batch  683/1173   train_loss = 3.301\n",
      "Epoch  29 Batch  708/1173   train_loss = 3.356\n",
      "Epoch  29 Batch  733/1173   train_loss = 3.322\n",
      "Epoch  29 Batch  758/1173   train_loss = 3.326\n",
      "Epoch  29 Batch  783/1173   train_loss = 3.347\n",
      "Epoch  29 Batch  808/1173   train_loss = 3.215\n",
      "Epoch  29 Batch  833/1173   train_loss = 3.375\n",
      "Epoch  29 Batch  858/1173   train_loss = 3.362\n",
      "Epoch  29 Batch  883/1173   train_loss = 3.306\n",
      "Epoch  29 Batch  908/1173   train_loss = 3.252\n",
      "Epoch  29 Batch  933/1173   train_loss = 3.289\n",
      "Epoch  29 Batch  958/1173   train_loss = 3.159\n",
      "Epoch  29 Batch  983/1173   train_loss = 3.242\n",
      "Epoch  29 Batch 1008/1173   train_loss = 3.194\n",
      "Epoch  29 Batch 1033/1173   train_loss = 3.271\n",
      "Epoch  29 Batch 1058/1173   train_loss = 3.419\n",
      "Epoch  29 Batch 1083/1173   train_loss = 3.295\n",
      "Epoch  29 Batch 1108/1173   train_loss = 3.193\n",
      "Epoch  29 Batch 1133/1173   train_loss = 3.305\n",
      "Epoch  29 Batch 1158/1173   train_loss = 3.139\n",
      "Epoch  30 Batch   10/1173   train_loss = 3.380\n",
      "Epoch  30 Batch   35/1173   train_loss = 3.332\n",
      "Epoch  30 Batch   60/1173   train_loss = 3.462\n",
      "Epoch  30 Batch   85/1173   train_loss = 3.361\n",
      "Epoch  30 Batch  110/1173   train_loss = 3.338\n",
      "Epoch  30 Batch  135/1173   train_loss = 3.313\n",
      "Epoch  30 Batch  160/1173   train_loss = 3.358\n",
      "Epoch  30 Batch  185/1173   train_loss = 3.362\n",
      "Epoch  30 Batch  210/1173   train_loss = 3.270\n",
      "Epoch  30 Batch  235/1173   train_loss = 3.408\n",
      "Epoch  30 Batch  260/1173   train_loss = 3.257\n",
      "Epoch  30 Batch  285/1173   train_loss = 3.254\n",
      "Epoch  30 Batch  310/1173   train_loss = 3.188\n",
      "Epoch  30 Batch  335/1173   train_loss = 3.342\n",
      "Epoch  30 Batch  360/1173   train_loss = 3.171\n",
      "Epoch  30 Batch  385/1173   train_loss = 3.233\n",
      "Epoch  30 Batch  410/1173   train_loss = 3.375\n",
      "Epoch  30 Batch  435/1173   train_loss = 3.198\n",
      "Epoch  30 Batch  460/1173   train_loss = 3.289\n",
      "Epoch  30 Batch  485/1173   train_loss = 3.267\n",
      "Epoch  30 Batch  510/1173   train_loss = 3.387\n",
      "Epoch  30 Batch  535/1173   train_loss = 3.325\n",
      "Epoch  30 Batch  560/1173   train_loss = 3.399\n",
      "Epoch  30 Batch  585/1173   train_loss = 3.269\n",
      "Epoch  30 Batch  610/1173   train_loss = 3.387\n",
      "Epoch  30 Batch  635/1173   train_loss = 3.261\n",
      "Epoch  30 Batch  660/1173   train_loss = 3.189\n",
      "Epoch  30 Batch  685/1173   train_loss = 3.298\n",
      "Epoch  30 Batch  710/1173   train_loss = 3.271\n",
      "Epoch  30 Batch  735/1173   train_loss = 3.248\n",
      "Epoch  30 Batch  760/1173   train_loss = 3.329\n",
      "Epoch  30 Batch  785/1173   train_loss = 3.171\n",
      "Epoch  30 Batch  810/1173   train_loss = 3.368\n",
      "Epoch  30 Batch  835/1173   train_loss = 3.185\n",
      "Epoch  30 Batch  860/1173   train_loss = 3.256\n",
      "Epoch  30 Batch  885/1173   train_loss = 3.310\n",
      "Epoch  30 Batch  910/1173   train_loss = 3.323\n",
      "Epoch  30 Batch  935/1173   train_loss = 3.259\n",
      "Epoch  30 Batch  960/1173   train_loss = 3.333\n",
      "Epoch  30 Batch  985/1173   train_loss = 3.306\n",
      "Epoch  30 Batch 1010/1173   train_loss = 3.327\n",
      "Epoch  30 Batch 1035/1173   train_loss = 3.257\n",
      "Epoch  30 Batch 1060/1173   train_loss = 3.121\n",
      "Epoch  30 Batch 1085/1173   train_loss = 3.252\n",
      "Epoch  30 Batch 1110/1173   train_loss = 3.143\n",
      "Epoch  30 Batch 1135/1173   train_loss = 3.259\n",
      "Epoch  30 Batch 1160/1173   train_loss = 3.336\n",
      "Epoch  31 Batch   12/1173   train_loss = 3.362\n",
      "Epoch  31 Batch   37/1173   train_loss = 3.206\n",
      "Epoch  31 Batch   62/1173   train_loss = 3.357\n",
      "Epoch  31 Batch   87/1173   train_loss = 3.311\n",
      "Epoch  31 Batch  112/1173   train_loss = 3.343\n",
      "Epoch  31 Batch  137/1173   train_loss = 3.227\n",
      "Epoch  31 Batch  162/1173   train_loss = 3.399\n",
      "Epoch  31 Batch  187/1173   train_loss = 3.410\n",
      "Epoch  31 Batch  212/1173   train_loss = 3.353\n",
      "Epoch  31 Batch  237/1173   train_loss = 3.279\n",
      "Epoch  31 Batch  262/1173   train_loss = 3.364\n",
      "Epoch  31 Batch  287/1173   train_loss = 3.276\n",
      "Epoch  31 Batch  312/1173   train_loss = 3.218\n",
      "Epoch  31 Batch  337/1173   train_loss = 3.329\n",
      "Epoch  31 Batch  362/1173   train_loss = 3.379\n",
      "Epoch  31 Batch  387/1173   train_loss = 3.213\n",
      "Epoch  31 Batch  412/1173   train_loss = 3.155\n",
      "Epoch  31 Batch  437/1173   train_loss = 3.297\n",
      "Epoch  31 Batch  462/1173   train_loss = 3.296\n",
      "Epoch  31 Batch  487/1173   train_loss = 3.314\n",
      "Epoch  31 Batch  512/1173   train_loss = 3.244\n",
      "Epoch  31 Batch  537/1173   train_loss = 3.240\n",
      "Epoch  31 Batch  562/1173   train_loss = 3.383\n",
      "Epoch  31 Batch  587/1173   train_loss = 3.295\n",
      "Epoch  31 Batch  612/1173   train_loss = 3.291\n",
      "Epoch  31 Batch  637/1173   train_loss = 3.304\n",
      "Epoch  31 Batch  662/1173   train_loss = 3.354\n",
      "Epoch  31 Batch  687/1173   train_loss = 3.266\n",
      "Epoch  31 Batch  712/1173   train_loss = 3.191\n",
      "Epoch  31 Batch  737/1173   train_loss = 3.443\n",
      "Epoch  31 Batch  762/1173   train_loss = 3.452\n",
      "Epoch  31 Batch  787/1173   train_loss = 3.353\n",
      "Epoch  31 Batch  812/1173   train_loss = 3.275\n",
      "Epoch  31 Batch  837/1173   train_loss = 3.307\n",
      "Epoch  31 Batch  862/1173   train_loss = 3.258\n",
      "Epoch  31 Batch  887/1173   train_loss = 3.291\n",
      "Epoch  31 Batch  912/1173   train_loss = 3.327\n",
      "Epoch  31 Batch  937/1173   train_loss = 3.329\n",
      "Epoch  31 Batch  962/1173   train_loss = 3.275\n",
      "Epoch  31 Batch  987/1173   train_loss = 3.202\n",
      "Epoch  31 Batch 1012/1173   train_loss = 3.243\n",
      "Epoch  31 Batch 1037/1173   train_loss = 3.266\n",
      "Epoch  31 Batch 1062/1173   train_loss = 3.268\n",
      "Epoch  31 Batch 1087/1173   train_loss = 3.275\n",
      "Epoch  31 Batch 1112/1173   train_loss = 3.197\n",
      "Epoch  31 Batch 1137/1173   train_loss = 3.205\n",
      "Epoch  31 Batch 1162/1173   train_loss = 3.292\n",
      "Epoch  32 Batch   14/1173   train_loss = 3.308\n",
      "Epoch  32 Batch   39/1173   train_loss = 3.321\n",
      "Epoch  32 Batch   64/1173   train_loss = 3.312\n",
      "Epoch  32 Batch   89/1173   train_loss = 3.346\n",
      "Epoch  32 Batch  114/1173   train_loss = 3.319\n",
      "Epoch  32 Batch  139/1173   train_loss = 3.333\n",
      "Epoch  32 Batch  164/1173   train_loss = 3.359\n",
      "Epoch  32 Batch  189/1173   train_loss = 3.423\n",
      "Epoch  32 Batch  214/1173   train_loss = 3.229\n",
      "Epoch  32 Batch  239/1173   train_loss = 3.344\n",
      "Epoch  32 Batch  264/1173   train_loss = 3.158\n",
      "Epoch  32 Batch  289/1173   train_loss = 3.274\n",
      "Epoch  32 Batch  314/1173   train_loss = 3.445\n",
      "Epoch  32 Batch  339/1173   train_loss = 3.319\n",
      "Epoch  32 Batch  364/1173   train_loss = 3.319\n",
      "Epoch  32 Batch  389/1173   train_loss = 3.202\n",
      "Epoch  32 Batch  414/1173   train_loss = 3.339\n",
      "Epoch  32 Batch  439/1173   train_loss = 3.197\n",
      "Epoch  32 Batch  464/1173   train_loss = 3.322\n",
      "Epoch  32 Batch  489/1173   train_loss = 3.373\n",
      "Epoch  32 Batch  514/1173   train_loss = 3.434\n",
      "Epoch  32 Batch  539/1173   train_loss = 3.356\n",
      "Epoch  32 Batch  564/1173   train_loss = 3.204\n",
      "Epoch  32 Batch  589/1173   train_loss = 3.227\n",
      "Epoch  32 Batch  614/1173   train_loss = 3.312\n",
      "Epoch  32 Batch  639/1173   train_loss = 3.333\n",
      "Epoch  32 Batch  664/1173   train_loss = 3.253\n",
      "Epoch  32 Batch  689/1173   train_loss = 3.216\n",
      "Epoch  32 Batch  714/1173   train_loss = 3.211\n",
      "Epoch  32 Batch  739/1173   train_loss = 3.202\n",
      "Epoch  32 Batch  764/1173   train_loss = 3.250\n",
      "Epoch  32 Batch  789/1173   train_loss = 3.227\n",
      "Epoch  32 Batch  814/1173   train_loss = 3.180\n",
      "Epoch  32 Batch  839/1173   train_loss = 3.213\n",
      "Epoch  32 Batch  864/1173   train_loss = 3.224\n",
      "Epoch  32 Batch  889/1173   train_loss = 3.336\n",
      "Epoch  32 Batch  914/1173   train_loss = 3.310\n",
      "Epoch  32 Batch  939/1173   train_loss = 3.287\n",
      "Epoch  32 Batch  964/1173   train_loss = 3.264\n",
      "Epoch  32 Batch  989/1173   train_loss = 3.243\n",
      "Epoch  32 Batch 1014/1173   train_loss = 3.210\n",
      "Epoch  32 Batch 1039/1173   train_loss = 3.238\n",
      "Epoch  32 Batch 1064/1173   train_loss = 3.071\n",
      "Epoch  32 Batch 1089/1173   train_loss = 3.409\n",
      "Epoch  32 Batch 1114/1173   train_loss = 3.126\n",
      "Epoch  32 Batch 1139/1173   train_loss = 3.230\n",
      "Epoch  32 Batch 1164/1173   train_loss = 3.164\n",
      "Epoch  33 Batch   16/1173   train_loss = 3.350\n",
      "Epoch  33 Batch   41/1173   train_loss = 3.104\n",
      "Epoch  33 Batch   66/1173   train_loss = 3.260\n",
      "Epoch  33 Batch   91/1173   train_loss = 3.352\n",
      "Epoch  33 Batch  116/1173   train_loss = 3.352\n",
      "Epoch  33 Batch  141/1173   train_loss = 3.192\n",
      "Epoch  33 Batch  166/1173   train_loss = 3.259\n",
      "Epoch  33 Batch  191/1173   train_loss = 3.213\n",
      "Epoch  33 Batch  216/1173   train_loss = 3.253\n",
      "Epoch  33 Batch  241/1173   train_loss = 3.337\n",
      "Epoch  33 Batch  266/1173   train_loss = 3.196\n",
      "Epoch  33 Batch  291/1173   train_loss = 3.314\n",
      "Epoch  33 Batch  316/1173   train_loss = 3.340\n",
      "Epoch  33 Batch  341/1173   train_loss = 3.364\n",
      "Epoch  33 Batch  366/1173   train_loss = 3.263\n",
      "Epoch  33 Batch  391/1173   train_loss = 3.299\n",
      "Epoch  33 Batch  416/1173   train_loss = 3.289\n",
      "Epoch  33 Batch  441/1173   train_loss = 3.244\n",
      "Epoch  33 Batch  466/1173   train_loss = 3.165\n",
      "Epoch  33 Batch  491/1173   train_loss = 3.220\n",
      "Epoch  33 Batch  516/1173   train_loss = 3.310\n",
      "Epoch  33 Batch  541/1173   train_loss = 3.273\n",
      "Epoch  33 Batch  566/1173   train_loss = 3.448\n",
      "Epoch  33 Batch  591/1173   train_loss = 3.252\n",
      "Epoch  33 Batch  616/1173   train_loss = 3.293\n",
      "Epoch  33 Batch  641/1173   train_loss = 3.218\n",
      "Epoch  33 Batch  666/1173   train_loss = 3.282\n",
      "Epoch  33 Batch  691/1173   train_loss = 3.190\n",
      "Epoch  33 Batch  716/1173   train_loss = 3.379\n",
      "Epoch  33 Batch  741/1173   train_loss = 3.429\n",
      "Epoch  33 Batch  766/1173   train_loss = 3.340\n",
      "Epoch  33 Batch  791/1173   train_loss = 3.233\n",
      "Epoch  33 Batch  816/1173   train_loss = 3.324\n",
      "Epoch  33 Batch  841/1173   train_loss = 3.396\n",
      "Epoch  33 Batch  866/1173   train_loss = 3.260\n",
      "Epoch  33 Batch  891/1173   train_loss = 3.283\n",
      "Epoch  33 Batch  916/1173   train_loss = 3.323\n",
      "Epoch  33 Batch  941/1173   train_loss = 3.179\n",
      "Epoch  33 Batch  966/1173   train_loss = 3.355\n",
      "Epoch  33 Batch  991/1173   train_loss = 3.365\n",
      "Epoch  33 Batch 1016/1173   train_loss = 3.180\n",
      "Epoch  33 Batch 1041/1173   train_loss = 3.206\n",
      "Epoch  33 Batch 1066/1173   train_loss = 3.272\n",
      "Epoch  33 Batch 1091/1173   train_loss = 3.239\n",
      "Epoch  33 Batch 1116/1173   train_loss = 3.349\n",
      "Epoch  33 Batch 1141/1173   train_loss = 3.254\n",
      "Epoch  33 Batch 1166/1173   train_loss = 3.258\n",
      "Epoch  34 Batch   18/1173   train_loss = 3.350\n",
      "Epoch  34 Batch   43/1173   train_loss = 3.204\n",
      "Epoch  34 Batch   68/1173   train_loss = 3.296\n",
      "Epoch  34 Batch   93/1173   train_loss = 3.283\n",
      "Epoch  34 Batch  118/1173   train_loss = 3.416\n",
      "Epoch  34 Batch  143/1173   train_loss = 3.263\n",
      "Epoch  34 Batch  168/1173   train_loss = 3.160\n",
      "Epoch  34 Batch  193/1173   train_loss = 3.222\n",
      "Epoch  34 Batch  218/1173   train_loss = 3.129\n",
      "Epoch  34 Batch  243/1173   train_loss = 3.339\n",
      "Epoch  34 Batch  268/1173   train_loss = 3.225\n",
      "Epoch  34 Batch  293/1173   train_loss = 3.301\n",
      "Epoch  34 Batch  318/1173   train_loss = 3.224\n",
      "Epoch  34 Batch  343/1173   train_loss = 3.311\n",
      "Epoch  34 Batch  368/1173   train_loss = 3.299\n",
      "Epoch  34 Batch  393/1173   train_loss = 3.294\n",
      "Epoch  34 Batch  418/1173   train_loss = 3.307\n",
      "Epoch  34 Batch  443/1173   train_loss = 3.379\n",
      "Epoch  34 Batch  468/1173   train_loss = 3.271\n",
      "Epoch  34 Batch  493/1173   train_loss = 3.400\n",
      "Epoch  34 Batch  518/1173   train_loss = 3.237\n",
      "Epoch  34 Batch  543/1173   train_loss = 3.316\n",
      "Epoch  34 Batch  568/1173   train_loss = 3.318\n",
      "Epoch  34 Batch  593/1173   train_loss = 3.241\n",
      "Epoch  34 Batch  618/1173   train_loss = 3.348\n",
      "Epoch  34 Batch  643/1173   train_loss = 3.357\n",
      "Epoch  34 Batch  668/1173   train_loss = 3.155\n",
      "Epoch  34 Batch  693/1173   train_loss = 3.262\n",
      "Epoch  34 Batch  718/1173   train_loss = 3.126\n",
      "Epoch  34 Batch  743/1173   train_loss = 3.258\n",
      "Epoch  34 Batch  768/1173   train_loss = 3.306\n",
      "Epoch  34 Batch  793/1173   train_loss = 3.215\n",
      "Epoch  34 Batch  818/1173   train_loss = 3.352\n",
      "Epoch  34 Batch  843/1173   train_loss = 3.194\n",
      "Epoch  34 Batch  868/1173   train_loss = 3.237\n",
      "Epoch  34 Batch  893/1173   train_loss = 3.261\n",
      "Epoch  34 Batch  918/1173   train_loss = 3.284\n",
      "Epoch  34 Batch  943/1173   train_loss = 3.243\n",
      "Epoch  34 Batch  968/1173   train_loss = 3.327\n",
      "Epoch  34 Batch  993/1173   train_loss = 3.228\n",
      "Epoch  34 Batch 1018/1173   train_loss = 3.200\n",
      "Epoch  34 Batch 1043/1173   train_loss = 3.281\n",
      "Epoch  34 Batch 1068/1173   train_loss = 3.344\n",
      "Epoch  34 Batch 1093/1173   train_loss = 3.304\n",
      "Epoch  34 Batch 1118/1173   train_loss = 3.250\n",
      "Epoch  34 Batch 1143/1173   train_loss = 3.217\n",
      "Epoch  34 Batch 1168/1173   train_loss = 3.189\n",
      "Epoch  35 Batch   20/1173   train_loss = 3.201\n",
      "Epoch  35 Batch   45/1173   train_loss = 3.182\n",
      "Epoch  35 Batch   70/1173   train_loss = 3.250\n",
      "Epoch  35 Batch   95/1173   train_loss = 3.314\n",
      "Epoch  35 Batch  120/1173   train_loss = 3.296\n",
      "Epoch  35 Batch  145/1173   train_loss = 3.303\n",
      "Epoch  35 Batch  170/1173   train_loss = 3.317\n",
      "Epoch  35 Batch  195/1173   train_loss = 3.282\n",
      "Epoch  35 Batch  220/1173   train_loss = 3.285\n",
      "Epoch  35 Batch  245/1173   train_loss = 3.271\n",
      "Epoch  35 Batch  270/1173   train_loss = 3.269\n",
      "Epoch  35 Batch  295/1173   train_loss = 3.202\n",
      "Epoch  35 Batch  320/1173   train_loss = 3.244\n",
      "Epoch  35 Batch  345/1173   train_loss = 3.243\n",
      "Epoch  35 Batch  370/1173   train_loss = 3.240\n",
      "Epoch  35 Batch  395/1173   train_loss = 3.228\n",
      "Epoch  35 Batch  420/1173   train_loss = 3.348\n",
      "Epoch  35 Batch  445/1173   train_loss = 3.127\n",
      "Epoch  35 Batch  470/1173   train_loss = 3.299\n",
      "Epoch  35 Batch  495/1173   train_loss = 3.322\n",
      "Epoch  35 Batch  520/1173   train_loss = 3.267\n",
      "Epoch  35 Batch  545/1173   train_loss = 3.287\n",
      "Epoch  35 Batch  570/1173   train_loss = 3.303\n",
      "Epoch  35 Batch  595/1173   train_loss = 3.438\n",
      "Epoch  35 Batch  620/1173   train_loss = 3.366\n",
      "Epoch  35 Batch  645/1173   train_loss = 3.328\n",
      "Epoch  35 Batch  670/1173   train_loss = 3.263\n",
      "Epoch  35 Batch  695/1173   train_loss = 3.380\n",
      "Epoch  35 Batch  720/1173   train_loss = 3.209\n",
      "Epoch  35 Batch  745/1173   train_loss = 3.352\n",
      "Epoch  35 Batch  770/1173   train_loss = 3.326\n",
      "Epoch  35 Batch  795/1173   train_loss = 3.241\n",
      "Epoch  35 Batch  820/1173   train_loss = 3.259\n",
      "Epoch  35 Batch  845/1173   train_loss = 3.344\n",
      "Epoch  35 Batch  870/1173   train_loss = 3.296\n",
      "Epoch  35 Batch  895/1173   train_loss = 3.337\n",
      "Epoch  35 Batch  920/1173   train_loss = 3.290\n",
      "Epoch  35 Batch  945/1173   train_loss = 3.306\n",
      "Epoch  35 Batch  970/1173   train_loss = 3.204\n",
      "Epoch  35 Batch  995/1173   train_loss = 3.254\n",
      "Epoch  35 Batch 1020/1173   train_loss = 3.150\n",
      "Epoch  35 Batch 1045/1173   train_loss = 3.229\n",
      "Epoch  35 Batch 1070/1173   train_loss = 3.246\n",
      "Epoch  35 Batch 1095/1173   train_loss = 3.279\n",
      "Epoch  35 Batch 1120/1173   train_loss = 3.230\n",
      "Epoch  35 Batch 1145/1173   train_loss = 3.322\n",
      "Epoch  35 Batch 1170/1173   train_loss = 3.257\n",
      "Epoch  36 Batch   22/1173   train_loss = 3.265\n",
      "Epoch  36 Batch   47/1173   train_loss = 3.253\n",
      "Epoch  36 Batch   72/1173   train_loss = 3.289\n",
      "Epoch  36 Batch   97/1173   train_loss = 3.167\n",
      "Epoch  36 Batch  122/1173   train_loss = 3.252\n",
      "Epoch  36 Batch  147/1173   train_loss = 3.232\n",
      "Epoch  36 Batch  172/1173   train_loss = 3.116\n",
      "Epoch  36 Batch  197/1173   train_loss = 3.263\n",
      "Epoch  36 Batch  222/1173   train_loss = 3.332\n",
      "Epoch  36 Batch  247/1173   train_loss = 3.333\n",
      "Epoch  36 Batch  272/1173   train_loss = 3.334\n",
      "Epoch  36 Batch  297/1173   train_loss = 3.370\n",
      "Epoch  36 Batch  322/1173   train_loss = 3.362\n",
      "Epoch  36 Batch  347/1173   train_loss = 3.256\n",
      "Epoch  36 Batch  372/1173   train_loss = 3.433\n",
      "Epoch  36 Batch  397/1173   train_loss = 3.362\n",
      "Epoch  36 Batch  422/1173   train_loss = 3.249\n",
      "Epoch  36 Batch  447/1173   train_loss = 3.359\n",
      "Epoch  36 Batch  472/1173   train_loss = 3.196\n",
      "Epoch  36 Batch  497/1173   train_loss = 3.238\n",
      "Epoch  36 Batch  522/1173   train_loss = 3.243\n",
      "Epoch  36 Batch  547/1173   train_loss = 3.373\n",
      "Epoch  36 Batch  572/1173   train_loss = 3.268\n",
      "Epoch  36 Batch  597/1173   train_loss = 3.220\n",
      "Epoch  36 Batch  622/1173   train_loss = 3.217\n",
      "Epoch  36 Batch  647/1173   train_loss = 3.461\n",
      "Epoch  36 Batch  672/1173   train_loss = 3.218\n",
      "Epoch  36 Batch  697/1173   train_loss = 3.336\n",
      "Epoch  36 Batch  722/1173   train_loss = 3.317\n",
      "Epoch  36 Batch  747/1173   train_loss = 3.382\n",
      "Epoch  36 Batch  772/1173   train_loss = 3.166\n",
      "Epoch  36 Batch  797/1173   train_loss = 3.195\n",
      "Epoch  36 Batch  822/1173   train_loss = 3.293\n",
      "Epoch  36 Batch  847/1173   train_loss = 3.134\n",
      "Epoch  36 Batch  872/1173   train_loss = 3.261\n",
      "Epoch  36 Batch  897/1173   train_loss = 3.312\n",
      "Epoch  36 Batch  922/1173   train_loss = 3.310\n",
      "Epoch  36 Batch  947/1173   train_loss = 3.282\n",
      "Epoch  36 Batch  972/1173   train_loss = 3.229\n",
      "Epoch  36 Batch  997/1173   train_loss = 3.241\n",
      "Epoch  36 Batch 1022/1173   train_loss = 3.247\n",
      "Epoch  36 Batch 1047/1173   train_loss = 3.212\n",
      "Epoch  36 Batch 1072/1173   train_loss = 3.114\n",
      "Epoch  36 Batch 1097/1173   train_loss = 3.308\n",
      "Epoch  36 Batch 1122/1173   train_loss = 3.353\n",
      "Epoch  36 Batch 1147/1173   train_loss = 3.356\n",
      "Epoch  36 Batch 1172/1173   train_loss = 3.400\n",
      "Epoch  37 Batch   24/1173   train_loss = 3.281\n",
      "Epoch  37 Batch   49/1173   train_loss = 3.167\n",
      "Epoch  37 Batch   74/1173   train_loss = 3.391\n",
      "Epoch  37 Batch   99/1173   train_loss = 3.156\n",
      "Epoch  37 Batch  124/1173   train_loss = 3.109\n",
      "Epoch  37 Batch  149/1173   train_loss = 3.436\n",
      "Epoch  37 Batch  174/1173   train_loss = 3.228\n",
      "Epoch  37 Batch  199/1173   train_loss = 3.268\n",
      "Epoch  37 Batch  224/1173   train_loss = 3.319\n",
      "Epoch  37 Batch  249/1173   train_loss = 3.377\n",
      "Epoch  37 Batch  274/1173   train_loss = 3.272\n",
      "Epoch  37 Batch  299/1173   train_loss = 3.288\n",
      "Epoch  37 Batch  324/1173   train_loss = 3.365\n",
      "Epoch  37 Batch  349/1173   train_loss = 3.238\n",
      "Epoch  37 Batch  374/1173   train_loss = 3.389\n",
      "Epoch  37 Batch  399/1173   train_loss = 3.365\n",
      "Epoch  37 Batch  424/1173   train_loss = 3.367\n",
      "Epoch  37 Batch  449/1173   train_loss = 3.262\n",
      "Epoch  37 Batch  474/1173   train_loss = 3.250\n",
      "Epoch  37 Batch  499/1173   train_loss = 3.208\n",
      "Epoch  37 Batch  524/1173   train_loss = 3.270\n",
      "Epoch  37 Batch  549/1173   train_loss = 3.182\n",
      "Epoch  37 Batch  574/1173   train_loss = 3.377\n",
      "Epoch  37 Batch  599/1173   train_loss = 3.145\n",
      "Epoch  37 Batch  624/1173   train_loss = 3.234\n",
      "Epoch  37 Batch  649/1173   train_loss = 3.296\n",
      "Epoch  37 Batch  674/1173   train_loss = 3.301\n",
      "Epoch  37 Batch  699/1173   train_loss = 3.336\n",
      "Epoch  37 Batch  724/1173   train_loss = 3.198\n",
      "Epoch  37 Batch  749/1173   train_loss = 3.334\n",
      "Epoch  37 Batch  774/1173   train_loss = 3.246\n",
      "Epoch  37 Batch  799/1173   train_loss = 3.153\n",
      "Epoch  37 Batch  824/1173   train_loss = 3.169\n",
      "Epoch  37 Batch  849/1173   train_loss = 3.379\n",
      "Epoch  37 Batch  874/1173   train_loss = 3.274\n",
      "Epoch  37 Batch  899/1173   train_loss = 3.169\n",
      "Epoch  37 Batch  924/1173   train_loss = 3.215\n",
      "Epoch  37 Batch  949/1173   train_loss = 3.284\n",
      "Epoch  37 Batch  974/1173   train_loss = 3.154\n",
      "Epoch  37 Batch  999/1173   train_loss = 3.137\n",
      "Epoch  37 Batch 1024/1173   train_loss = 3.169\n",
      "Epoch  37 Batch 1049/1173   train_loss = 3.332\n",
      "Epoch  37 Batch 1074/1173   train_loss = 3.359\n",
      "Epoch  37 Batch 1099/1173   train_loss = 3.249\n",
      "Epoch  37 Batch 1124/1173   train_loss = 3.285\n",
      "Epoch  37 Batch 1149/1173   train_loss = 3.293\n",
      "Epoch  38 Batch    1/1173   train_loss = 3.219\n",
      "Epoch  38 Batch   26/1173   train_loss = 3.199\n",
      "Epoch  38 Batch   51/1173   train_loss = 3.388\n",
      "Epoch  38 Batch   76/1173   train_loss = 3.261\n",
      "Epoch  38 Batch  101/1173   train_loss = 3.296\n",
      "Epoch  38 Batch  126/1173   train_loss = 3.348\n",
      "Epoch  38 Batch  151/1173   train_loss = 3.335\n",
      "Epoch  38 Batch  176/1173   train_loss = 3.233\n",
      "Epoch  38 Batch  201/1173   train_loss = 3.270\n",
      "Epoch  38 Batch  226/1173   train_loss = 3.310\n",
      "Epoch  38 Batch  251/1173   train_loss = 3.321\n",
      "Epoch  38 Batch  276/1173   train_loss = 3.290\n",
      "Epoch  38 Batch  301/1173   train_loss = 3.160\n",
      "Epoch  38 Batch  326/1173   train_loss = 3.436\n",
      "Epoch  38 Batch  351/1173   train_loss = 3.229\n",
      "Epoch  38 Batch  376/1173   train_loss = 3.235\n",
      "Epoch  38 Batch  401/1173   train_loss = 3.329\n",
      "Epoch  38 Batch  426/1173   train_loss = 3.385\n",
      "Epoch  38 Batch  451/1173   train_loss = 3.293\n",
      "Epoch  38 Batch  476/1173   train_loss = 3.204\n",
      "Epoch  38 Batch  501/1173   train_loss = 3.260\n",
      "Epoch  38 Batch  526/1173   train_loss = 3.270\n",
      "Epoch  38 Batch  551/1173   train_loss = 3.228\n",
      "Epoch  38 Batch  576/1173   train_loss = 3.247\n",
      "Epoch  38 Batch  601/1173   train_loss = 3.352\n",
      "Epoch  38 Batch  626/1173   train_loss = 3.358\n",
      "Epoch  38 Batch  651/1173   train_loss = 3.351\n",
      "Epoch  38 Batch  676/1173   train_loss = 3.258\n",
      "Epoch  38 Batch  701/1173   train_loss = 3.428\n",
      "Epoch  38 Batch  726/1173   train_loss = 3.314\n",
      "Epoch  38 Batch  751/1173   train_loss = 3.268\n",
      "Epoch  38 Batch  776/1173   train_loss = 3.212\n",
      "Epoch  38 Batch  801/1173   train_loss = 3.248\n",
      "Epoch  38 Batch  826/1173   train_loss = 3.231\n",
      "Epoch  38 Batch  851/1173   train_loss = 3.283\n",
      "Epoch  38 Batch  876/1173   train_loss = 3.334\n",
      "Epoch  38 Batch  901/1173   train_loss = 3.298\n",
      "Epoch  38 Batch  926/1173   train_loss = 3.271\n",
      "Epoch  38 Batch  951/1173   train_loss = 3.300\n",
      "Epoch  38 Batch  976/1173   train_loss = 3.430\n",
      "Epoch  38 Batch 1001/1173   train_loss = 3.243\n",
      "Epoch  38 Batch 1026/1173   train_loss = 3.139\n",
      "Epoch  38 Batch 1051/1173   train_loss = 3.250\n",
      "Epoch  38 Batch 1076/1173   train_loss = 3.330\n",
      "Epoch  38 Batch 1101/1173   train_loss = 3.276\n",
      "Epoch  38 Batch 1126/1173   train_loss = 3.191\n",
      "Epoch  38 Batch 1151/1173   train_loss = 3.213\n",
      "Epoch  39 Batch    3/1173   train_loss = 3.159\n",
      "Epoch  39 Batch   28/1173   train_loss = 3.296\n",
      "Epoch  39 Batch   53/1173   train_loss = 3.273\n",
      "Epoch  39 Batch   78/1173   train_loss = 3.258\n",
      "Epoch  39 Batch  103/1173   train_loss = 3.299\n",
      "Epoch  39 Batch  128/1173   train_loss = 3.295\n",
      "Epoch  39 Batch  153/1173   train_loss = 3.157\n",
      "Epoch  39 Batch  178/1173   train_loss = 3.239\n",
      "Epoch  39 Batch  203/1173   train_loss = 3.191\n",
      "Epoch  39 Batch  228/1173   train_loss = 3.137\n",
      "Epoch  39 Batch  253/1173   train_loss = 3.301\n",
      "Epoch  39 Batch  278/1173   train_loss = 3.307\n",
      "Epoch  39 Batch  303/1173   train_loss = 3.189\n",
      "Epoch  39 Batch  328/1173   train_loss = 3.170\n",
      "Epoch  39 Batch  353/1173   train_loss = 3.304\n",
      "Epoch  39 Batch  378/1173   train_loss = 3.122\n",
      "Epoch  39 Batch  403/1173   train_loss = 3.352\n",
      "Epoch  39 Batch  428/1173   train_loss = 3.183\n",
      "Epoch  39 Batch  453/1173   train_loss = 3.354\n",
      "Epoch  39 Batch  478/1173   train_loss = 3.239\n",
      "Epoch  39 Batch  503/1173   train_loss = 3.236\n",
      "Epoch  39 Batch  528/1173   train_loss = 3.107\n",
      "Epoch  39 Batch  553/1173   train_loss = 3.215\n",
      "Epoch  39 Batch  578/1173   train_loss = 3.317\n",
      "Epoch  39 Batch  603/1173   train_loss = 3.312\n",
      "Epoch  39 Batch  628/1173   train_loss = 3.227\n",
      "Epoch  39 Batch  653/1173   train_loss = 3.258\n",
      "Epoch  39 Batch  678/1173   train_loss = 3.326\n",
      "Epoch  39 Batch  703/1173   train_loss = 3.369\n",
      "Epoch  39 Batch  728/1173   train_loss = 3.236\n",
      "Epoch  39 Batch  753/1173   train_loss = 3.228\n",
      "Epoch  39 Batch  778/1173   train_loss = 3.429\n",
      "Epoch  39 Batch  803/1173   train_loss = 3.085\n",
      "Epoch  39 Batch  828/1173   train_loss = 3.424\n",
      "Epoch  39 Batch  853/1173   train_loss = 3.350\n",
      "Epoch  39 Batch  878/1173   train_loss = 3.118\n",
      "Epoch  39 Batch  903/1173   train_loss = 3.305\n",
      "Epoch  39 Batch  928/1173   train_loss = 3.440\n",
      "Epoch  39 Batch  953/1173   train_loss = 3.310\n",
      "Epoch  39 Batch  978/1173   train_loss = 3.197\n",
      "Epoch  39 Batch 1003/1173   train_loss = 3.093\n",
      "Epoch  39 Batch 1028/1173   train_loss = 3.202\n",
      "Epoch  39 Batch 1053/1173   train_loss = 3.327\n",
      "Epoch  39 Batch 1078/1173   train_loss = 3.283\n",
      "Epoch  39 Batch 1103/1173   train_loss = 3.284\n",
      "Epoch  39 Batch 1128/1173   train_loss = 3.307\n",
      "Epoch  39 Batch 1153/1173   train_loss = 3.251\n",
      "Epoch  40 Batch    5/1173   train_loss = 3.037\n",
      "Epoch  40 Batch   30/1173   train_loss = 3.267\n",
      "Epoch  40 Batch   55/1173   train_loss = 3.235\n",
      "Epoch  40 Batch   80/1173   train_loss = 3.263\n",
      "Epoch  40 Batch  105/1173   train_loss = 3.264\n",
      "Epoch  40 Batch  130/1173   train_loss = 3.216\n",
      "Epoch  40 Batch  155/1173   train_loss = 3.263\n",
      "Epoch  40 Batch  180/1173   train_loss = 3.317\n",
      "Epoch  40 Batch  205/1173   train_loss = 3.177\n",
      "Epoch  40 Batch  230/1173   train_loss = 3.230\n",
      "Epoch  40 Batch  255/1173   train_loss = 3.338\n",
      "Epoch  40 Batch  280/1173   train_loss = 3.364\n",
      "Epoch  40 Batch  305/1173   train_loss = 3.290\n",
      "Epoch  40 Batch  330/1173   train_loss = 3.205\n",
      "Epoch  40 Batch  355/1173   train_loss = 3.314\n",
      "Epoch  40 Batch  380/1173   train_loss = 3.204\n",
      "Epoch  40 Batch  405/1173   train_loss = 3.335\n",
      "Epoch  40 Batch  430/1173   train_loss = 3.178\n",
      "Epoch  40 Batch  455/1173   train_loss = 3.283\n",
      "Epoch  40 Batch  480/1173   train_loss = 3.137\n",
      "Epoch  40 Batch  505/1173   train_loss = 3.366\n",
      "Epoch  40 Batch  530/1173   train_loss = 3.150\n",
      "Epoch  40 Batch  555/1173   train_loss = 3.219\n",
      "Epoch  40 Batch  580/1173   train_loss = 3.275\n",
      "Epoch  40 Batch  605/1173   train_loss = 3.363\n",
      "Epoch  40 Batch  630/1173   train_loss = 3.213\n",
      "Epoch  40 Batch  655/1173   train_loss = 3.181\n",
      "Epoch  40 Batch  680/1173   train_loss = 3.236\n",
      "Epoch  40 Batch  705/1173   train_loss = 3.277\n",
      "Epoch  40 Batch  730/1173   train_loss = 3.269\n",
      "Epoch  40 Batch  755/1173   train_loss = 3.345\n",
      "Epoch  40 Batch  780/1173   train_loss = 3.228\n",
      "Epoch  40 Batch  805/1173   train_loss = 3.207\n",
      "Epoch  40 Batch  830/1173   train_loss = 3.357\n",
      "Epoch  40 Batch  855/1173   train_loss = 3.246\n",
      "Epoch  40 Batch  880/1173   train_loss = 3.219\n",
      "Epoch  40 Batch  905/1173   train_loss = 3.245\n",
      "Epoch  40 Batch  930/1173   train_loss = 3.151\n",
      "Epoch  40 Batch  955/1173   train_loss = 3.298\n",
      "Epoch  40 Batch  980/1173   train_loss = 3.282\n",
      "Epoch  40 Batch 1005/1173   train_loss = 3.190\n",
      "Epoch  40 Batch 1030/1173   train_loss = 3.163\n",
      "Epoch  40 Batch 1055/1173   train_loss = 3.267\n",
      "Epoch  40 Batch 1080/1173   train_loss = 3.210\n",
      "Epoch  40 Batch 1105/1173   train_loss = 3.245\n",
      "Epoch  40 Batch 1130/1173   train_loss = 3.188\n",
      "Epoch  40 Batch 1155/1173   train_loss = 3.141\n",
      "Epoch  41 Batch    7/1173   train_loss = 3.194\n",
      "Epoch  41 Batch   32/1173   train_loss = 3.266\n",
      "Epoch  41 Batch   57/1173   train_loss = 3.182\n",
      "Epoch  41 Batch   82/1173   train_loss = 3.251\n",
      "Epoch  41 Batch  107/1173   train_loss = 3.316\n",
      "Epoch  41 Batch  132/1173   train_loss = 3.214\n",
      "Epoch  41 Batch  157/1173   train_loss = 3.355\n",
      "Epoch  41 Batch  182/1173   train_loss = 3.240\n",
      "Epoch  41 Batch  207/1173   train_loss = 3.155\n",
      "Epoch  41 Batch  232/1173   train_loss = 3.295\n",
      "Epoch  41 Batch  257/1173   train_loss = 3.298\n",
      "Epoch  41 Batch  282/1173   train_loss = 3.264\n",
      "Epoch  41 Batch  307/1173   train_loss = 3.194\n",
      "Epoch  41 Batch  332/1173   train_loss = 3.179\n",
      "Epoch  41 Batch  357/1173   train_loss = 3.349\n",
      "Epoch  41 Batch  382/1173   train_loss = 3.364\n",
      "Epoch  41 Batch  407/1173   train_loss = 3.317\n",
      "Epoch  41 Batch  432/1173   train_loss = 3.415\n",
      "Epoch  41 Batch  457/1173   train_loss = 3.388\n",
      "Epoch  41 Batch  482/1173   train_loss = 3.258\n",
      "Epoch  41 Batch  507/1173   train_loss = 3.334\n",
      "Epoch  41 Batch  532/1173   train_loss = 3.340\n",
      "Epoch  41 Batch  557/1173   train_loss = 3.190\n",
      "Epoch  41 Batch  582/1173   train_loss = 3.288\n",
      "Epoch  41 Batch  607/1173   train_loss = 3.239\n",
      "Epoch  41 Batch  632/1173   train_loss = 3.247\n",
      "Epoch  41 Batch  657/1173   train_loss = 3.274\n",
      "Epoch  41 Batch  682/1173   train_loss = 3.206\n",
      "Epoch  41 Batch  707/1173   train_loss = 3.274\n",
      "Epoch  41 Batch  732/1173   train_loss = 3.198\n",
      "Epoch  41 Batch  757/1173   train_loss = 3.274\n",
      "Epoch  41 Batch  782/1173   train_loss = 3.201\n",
      "Epoch  41 Batch  807/1173   train_loss = 3.261\n",
      "Epoch  41 Batch  832/1173   train_loss = 3.338\n",
      "Epoch  41 Batch  857/1173   train_loss = 3.300\n",
      "Epoch  41 Batch  882/1173   train_loss = 3.291\n",
      "Epoch  41 Batch  907/1173   train_loss = 3.173\n",
      "Epoch  41 Batch  932/1173   train_loss = 3.165\n",
      "Epoch  41 Batch  957/1173   train_loss = 3.142\n",
      "Epoch  41 Batch  982/1173   train_loss = 3.074\n",
      "Epoch  41 Batch 1007/1173   train_loss = 3.278\n",
      "Epoch  41 Batch 1032/1173   train_loss = 3.088\n",
      "Epoch  41 Batch 1057/1173   train_loss = 3.144\n",
      "Epoch  41 Batch 1082/1173   train_loss = 3.233\n",
      "Epoch  41 Batch 1107/1173   train_loss = 3.291\n",
      "Epoch  41 Batch 1132/1173   train_loss = 3.023\n",
      "Epoch  41 Batch 1157/1173   train_loss = 3.186\n",
      "Epoch  42 Batch    9/1173   train_loss = 3.282\n",
      "Epoch  42 Batch   34/1173   train_loss = 3.167\n",
      "Epoch  42 Batch   59/1173   train_loss = 3.293\n",
      "Epoch  42 Batch   84/1173   train_loss = 3.231\n",
      "Epoch  42 Batch  109/1173   train_loss = 3.234\n",
      "Epoch  42 Batch  134/1173   train_loss = 3.413\n",
      "Epoch  42 Batch  159/1173   train_loss = 3.334\n",
      "Epoch  42 Batch  184/1173   train_loss = 3.284\n",
      "Epoch  42 Batch  209/1173   train_loss = 3.225\n",
      "Epoch  42 Batch  234/1173   train_loss = 3.350\n",
      "Epoch  42 Batch  259/1173   train_loss = 3.222\n",
      "Epoch  42 Batch  284/1173   train_loss = 3.303\n",
      "Epoch  42 Batch  309/1173   train_loss = 3.274\n",
      "Epoch  42 Batch  334/1173   train_loss = 3.189\n",
      "Epoch  42 Batch  359/1173   train_loss = 3.317\n",
      "Epoch  42 Batch  384/1173   train_loss = 3.226\n",
      "Epoch  42 Batch  409/1173   train_loss = 3.109\n",
      "Epoch  42 Batch  434/1173   train_loss = 3.278\n",
      "Epoch  42 Batch  459/1173   train_loss = 3.209\n",
      "Epoch  42 Batch  484/1173   train_loss = 3.287\n",
      "Epoch  42 Batch  509/1173   train_loss = 3.168\n",
      "Epoch  42 Batch  534/1173   train_loss = 3.176\n",
      "Epoch  42 Batch  559/1173   train_loss = 3.251\n",
      "Epoch  42 Batch  584/1173   train_loss = 3.257\n",
      "Epoch  42 Batch  609/1173   train_loss = 3.265\n",
      "Epoch  42 Batch  634/1173   train_loss = 3.335\n",
      "Epoch  42 Batch  659/1173   train_loss = 3.151\n",
      "Epoch  42 Batch  684/1173   train_loss = 3.258\n",
      "Epoch  42 Batch  709/1173   train_loss = 3.254\n",
      "Epoch  42 Batch  734/1173   train_loss = 3.246\n",
      "Epoch  42 Batch  759/1173   train_loss = 3.242\n",
      "Epoch  42 Batch  784/1173   train_loss = 3.266\n",
      "Epoch  42 Batch  809/1173   train_loss = 3.260\n",
      "Epoch  42 Batch  834/1173   train_loss = 3.206\n",
      "Epoch  42 Batch  859/1173   train_loss = 3.330\n",
      "Epoch  42 Batch  884/1173   train_loss = 3.246\n",
      "Epoch  42 Batch  909/1173   train_loss = 3.268\n",
      "Epoch  42 Batch  934/1173   train_loss = 3.271\n",
      "Epoch  42 Batch  959/1173   train_loss = 3.181\n",
      "Epoch  42 Batch  984/1173   train_loss = 3.296\n",
      "Epoch  42 Batch 1009/1173   train_loss = 3.180\n",
      "Epoch  42 Batch 1034/1173   train_loss = 3.318\n",
      "Epoch  42 Batch 1059/1173   train_loss = 3.224\n",
      "Epoch  42 Batch 1084/1173   train_loss = 3.281\n",
      "Epoch  42 Batch 1109/1173   train_loss = 3.173\n",
      "Epoch  42 Batch 1134/1173   train_loss = 3.195\n",
      "Epoch  42 Batch 1159/1173   train_loss = 3.053\n",
      "Epoch  43 Batch   11/1173   train_loss = 3.405\n",
      "Epoch  43 Batch   36/1173   train_loss = 3.453\n",
      "Epoch  43 Batch   61/1173   train_loss = 3.240\n",
      "Epoch  43 Batch   86/1173   train_loss = 3.459\n",
      "Epoch  43 Batch  111/1173   train_loss = 3.134\n",
      "Epoch  43 Batch  136/1173   train_loss = 3.217\n",
      "Epoch  43 Batch  161/1173   train_loss = 3.104\n",
      "Epoch  43 Batch  186/1173   train_loss = 3.251\n",
      "Epoch  43 Batch  211/1173   train_loss = 3.261\n",
      "Epoch  43 Batch  236/1173   train_loss = 3.155\n",
      "Epoch  43 Batch  261/1173   train_loss = 3.202\n",
      "Epoch  43 Batch  286/1173   train_loss = 3.322\n",
      "Epoch  43 Batch  311/1173   train_loss = 3.290\n",
      "Epoch  43 Batch  336/1173   train_loss = 3.225\n",
      "Epoch  43 Batch  361/1173   train_loss = 3.208\n",
      "Epoch  43 Batch  386/1173   train_loss = 3.306\n",
      "Epoch  43 Batch  411/1173   train_loss = 3.278\n",
      "Epoch  43 Batch  436/1173   train_loss = 3.285\n",
      "Epoch  43 Batch  461/1173   train_loss = 3.301\n",
      "Epoch  43 Batch  486/1173   train_loss = 3.329\n",
      "Epoch  43 Batch  511/1173   train_loss = 3.211\n",
      "Epoch  43 Batch  536/1173   train_loss = 3.302\n",
      "Epoch  43 Batch  561/1173   train_loss = 3.201\n",
      "Epoch  43 Batch  586/1173   train_loss = 3.214\n",
      "Epoch  43 Batch  611/1173   train_loss = 3.146\n",
      "Epoch  43 Batch  636/1173   train_loss = 3.263\n",
      "Epoch  43 Batch  661/1173   train_loss = 3.104\n",
      "Epoch  43 Batch  686/1173   train_loss = 3.302\n",
      "Epoch  43 Batch  711/1173   train_loss = 3.248\n",
      "Epoch  43 Batch  736/1173   train_loss = 3.380\n",
      "Epoch  43 Batch  761/1173   train_loss = 3.230\n",
      "Epoch  43 Batch  786/1173   train_loss = 3.286\n",
      "Epoch  43 Batch  811/1173   train_loss = 3.238\n",
      "Epoch  43 Batch  836/1173   train_loss = 3.326\n",
      "Epoch  43 Batch  861/1173   train_loss = 3.264\n",
      "Epoch  43 Batch  886/1173   train_loss = 3.273\n",
      "Epoch  43 Batch  911/1173   train_loss = 3.309\n",
      "Epoch  43 Batch  936/1173   train_loss = 3.248\n",
      "Epoch  43 Batch  961/1173   train_loss = 3.297\n",
      "Epoch  43 Batch  986/1173   train_loss = 3.316\n",
      "Epoch  43 Batch 1011/1173   train_loss = 3.208\n",
      "Epoch  43 Batch 1036/1173   train_loss = 3.194\n",
      "Epoch  43 Batch 1061/1173   train_loss = 3.250\n",
      "Epoch  43 Batch 1086/1173   train_loss = 3.308\n",
      "Epoch  43 Batch 1111/1173   train_loss = 3.200\n",
      "Epoch  43 Batch 1136/1173   train_loss = 3.300\n",
      "Epoch  43 Batch 1161/1173   train_loss = 3.155\n",
      "Epoch  44 Batch   13/1173   train_loss = 3.236\n",
      "Epoch  44 Batch   38/1173   train_loss = 3.341\n",
      "Epoch  44 Batch   63/1173   train_loss = 3.200\n",
      "Epoch  44 Batch   88/1173   train_loss = 3.212\n",
      "Epoch  44 Batch  113/1173   train_loss = 3.218\n",
      "Epoch  44 Batch  138/1173   train_loss = 3.271\n",
      "Epoch  44 Batch  163/1173   train_loss = 3.301\n",
      "Epoch  44 Batch  188/1173   train_loss = 3.295\n",
      "Epoch  44 Batch  213/1173   train_loss = 3.237\n",
      "Epoch  44 Batch  238/1173   train_loss = 3.233\n",
      "Epoch  44 Batch  263/1173   train_loss = 3.291\n",
      "Epoch  44 Batch  288/1173   train_loss = 3.182\n",
      "Epoch  44 Batch  313/1173   train_loss = 3.255\n",
      "Epoch  44 Batch  338/1173   train_loss = 3.291\n",
      "Epoch  44 Batch  363/1173   train_loss = 3.184\n",
      "Epoch  44 Batch  388/1173   train_loss = 3.236\n",
      "Epoch  44 Batch  413/1173   train_loss = 3.171\n",
      "Epoch  44 Batch  438/1173   train_loss = 3.282\n",
      "Epoch  44 Batch  463/1173   train_loss = 3.228\n",
      "Epoch  44 Batch  488/1173   train_loss = 3.255\n",
      "Epoch  44 Batch  513/1173   train_loss = 3.282\n",
      "Epoch  44 Batch  538/1173   train_loss = 3.298\n",
      "Epoch  44 Batch  563/1173   train_loss = 3.246\n",
      "Epoch  44 Batch  588/1173   train_loss = 3.335\n",
      "Epoch  44 Batch  613/1173   train_loss = 3.241\n",
      "Epoch  44 Batch  638/1173   train_loss = 3.110\n",
      "Epoch  44 Batch  663/1173   train_loss = 3.298\n",
      "Epoch  44 Batch  688/1173   train_loss = 3.225\n",
      "Epoch  44 Batch  713/1173   train_loss = 3.322\n",
      "Epoch  44 Batch  738/1173   train_loss = 3.325\n",
      "Epoch  44 Batch  763/1173   train_loss = 3.187\n",
      "Epoch  44 Batch  788/1173   train_loss = 3.232\n",
      "Epoch  44 Batch  813/1173   train_loss = 3.291\n",
      "Epoch  44 Batch  838/1173   train_loss = 3.282\n",
      "Epoch  44 Batch  863/1173   train_loss = 3.319\n",
      "Epoch  44 Batch  888/1173   train_loss = 3.174\n",
      "Epoch  44 Batch  913/1173   train_loss = 3.244\n",
      "Epoch  44 Batch  938/1173   train_loss = 3.324\n",
      "Epoch  44 Batch  963/1173   train_loss = 3.286\n",
      "Epoch  44 Batch  988/1173   train_loss = 3.243\n",
      "Epoch  44 Batch 1013/1173   train_loss = 3.192\n",
      "Epoch  44 Batch 1038/1173   train_loss = 3.330\n",
      "Epoch  44 Batch 1063/1173   train_loss = 3.067\n",
      "Epoch  44 Batch 1088/1173   train_loss = 3.269\n",
      "Epoch  44 Batch 1113/1173   train_loss = 3.322\n",
      "Epoch  44 Batch 1138/1173   train_loss = 3.253\n",
      "Epoch  44 Batch 1163/1173   train_loss = 3.132\n",
      "Epoch  45 Batch   15/1173   train_loss = 3.319\n",
      "Epoch  45 Batch   40/1173   train_loss = 3.119\n",
      "Epoch  45 Batch   65/1173   train_loss = 3.357\n",
      "Epoch  45 Batch   90/1173   train_loss = 3.329\n",
      "Epoch  45 Batch  115/1173   train_loss = 3.249\n",
      "Epoch  45 Batch  140/1173   train_loss = 3.231\n",
      "Epoch  45 Batch  165/1173   train_loss = 3.340\n",
      "Epoch  45 Batch  190/1173   train_loss = 3.248\n",
      "Epoch  45 Batch  215/1173   train_loss = 3.307\n",
      "Epoch  45 Batch  240/1173   train_loss = 3.304\n",
      "Epoch  45 Batch  265/1173   train_loss = 3.164\n",
      "Epoch  45 Batch  290/1173   train_loss = 3.223\n",
      "Epoch  45 Batch  315/1173   train_loss = 3.387\n",
      "Epoch  45 Batch  340/1173   train_loss = 3.335\n",
      "Epoch  45 Batch  365/1173   train_loss = 3.220\n",
      "Epoch  45 Batch  390/1173   train_loss = 3.197\n",
      "Epoch  45 Batch  415/1173   train_loss = 3.397\n",
      "Epoch  45 Batch  440/1173   train_loss = 3.233\n",
      "Epoch  45 Batch  465/1173   train_loss = 3.281\n",
      "Epoch  45 Batch  490/1173   train_loss = 3.261\n",
      "Epoch  45 Batch  515/1173   train_loss = 3.197\n",
      "Epoch  45 Batch  540/1173   train_loss = 3.269\n",
      "Epoch  45 Batch  565/1173   train_loss = 3.228\n",
      "Epoch  45 Batch  590/1173   train_loss = 3.216\n",
      "Epoch  45 Batch  615/1173   train_loss = 3.210\n",
      "Epoch  45 Batch  640/1173   train_loss = 3.303\n",
      "Epoch  45 Batch  665/1173   train_loss = 3.225\n",
      "Epoch  45 Batch  690/1173   train_loss = 3.179\n",
      "Epoch  45 Batch  715/1173   train_loss = 3.321\n",
      "Epoch  45 Batch  740/1173   train_loss = 3.198\n",
      "Epoch  45 Batch  765/1173   train_loss = 3.275\n",
      "Epoch  45 Batch  790/1173   train_loss = 3.275\n",
      "Epoch  45 Batch  815/1173   train_loss = 3.260\n",
      "Epoch  45 Batch  840/1173   train_loss = 3.235\n",
      "Epoch  45 Batch  865/1173   train_loss = 3.420\n",
      "Epoch  45 Batch  890/1173   train_loss = 3.270\n",
      "Epoch  45 Batch  915/1173   train_loss = 3.295\n",
      "Epoch  45 Batch  940/1173   train_loss = 3.253\n",
      "Epoch  45 Batch  965/1173   train_loss = 3.303\n",
      "Epoch  45 Batch  990/1173   train_loss = 3.136\n",
      "Epoch  45 Batch 1015/1173   train_loss = 3.265\n",
      "Epoch  45 Batch 1040/1173   train_loss = 3.255\n",
      "Epoch  45 Batch 1065/1173   train_loss = 3.191\n",
      "Epoch  45 Batch 1090/1173   train_loss = 3.236\n",
      "Epoch  45 Batch 1115/1173   train_loss = 3.150\n",
      "Epoch  45 Batch 1140/1173   train_loss = 3.165\n",
      "Epoch  45 Batch 1165/1173   train_loss = 3.265\n",
      "Epoch  46 Batch   17/1173   train_loss = 3.245\n",
      "Epoch  46 Batch   42/1173   train_loss = 3.158\n",
      "Epoch  46 Batch   67/1173   train_loss = 3.351\n",
      "Epoch  46 Batch   92/1173   train_loss = 3.268\n",
      "Epoch  46 Batch  117/1173   train_loss = 3.240\n",
      "Epoch  46 Batch  142/1173   train_loss = 3.255\n",
      "Epoch  46 Batch  167/1173   train_loss = 3.301\n",
      "Epoch  46 Batch  192/1173   train_loss = 3.231\n",
      "Epoch  46 Batch  217/1173   train_loss = 3.103\n",
      "Epoch  46 Batch  242/1173   train_loss = 3.339\n",
      "Epoch  46 Batch  267/1173   train_loss = 3.340\n",
      "Epoch  46 Batch  292/1173   train_loss = 3.070\n",
      "Epoch  46 Batch  317/1173   train_loss = 3.353\n",
      "Epoch  46 Batch  342/1173   train_loss = 3.241\n",
      "Epoch  46 Batch  367/1173   train_loss = 3.231\n",
      "Epoch  46 Batch  392/1173   train_loss = 3.157\n",
      "Epoch  46 Batch  417/1173   train_loss = 3.364\n",
      "Epoch  46 Batch  442/1173   train_loss = 3.290\n",
      "Epoch  46 Batch  467/1173   train_loss = 3.282\n",
      "Epoch  46 Batch  492/1173   train_loss = 3.210\n",
      "Epoch  46 Batch  517/1173   train_loss = 3.169\n",
      "Epoch  46 Batch  542/1173   train_loss = 3.322\n",
      "Epoch  46 Batch  567/1173   train_loss = 3.163\n",
      "Epoch  46 Batch  592/1173   train_loss = 3.199\n",
      "Epoch  46 Batch  617/1173   train_loss = 3.159\n",
      "Epoch  46 Batch  642/1173   train_loss = 3.294\n",
      "Epoch  46 Batch  667/1173   train_loss = 3.219\n",
      "Epoch  46 Batch  692/1173   train_loss = 3.114\n",
      "Epoch  46 Batch  717/1173   train_loss = 3.429\n",
      "Epoch  46 Batch  742/1173   train_loss = 3.257\n",
      "Epoch  46 Batch  767/1173   train_loss = 3.210\n",
      "Epoch  46 Batch  792/1173   train_loss = 3.218\n",
      "Epoch  46 Batch  817/1173   train_loss = 3.338\n",
      "Epoch  46 Batch  842/1173   train_loss = 3.294\n",
      "Epoch  46 Batch  867/1173   train_loss = 3.210\n",
      "Epoch  46 Batch  892/1173   train_loss = 3.324\n",
      "Epoch  46 Batch  917/1173   train_loss = 3.316\n",
      "Epoch  46 Batch  942/1173   train_loss = 3.243\n",
      "Epoch  46 Batch  967/1173   train_loss = 3.240\n",
      "Epoch  46 Batch  992/1173   train_loss = 3.169\n",
      "Epoch  46 Batch 1017/1173   train_loss = 3.265\n",
      "Epoch  46 Batch 1042/1173   train_loss = 3.183\n",
      "Epoch  46 Batch 1067/1173   train_loss = 3.349\n",
      "Epoch  46 Batch 1092/1173   train_loss = 3.345\n",
      "Epoch  46 Batch 1117/1173   train_loss = 3.204\n",
      "Epoch  46 Batch 1142/1173   train_loss = 3.226\n",
      "Epoch  46 Batch 1167/1173   train_loss = 3.086\n",
      "Epoch  47 Batch   19/1173   train_loss = 3.205\n",
      "Epoch  47 Batch   44/1173   train_loss = 3.308\n",
      "Epoch  47 Batch   69/1173   train_loss = 3.153\n",
      "Epoch  47 Batch   94/1173   train_loss = 3.374\n",
      "Epoch  47 Batch  119/1173   train_loss = 3.285\n",
      "Epoch  47 Batch  144/1173   train_loss = 3.283\n",
      "Epoch  47 Batch  169/1173   train_loss = 3.217\n",
      "Epoch  47 Batch  194/1173   train_loss = 3.299\n",
      "Epoch  47 Batch  219/1173   train_loss = 3.176\n",
      "Epoch  47 Batch  244/1173   train_loss = 3.357\n",
      "Epoch  47 Batch  269/1173   train_loss = 3.322\n",
      "Epoch  47 Batch  294/1173   train_loss = 3.184\n",
      "Epoch  47 Batch  319/1173   train_loss = 3.285\n",
      "Epoch  47 Batch  344/1173   train_loss = 3.133\n",
      "Epoch  47 Batch  369/1173   train_loss = 3.089\n",
      "Epoch  47 Batch  394/1173   train_loss = 3.177\n",
      "Epoch  47 Batch  419/1173   train_loss = 3.280\n",
      "Epoch  47 Batch  444/1173   train_loss = 3.136\n",
      "Epoch  47 Batch  469/1173   train_loss = 3.334\n",
      "Epoch  47 Batch  494/1173   train_loss = 3.200\n",
      "Epoch  47 Batch  519/1173   train_loss = 3.185\n",
      "Epoch  47 Batch  544/1173   train_loss = 3.162\n",
      "Epoch  47 Batch  569/1173   train_loss = 3.160\n",
      "Epoch  47 Batch  594/1173   train_loss = 3.266\n",
      "Epoch  47 Batch  619/1173   train_loss = 3.339\n",
      "Epoch  47 Batch  644/1173   train_loss = 3.177\n",
      "Epoch  47 Batch  669/1173   train_loss = 3.230\n",
      "Epoch  47 Batch  694/1173   train_loss = 3.267\n",
      "Epoch  47 Batch  719/1173   train_loss = 3.104\n",
      "Epoch  47 Batch  744/1173   train_loss = 3.205\n",
      "Epoch  47 Batch  769/1173   train_loss = 3.286\n",
      "Epoch  47 Batch  794/1173   train_loss = 3.144\n",
      "Epoch  47 Batch  819/1173   train_loss = 3.196\n",
      "Epoch  47 Batch  844/1173   train_loss = 3.240\n",
      "Epoch  47 Batch  869/1173   train_loss = 3.125\n",
      "Epoch  47 Batch  894/1173   train_loss = 3.269\n",
      "Epoch  47 Batch  919/1173   train_loss = 3.227\n",
      "Epoch  47 Batch  944/1173   train_loss = 3.266\n",
      "Epoch  47 Batch  969/1173   train_loss = 3.192\n",
      "Epoch  47 Batch  994/1173   train_loss = 3.300\n",
      "Epoch  47 Batch 1019/1173   train_loss = 3.188\n",
      "Epoch  47 Batch 1044/1173   train_loss = 3.436\n",
      "Epoch  47 Batch 1069/1173   train_loss = 3.206\n",
      "Epoch  47 Batch 1094/1173   train_loss = 3.114\n",
      "Epoch  47 Batch 1119/1173   train_loss = 3.136\n",
      "Epoch  47 Batch 1144/1173   train_loss = 3.213\n",
      "Epoch  47 Batch 1169/1173   train_loss = 3.126\n",
      "Epoch  48 Batch   21/1173   train_loss = 3.079\n",
      "Epoch  48 Batch   46/1173   train_loss = 3.172\n",
      "Epoch  48 Batch   71/1173   train_loss = 3.328\n",
      "Epoch  48 Batch   96/1173   train_loss = 3.154\n",
      "Epoch  48 Batch  121/1173   train_loss = 3.176\n",
      "Epoch  48 Batch  146/1173   train_loss = 3.377\n",
      "Epoch  48 Batch  171/1173   train_loss = 3.249\n",
      "Epoch  48 Batch  196/1173   train_loss = 3.315\n",
      "Epoch  48 Batch  221/1173   train_loss = 3.206\n",
      "Epoch  48 Batch  246/1173   train_loss = 3.460\n",
      "Epoch  48 Batch  271/1173   train_loss = 3.346\n",
      "Epoch  48 Batch  296/1173   train_loss = 3.249\n",
      "Epoch  48 Batch  321/1173   train_loss = 3.261\n",
      "Epoch  48 Batch  346/1173   train_loss = 3.309\n",
      "Epoch  48 Batch  371/1173   train_loss = 3.245\n",
      "Epoch  48 Batch  396/1173   train_loss = 3.306\n",
      "Epoch  48 Batch  421/1173   train_loss = 3.151\n",
      "Epoch  48 Batch  446/1173   train_loss = 3.251\n",
      "Epoch  48 Batch  471/1173   train_loss = 3.212\n",
      "Epoch  48 Batch  496/1173   train_loss = 3.248\n",
      "Epoch  48 Batch  521/1173   train_loss = 3.145\n",
      "Epoch  48 Batch  546/1173   train_loss = 3.327\n",
      "Epoch  48 Batch  571/1173   train_loss = 3.316\n",
      "Epoch  48 Batch  596/1173   train_loss = 3.288\n",
      "Epoch  48 Batch  621/1173   train_loss = 3.298\n",
      "Epoch  48 Batch  646/1173   train_loss = 3.375\n",
      "Epoch  48 Batch  671/1173   train_loss = 3.356\n",
      "Epoch  48 Batch  696/1173   train_loss = 3.356\n",
      "Epoch  48 Batch  721/1173   train_loss = 3.253\n",
      "Epoch  48 Batch  746/1173   train_loss = 3.221\n",
      "Epoch  48 Batch  771/1173   train_loss = 3.201\n",
      "Epoch  48 Batch  796/1173   train_loss = 3.112\n",
      "Epoch  48 Batch  821/1173   train_loss = 3.187\n",
      "Epoch  48 Batch  846/1173   train_loss = 3.200\n",
      "Epoch  48 Batch  871/1173   train_loss = 3.203\n",
      "Epoch  48 Batch  896/1173   train_loss = 3.310\n",
      "Epoch  48 Batch  921/1173   train_loss = 3.318\n",
      "Epoch  48 Batch  946/1173   train_loss = 3.330\n",
      "Epoch  48 Batch  971/1173   train_loss = 3.194\n",
      "Epoch  48 Batch  996/1173   train_loss = 3.292\n",
      "Epoch  48 Batch 1021/1173   train_loss = 3.342\n",
      "Epoch  48 Batch 1046/1173   train_loss = 3.171\n",
      "Epoch  48 Batch 1071/1173   train_loss = 3.244\n",
      "Epoch  48 Batch 1096/1173   train_loss = 3.222\n",
      "Epoch  48 Batch 1121/1173   train_loss = 3.281\n",
      "Epoch  48 Batch 1146/1173   train_loss = 3.219\n",
      "Epoch  48 Batch 1171/1173   train_loss = 3.205\n",
      "Epoch  49 Batch   23/1173   train_loss = 3.137\n",
      "Epoch  49 Batch   48/1173   train_loss = 3.259\n",
      "Epoch  49 Batch   73/1173   train_loss = 3.168\n",
      "Epoch  49 Batch   98/1173   train_loss = 3.291\n",
      "Epoch  49 Batch  123/1173   train_loss = 3.230\n",
      "Epoch  49 Batch  148/1173   train_loss = 3.200\n",
      "Epoch  49 Batch  173/1173   train_loss = 3.198\n",
      "Epoch  49 Batch  198/1173   train_loss = 3.209\n",
      "Epoch  49 Batch  223/1173   train_loss = 3.164\n",
      "Epoch  49 Batch  248/1173   train_loss = 3.263\n",
      "Epoch  49 Batch  273/1173   train_loss = 3.243\n",
      "Epoch  49 Batch  298/1173   train_loss = 3.235\n",
      "Epoch  49 Batch  323/1173   train_loss = 3.233\n",
      "Epoch  49 Batch  348/1173   train_loss = 3.333\n",
      "Epoch  49 Batch  373/1173   train_loss = 3.277\n",
      "Epoch  49 Batch  398/1173   train_loss = 3.302\n",
      "Epoch  49 Batch  423/1173   train_loss = 3.240\n",
      "Epoch  49 Batch  448/1173   train_loss = 3.271\n",
      "Epoch  49 Batch  473/1173   train_loss = 3.207\n",
      "Epoch  49 Batch  498/1173   train_loss = 3.349\n",
      "Epoch  49 Batch  523/1173   train_loss = 3.308\n",
      "Epoch  49 Batch  548/1173   train_loss = 3.220\n",
      "Epoch  49 Batch  573/1173   train_loss = 3.404\n",
      "Epoch  49 Batch  598/1173   train_loss = 3.260\n",
      "Epoch  49 Batch  623/1173   train_loss = 3.310\n",
      "Epoch  49 Batch  648/1173   train_loss = 3.328\n",
      "Epoch  49 Batch  673/1173   train_loss = 3.181\n",
      "Epoch  49 Batch  698/1173   train_loss = 3.307\n",
      "Epoch  49 Batch  723/1173   train_loss = 3.305\n",
      "Epoch  49 Batch  748/1173   train_loss = 3.216\n",
      "Epoch  49 Batch  773/1173   train_loss = 3.270\n",
      "Epoch  49 Batch  798/1173   train_loss = 3.243\n",
      "Epoch  49 Batch  823/1173   train_loss = 3.159\n",
      "Epoch  49 Batch  848/1173   train_loss = 3.245\n",
      "Epoch  49 Batch  873/1173   train_loss = 3.186\n",
      "Epoch  49 Batch  898/1173   train_loss = 3.388\n",
      "Epoch  49 Batch  923/1173   train_loss = 3.362\n",
      "Epoch  49 Batch  948/1173   train_loss = 3.322\n",
      "Epoch  49 Batch  973/1173   train_loss = 3.143\n",
      "Epoch  49 Batch  998/1173   train_loss = 3.237\n",
      "Epoch  49 Batch 1023/1173   train_loss = 3.267\n",
      "Epoch  49 Batch 1048/1173   train_loss = 3.262\n",
      "Epoch  49 Batch 1073/1173   train_loss = 3.302\n",
      "Epoch  49 Batch 1098/1173   train_loss = 3.309\n",
      "Epoch  49 Batch 1123/1173   train_loss = 3.172\n",
      "Epoch  49 Batch 1148/1173   train_loss = 3.142\n",
      "Model Trained and Saved\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL\n",
    "\"\"\"\n",
    "batches = get_batches(int_text, batch_size, seq_length)\n",
    "\n",
    "with tf.Session(graph=train_graph) as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "\n",
    "    for epoch_i in range(num_epochs):\n",
    "        state = sess.run(initial_state, {input_text: batches[0][0]})\n",
    "\n",
    "        for batch_i, (x, y) in enumerate(batches):\n",
    "            feed = {\n",
    "                input_text: x,\n",
    "                targets: y,\n",
    "                initial_state: state,\n",
    "                lr: learning_rate}\n",
    "            train_loss, state, _ = sess.run([cost, final_state, train_op], feed)\n",
    "\n",
    "            # Show every <show_every_n_batches> batches\n",
    "            if (epoch_i * len(batches) + batch_i) % show_every_n_batches == 0:\n",
    "                print('Epoch {:>3} Batch {:>4}/{}   train_loss = {:.3f}'.format(\n",
    "                    epoch_i,\n",
    "                    batch_i,\n",
    "                    len(batches),\n",
    "                    train_loss))\n",
    "\n",
    "    # Save Model\n",
    "    saver = tf.train.Saver()\n",
    "    saver.save(sess, save_dir)\n",
    "    print('Model Trained and Saved')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Save Parameters\n",
    "Save `seq_length` and `save_dir` for generating a new TV script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL\n",
    "\"\"\"\n",
    "# Save parameters for checkpoint\n",
    "helper.save_params((seq_length, save_dir))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL\n",
    "\"\"\"\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import helper\n",
    "import problem_unittests as tests\n",
    "\n",
    "_, vocab_to_int, int_to_vocab, token_dict = helper.load_preprocess()\n",
    "seq_length, load_dir = helper.load_params()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Implement Generate Functions\n",
    "### Get Tensors\n",
    "Get tensors from `loaded_graph` using the function [`get_tensor_by_name()`](https://www.tensorflow.org/api_docs/python/tf/Graph#get_tensor_by_name).  Get the tensors using the following names:\n",
    "- \"input:0\"\n",
    "- \"initial_state:0\"\n",
    "- \"final_state:0\"\n",
    "- \"probs:0\"\n",
    "\n",
    "Return the tensors in the following tuple `(InputTensor, InitialStateTensor, FinalStateTensor, ProbsTensor)` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def get_tensors(loaded_graph):\n",
    "    \"\"\"\n",
    "    Get input, initial state, final state, and probabilities tensor from <loaded_graph>\n",
    "    :param loaded_graph: TensorFlow graph loaded from file\n",
    "    :return: Tuple (InputTensor, InitialStateTensor, FinalStateTensor, ProbsTensor)\n",
    "    \"\"\"\n",
    "    input_tensor = loaded_graph.get_tensor_by_name('input:0')\n",
    "    init_state_tensor = loaded_graph.get_tensor_by_name('initial_state:0')\n",
    "    final_state_tensor = loaded_graph.get_tensor_by_name('final_state:0')\n",
    "    probs_tensor = loaded_graph.get_tensor_by_name('probs:0')\n",
    "    return input_tensor, init_state_tensor, final_state_tensor, probs_tensor\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_get_tensors(get_tensors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Choose Word\n",
    "Implement the `pick_word()` function to select the next word using `probabilities`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def weighted_choice(choices):\n",
    "    \"\"\"\n",
    "    Cribbed from http://stackoverflow.com/questions/3679694/a-weighted-version-of-random-choice\n",
    "    \"\"\"\n",
    "    total = sum(w for c, w in choices)\n",
    "    r = random.uniform(0, total)\n",
    "    upto = 0\n",
    "    for c, w in choices:\n",
    "        if upto + w >= r:\n",
    "            return c\n",
    "        upto += w\n",
    "    assert False, \"Shouldn't get here\"\n",
    "\n",
    "def pick_word(probabilities, int_to_vocab, top_n=5):\n",
    "    \"\"\"\n",
    "    Pick the next word in the generated text\n",
    "    :param probabilities: Probabilites of the next word\n",
    "    :param int_to_vocab: Dictionary of word ids as the keys and words as the values\n",
    "    :return: String of the predicted word\n",
    "    \"\"\"\n",
    "    #print('Num probs: {}'.format(len(probabilities)))\n",
    "\n",
    "    top_n_choices = []\n",
    "    for i in range(min(len(probabilities), top_n)):\n",
    "        max_idx = np.argmax(probabilities)\n",
    "        top_n_choices.append((max_idx, probabilities[max_idx]))\n",
    "        probabilities.itemset(max_idx, 0)\n",
    "\n",
    "    #print('Top {} highest indexes: {}'.format(top_n, top_n_choices))\n",
    "\n",
    "    word_idx = weighted_choice(top_n_choices)\n",
    "    word = int_to_vocab[word_idx]\n",
    "    #print('Chosen word: {} (idx: {})'.format(word_idx, word))\n",
    "    return word\n",
    "\n",
    "    #highest_prob_idx = np.squeeze(np.argwhere(probabilities == np.max(probabilities)))\n",
    "    #word_idx = np.random.choice(highest_prob_idx)\n",
    "    #word = int_to_vocab[word_idx]\n",
    "    #return word\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_pick_word(pick_word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Generate TV Script\n",
    "This will generate the TV script for you.  Set `gen_length` to the length of TV script you want to generate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bart_simpson:(sincere) i am not. i can't let you go back to the case of hers.\n",
      "homer_simpson:(touched) oh, that's just a carp laundry.\n",
      "homer_simpson:(horrified) baaart!\n",
      "(wiggum_home: ext. video store - continuous)\n",
      "marge_simpson:(into phone)\n",
      "(s. a.) glands ointed...\n",
      "homer_simpson: oh, my little homie-bear! you don't know what to do.\n",
      "homer_simpson:(touched noise) well, i guess.\n",
      "(hallway: int. hallway - continuous)\n",
      "homer_simpson:(chuckles, looks at marge) i can't do anything. i can't do that.\n",
      "(death_row: int. log - moments later)\n",
      "lisa_simpson: i don't care what i am.(flips o. s.) a birthday service?!\n",
      "(simpson_home: ext. simpson house - front steps - day)\n",
      "lisa_simpson:(panting)\n",
      "homer_simpson:(chuckles)\n",
      "(sideshow_mel's_dressing_room: int. sideshow bob's cabin - continuous)\n",
      "lisa_simpson:(blissed-out noise) oh my brother, i'm a real lot of my life, homer.\n",
      "(krusty's_office: int. krusty's office - night)\n",
      "homer_simpson: oh, my-diddily-i.\n",
      "marge_simpson:(to lisa)\" the beer\"...(brief pause)\n",
      "(below_bridge: ext. below.\n",
      "(simpson_home: int. simpson house - living room - later)\n",
      "bart_simpson:(to himself) wacky, oh, caramba!\n",
      "homer_simpson:(moans)\n",
      "(simpson_home: ext. simpson house - day - establishing)\n",
      "homer_simpson: oh, my-diddily-i.\n",
      "sheriff:(irish snarl)\n",
      "marge_simpson:(gasp) oh, homie...\n",
      "homer_simpson: well, i guess i'll be right to the level.\n",
      "marge_simpson:(to marge) you don't know what to do.\n",
      "(girder: ext. girder cab - later)\n",
      "(simpson_living_room: int. simpson living room - continuous)\n",
      "homer_simpson: i don't know.\n",
      "homer_simpson: oh, my-diddily-i. i'm buyin'!\n",
      "homer_simpson:(chuckles)\n",
      "homer_simpson:(grooving sigh) oh, i know you're not a miracle! it's a miracle!\n",
      "(adult_film_section:\n"
     ]
    }
   ],
   "source": [
    "gen_length = 400\n",
    "# homer_simpson, moe_szyslak, or Barney_Gumble\n",
    "prime_word = 'bart_simpson'\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "loaded_graph = tf.Graph()\n",
    "with tf.Session(graph=loaded_graph) as sess:\n",
    "    # Load saved model\n",
    "    loader = tf.train.import_meta_graph(load_dir + '.meta')\n",
    "    loader.restore(sess, load_dir)\n",
    "\n",
    "    # Get Tensors from loaded model\n",
    "    input_text, initial_state, final_state, probs = get_tensors(loaded_graph)\n",
    "\n",
    "    # Sentences generation setup\n",
    "    gen_sentences = [prime_word + ':']\n",
    "    prev_state = sess.run(initial_state, {input_text: np.array([[1]])})\n",
    "\n",
    "    # Generate sentences\n",
    "    for n in range(gen_length):\n",
    "        # Dynamic Input\n",
    "        dyn_input = [[vocab_to_int[word] for word in gen_sentences[-seq_length:]]]\n",
    "        dyn_seq_length = len(dyn_input[0])\n",
    "\n",
    "        # Get Prediction\n",
    "        probabilities, prev_state = sess.run(\n",
    "            [probs, final_state],\n",
    "            {input_text: dyn_input, initial_state: prev_state})\n",
    "        \n",
    "        pred_word = pick_word(probabilities[dyn_seq_length-1], int_to_vocab)\n",
    "\n",
    "        gen_sentences.append(pred_word)\n",
    "\n",
    "    # Remove tokens\n",
    "    tv_script = ' '.join(gen_sentences)\n",
    "    for key, token in token_dict.items():\n",
    "        ending = ' ' if key in ['\\n', '(', '\"'] else ''\n",
    "        tv_script = tv_script.replace(' ' + token.lower(), key)\n",
    "    tv_script = tv_script.replace('\\n ', '\\n')\n",
    "    tv_script = tv_script.replace('( ', '(')\n",
    "\n",
    "    print(tv_script)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# The TV Script is Nonsensical\n",
    "It's ok if the TV script doesn't make any sense.  We trained on less than a megabyte of text.  In order to get good results, you'll have to use a smaller vocabulary or get more data.  Luckly there's more data!  As we mentioned in the begging of this project, this is a subset of [another dataset](https://www.kaggle.com/wcukierski/the-simpsons-by-the-data).  We didn't have you train on all the data, because that would take too long.  However, you are free to train your neural network on all the data.  After you complete the project, of course.\n",
    "# Submitting This Project\n",
    "When submitting this project, make sure to run all the cells before saving the notebook. Save the notebook file as \"dlnd_tv_script_generation.ipynb\" and save it as a HTML file under \"File\" -> \"Download as\". Include the \"helper.py\" and \"problem_unittests.py\" files in your submission."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
